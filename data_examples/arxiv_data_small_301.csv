"names","day","month","year","abstract","link","tag","title","emails"
"Ahmed Osman	Wojciech Samek","1","2","2018","We propose an architecture for VQA which utilizes recurrent layers to generate visual and textual attention. The memory characteristic of the proposed recurrent attention units offers a rich joint embedding of visual and textual features and enables the model to reason relations between several parts of the image and question. Our single model outperforms the first place winner on the VQA 1.0 dataset, performs within margin to the current state-of-the-art ensemble model. We also experiment with replacing attention mechanisms in other state-of-the-art models with our implementation and show increased accuracy. In both cases, our recurrent attention mechanism improves performance in tasks requiring sequential or relational reasoning on the VQA dataset.","http://arxiv.org/pdf/1802.00209v1","cs.AI	cs.CL	cs.CV	cs.NE	stat.ML","Dual Recurrent Attention Units for Visual Question Answering","ahmed.osman@hhi.fraunhofer.de	wojciech.samek@hhi.fraunhofer.de"
"Ji Young Lee	Franck Dernoncourt","12","3","2016","Recent approaches based on artificial neural networks (ANNs) have shown promising results for short-text classification. However, many short texts occur in sequences (e.g., sentences in a document or utterances in a dialog), and most existing ANN-based systems do not leverage the preceding short texts when classifying a subsequent one. In this work, we present a model based on recurrent neural networks and convolutional neural networks that incorporates the preceding short texts. Our model achieves state-of-the-art results on three different datasets for dialog act prediction.","http://arxiv.org/pdf/1603.03827v1","cs.CL	cs.AI	cs.LG	cs.NE	stat.ML","Sequential Short-Text Classification with Recurrent and Convolutional   Neural Networks","jjylee@mit.edu	francky@mit.edu"
"Iulian Vlad Serban	Tim Klinger	Gerald Tesauro	Kartik Talamadupula	Bowen Zhou	Yoshua Bengio	Aaron Courville","2","6","2016","We introduce the multiresolution recurrent neural network, which extends the sequence-to-sequence framework to model natural language generation as two parallel discrete stochastic processes: a sequence of high-level coarse tokens, and a sequence of natural language tokens. There are many ways to estimate or learn the high-level coarse tokens, but we argue that a simple extraction procedure is sufficient to capture a wealth of high-level discourse semantics. Such procedure allows training the multiresolution recurrent neural network by maximizing the exact joint log-likelihood over both sequences. In contrast to the standard log- likelihood objective w.r.t. natural language tokens (word perplexity), optimizing the joint log-likelihood biases the model towards modeling high-level abstractions. We apply the proposed model to the task of dialogue response generation in two challenging domains: the Ubuntu technical support domain, and Twitter conversations. On Ubuntu, the model outperforms competing approaches by a substantial margin, achieving state-of-the-art results according to both automatic evaluation metrics and a human evaluation study. On Twitter, the model appears to generate more relevant and on-topic responses according to automatic evaluation metrics. Finally, our experiments demonstrate that the proposed model is more adept at overcoming the sparsity of natural language and is better able to capture long-term structure.","http://arxiv.org/pdf/1606.00776v2","cs.CL	cs.AI	cs.LG	cs.NE	stat.ML	I.5.1; I.2.7","Multiresolution Recurrent Neural Networks: An Application to Dialogue   Response Generation",""
"Sebastian Ruder	Joachim Bingel	Isabelle Augenstein	Anders Søgaard","23","5","2017","Multi-task learning is motivated by the observation that humans bring to bear what they know about related problems when solving new ones. Similarly, deep neural networks can profit from related tasks by sharing parameters with other networks. However, humans do not consciously decide to transfer knowledge between tasks. In Natural Language Processing (NLP), it is hard to predict if sharing will lead to improvements, particularly if tasks are only loosely related. To overcome this, we introduce Sluice Networks, a general framework for multi-task learning where trainable parameters control the amount of sharing. Our framework generalizes previous proposals in enabling sharing of all combinations of subspaces, layers, and skip connections. We perform experiments on three task pairs, and across seven different domains, using data from OntoNotes 5.0, and achieve up to 15% average error reductions over common approaches to multi-task learning. We show that a) label entropy is predictive of gains in sluice networks, confirming findings for hard parameter sharing and b) while sluice networks easily fit noise, they are robust across domains in practice.","http://arxiv.org/pdf/1705.08142v2","stat.ML	cs.AI	cs.CL	cs.LG	cs.NE","Learning what to share between loosely related tasks","sebastian@ruder.io,	bingel|soegaard}@di.ku.dk	augenstein@di.ku.dk"
"Iulian V. Serban	Chinnadhurai Sankar	Mathieu Germain	Saizheng Zhang	Zhouhan Lin	Sandeep Subramanian	Taesup Kim	Michael Pieper	Sarath Chandar	Nan Rosemary Ke	Sai Rajeshwar	Alexandre de Brebisson	Jose M. R. Sotelo	Dendi Suhubdy	Vincent Michalski	Alexandre Nguyen	Joelle Pineau	Yoshua Bengio","7","9","2017","We present MILABOT: a deep reinforcement learning chatbot developed by the Montreal Institute for Learning Algorithms (MILA) for the Amazon Alexa Prize competition. MILABOT is capable of conversing with humans on popular small talk topics through both speech and text. The system consists of an ensemble of natural language generation and retrieval models, including template-based models, bag-of-words models, sequence-to-sequence neural network and latent variable neural network models. By applying reinforcement learning to crowdsourced data and real-world user interactions, the system has been trained to select an appropriate response from the models in its ensemble. The system has been evaluated through A/B testing with real-world users, where it performed significantly better than many competing systems. Due to its machine learning architecture, the system is likely to improve with additional data.","http://arxiv.org/pdf/1709.02349v2","cs.CL	cs.AI	cs.LG	cs.NE	stat.ML	I.5.1; I.2.7","A Deep Reinforcement Learning Chatbot",""
"Kelvin Guu	Tatsunori B. Hashimoto	Yonatan Oren	Percy Liang","26","9","2017","We propose a new generative model of sentences that first samples a prototype sentence from the training corpus and then edits it into a new sentence. Compared to traditional models that generate from scratch either left-to-right or by first sampling a latent sentence vector, our prototype-then-edit model improves perplexity on language modeling and generates higher quality outputs according to human evaluation. Furthermore, the model gives rise to a latent edit vector that captures interpretable semantics such as sentence similarity and sentence-level analogies.","http://arxiv.org/pdf/1709.08878v1","cs.CL	cs.AI	cs.LG	cs.NE	stat.ML","Generating Sentences by Editing Prototypes","kguu@stanford.edu	thashim@stanford.edu	yonatano@stanford.edu	pliang@cs.stanford.edu"
"Iulian V. Serban	Chinnadhurai Sankar	Mathieu Germain	Saizheng Zhang	Zhouhan Lin	Sandeep Subramanian	Taesup Kim	Michael Pieper	Sarath Chandar	Nan Rosemary Ke	Sai Rajeswar	Alexandre de Brebisson	Jose M. R. Sotelo	Dendi Suhubdy	Vincent Michalski	Alexandre Nguyen	Joelle Pineau	Yoshua Bengio","20","1","2018","We present MILABOT: a deep reinforcement learning chatbot developed by the Montreal Institute for Learning Algorithms (MILA) for the Amazon Alexa Prize competition. MILABOT is capable of conversing with humans on popular small talk topics through both speech and text. The system consists of an ensemble of natural language generation and retrieval models, including neural network and template-based models. By applying reinforcement learning to crowdsourced data and real-world user interactions, the system has been trained to select an appropriate response from the models in its ensemble. The system has been evaluated through A/B testing with real-world users, where it performed significantly better than other systems. The results highlight the potential of coupling ensemble systems with deep reinforcement learning as a fruitful path for developing real-world, open-domain conversational agents.","http://arxiv.org/pdf/1801.06700v1","cs.CL	cs.AI	cs.LG	cs.NE	stat.ML	I.5.1; I.2.7","A Deep Reinforcement Learning Chatbot (Short Version)",""
"Darko Brodic	Alessia Amelio	Zoran N. Milivojevic	Milena Jevtic","21","9","2016","The paper introduces a new method for discrimination of documents given in different scripts. The document is mapped into a uniformly coded text of numerical values. It is derived from the position of the letters in the text line, based on their typographical characteristics. Each code is considered as a gray level. Accordingly, the coded text determines a 1-D image, on which texture analysis by run-length statistics and local binary pattern is performed. It defines feature vectors representing the script content of the document. A modified clustering approach employed on document feature vector groups documents written in the same script. Experimentation performed on two custom oriented databases of historical documents in old Cyrillic, angular and round Glagolitic as well as Antiqua and Fraktur scripts demonstrates the superiority of the proposed method with respect to well-known methods in the state-of-the-art.","http://arxiv.org/pdf/1609.06492v1","cs.CV	cs.AI	cs.CL	cs.LG	cs.NE	97R40, 62H35, 68U15, 68T50,","Document Image Coding and Clustering for Script Discrimination","dbrodic@tf.bor.ac.rs	mjevtic@tf.bor.ac.rs	aamelio@dimes.unical.it	zoran.milivojevic@vtsnis.edu.rs"
"Mateusz Malinowski	Mario Fritz","4","10","2016","Together with the development of more accurate methods in Computer Vision and Natural Language Understanding, holistic architectures that answer on questions about the content of real-world images have emerged. In this tutorial, we build a neural-based approach to answer questions about images. We base our tutorial on two datasets: (mostly on) DAQUAR, and (a bit on) VQA. With small tweaks the models that we present here can achieve a competitive performance on both datasets, in fact, they are among the best methods that use a combination of LSTM with a global, full frame CNN representation of an image. We hope that after reading this tutorial, the reader will be able to use Deep Learning frameworks, such as Keras and introduced Kraino, to build various architectures that will lead to a further performance improvement on this challenging task.","http://arxiv.org/pdf/1610.01076v1","cs.CV	cs.AI	cs.CL	cs.LG	cs.NE","Tutorial on Answering Questions about Images with Deep Learning","mmalinow@mpi-inf.mpg.de	mfritz@mpi-inf.mpg.de"
"Tony Beltramelli","22","5","2017","Transforming a graphical user interface screenshot created by a designer into computer code is a typical task conducted by a developer in order to build customized software, websites, and mobile applications. In this paper, we show that deep learning methods can be leveraged to train a model end-to-end to automatically generate code from a single input image with over 77% of accuracy for three different platforms (i.e. iOS, Android and web-based technologies).","http://arxiv.org/pdf/1705.07962v2","cs.LG	cs.AI	cs.CL	cs.CV	cs.NE	68T45	I.2.1; I.2.10; I.2.2; I.2.6","pix2code: Generating Code from a Graphical User Interface Screenshot","tony@uizard.io"
"Fred Richardson	Douglas Reynolds	Najim Dehak","3","4","2015","Learned feature representations and sub-phoneme posteriors from Deep Neural Networks (DNNs) have been used separately to produce significant performance gains for speaker and language recognition tasks. In this work we show how these gains are possible using a single DNN for both speaker and language recognition. The unified DNN approach is shown to yield substantial performance improvements on the the 2013 Domain Adaptation Challenge speaker recognition task (55% reduction in EER for the out-of-domain condition) and on the NIST 2011 Language Recognition Evaluation (48% reduction in EER for the 30s test condition).","http://arxiv.org/pdf/1504.00923v1","cs.CL	cs.CV	cs.LG	cs.NE	stat.ML","A Unified Deep Neural Network for Speaker and Language Recognition",""
"Hieu Pham	Melody Y. Guan	Barret Zoph	Quoc V. Le	Jeff Dean","9","2","2018","We propose Efficient Neural Architecture Search (ENAS), a fast and inexpensive approach for automatic model design. In ENAS, a controller learns to discover neural network architectures by searching for an optimal subgraph within a large computational graph. The controller is trained with policy gradient to select a subgraph that maximizes the expected reward on the validation set. Meanwhile the model corresponding to the selected subgraph is trained to minimize a canonical cross entropy loss. Thanks to parameter sharing between child models, ENAS is fast: it delivers strong empirical performances using much fewer GPU-hours than all existing automatic model design approaches, and notably, 1000x less expensive than standard Neural Architecture Search. On the Penn Treebank dataset, ENAS discovers a novel architecture that achieves a test perplexity of 55.8, establishing a new state-of-the-art among all methods without post-training processing. On the CIFAR-10 dataset, ENAS designs novel architectures that achieve a test error of 2.89%, which is on par with NASNet (Zoph et al., 2018), whose test error is 2.65%.","http://arxiv.org/pdf/1802.03268v2","cs.LG	cs.CL	cs.CV	cs.NE	stat.ML","Efficient Neural Architecture Search via Parameter Sharing",""
"Brenden M. Lake	Tomer D. Ullman	Joshua B. Tenenbaum	Samuel J. Gershman","1","4","2016","Recent progress in artificial intelligence (AI) has renewed interest in building systems that learn and think like people. Many advances have come from using deep neural networks trained end-to-end in tasks such as object recognition, video games, and board games, achieving performance that equals or even beats humans in some respects. Despite their biological inspiration and performance achievements, these systems differ from human intelligence in crucial ways. We review progress in cognitive science suggesting that truly human-like learning and thinking machines will have to reach beyond current engineering trends in both what they learn, and how they learn it. Specifically, we argue that these machines should (a) build causal models of the world that support explanation and understanding, rather than merely solving pattern recognition problems; (b) ground learning in intuitive theories of physics and psychology, to support and enrich the knowledge that is learned; and (c) harness compositionality and learning-to-learn to rapidly acquire and generalize knowledge to new tasks and situations. We suggest concrete challenges and promising routes towards these goals that can combine the strengths of recent neural network advances with more structured cognitive models.","http://arxiv.org/pdf/1604.00289v3","cs.AI	cs.CV	cs.LG	cs.NE	stat.ML","Building Machines That Learn and Think Like People",""
"Hao Wang	Dit-Yan Yeung","6","4","2016","While perception tasks such as visual object recognition and text understanding play an important role in human intelligence, the subsequent tasks that involve inference, reasoning and planning require an even higher level of intelligence. The past few years have seen major advances in many perception tasks using deep learning models. For higher-level inference, however, probabilistic graphical models with their Bayesian nature are still more powerful and flexible. To achieve integrated intelligence that involves both perception and inference, it is naturally desirable to tightly integrate deep learning and Bayesian models within a principled probabilistic framework, which we call Bayesian deep learning. In this unified framework, the perception of text or images using deep learning can boost the performance of higher-level inference and in return, the feedback from the inference process is able to enhance the perception of text or images. This survey provides a general introduction to Bayesian deep learning and reviews its recent applications on recommender systems, topic models, and control. In this survey, we also discuss the relationship and differences between Bayesian deep learning and other related topics like Bayesian treatment of neural networks.","http://arxiv.org/pdf/1604.01662v2","stat.ML	cs.AI	cs.CV	cs.LG	cs.NE","Towards Bayesian Deep Learning: A Survey","hwangaz@cse.ust.hk	dyyeung@cse.ust.hk"
"Tejas D. Kulkarni	Karthik R. Narasimhan	Ardavan Saeedi	Joshua B. Tenenbaum","20","4","2016","Learning goal-directed behavior in environments with sparse feedback is a major challenge for reinforcement learning algorithms. The primary difficulty arises due to insufficient exploration, resulting in an agent being unable to learn robust value functions. Intrinsically motivated agents can explore new behavior for its own sake rather than to directly solve problems. Such intrinsic behaviors could eventually help the agent solve tasks posed by the environment. We present hierarchical-DQN (h-DQN), a framework to integrate hierarchical value functions, operating at different temporal scales, with intrinsically motivated deep reinforcement learning. A top-level value function learns a policy over intrinsic goals, and a lower-level function learns a policy over atomic actions to satisfy the given goals. h-DQN allows for flexible goal specifications, such as functions over entities and relations. This provides an efficient space for exploration in complicated environments. We demonstrate the strength of our approach on two problems with very sparse, delayed feedback: (1) a complex discrete stochastic decision process, and (2) the classic ATARI game `Montezuma's Revenge'.","http://arxiv.org/pdf/1604.06057v2","cs.LG	cs.AI	cs.CV	cs.NE	stat.ML","Hierarchical Deep Reinforcement Learning: Integrating Temporal   Abstraction and Intrinsic Motivation",""
"Deepak Pathak	Ross Girshick	Piotr Dollár	Trevor Darrell	Bharath Hariharan","19","12","2016","This paper presents a novel yet intuitive approach to unsupervised feature learning. Inspired by the human visual system, we explore whether low-level motion-based grouping cues can be used to learn an effective visual representation. Specifically, we use unsupervised motion-based segmentation on videos to obtain segments, which we use as 'pseudo ground truth' to train a convolutional network to segment objects from a single frame. Given the extensive evidence that motion plays a key role in the development of the human visual system, we hope that this straightforward approach to unsupervised learning will be more effective than cleverly designed 'pretext' tasks studied in the literature. Indeed, our extensive experiments show that this is the case. When used for transfer learning on object detection, our representation significantly outperforms previous unsupervised approaches across multiple settings, especially when training data for the target task is scarce.","http://arxiv.org/pdf/1612.06370v2","cs.CV	cs.AI	cs.LG	cs.NE	stat.ML","Learning Features by Watching Objects Move",""
"Muhammad Ghifary	W. Bastiaan Kleijn	Mengjie Zhang","21","9","2014","We propose a simple neural network model to deal with the domain adaptation problem in object recognition. Our model incorporates the Maximum Mean Discrepancy (MMD) measure as a regularization in the supervised learning to reduce the distribution mismatch between the source and target domains in the latent space. From experiments, we demonstrate that the MMD regularization is an effective tool to provide good domain adaptation models on both SURF features and raw image pixels of a particular image data set. We also show that our proposed model, preceded by the denoising auto-encoder pretraining, achieves better performance than recent benchmark models on the same data sets. This work represents the first study of MMD measure in the context of neural networks.","http://arxiv.org/pdf/1409.6041v1","cs.CV	cs.AI	cs.LG	cs.NE	stat.ML","Domain Adaptive Neural Networks for Object Recognition","muhammad.ghifary@ecs.vuw.ac.nz	bastiaan.kleijn@ecs.vuw.ac.nz	mengjie.zhang@ecs.vuw.ac.nz"
"Lionel Pigou	Aäron van den Oord	Sander Dieleman	Mieke Van Herreweghe	Joni Dambre","5","6","2015","Recent studies have demonstrated the power of recurrent neural networks for machine translation, image captioning and speech recognition. For the task of capturing temporal structure in video, however, there still remain numerous open research questions. Current research suggests using a simple temporal feature pooling strategy to take into account the temporal aspect of video. We demonstrate that this method is not sufficient for gesture recognition, where temporal information is more discriminative compared to general video classification tasks. We explore deep architectures for gesture recognition in video and propose a new end-to-end trainable neural network architecture incorporating temporal convolutions and bidirectional recurrence. Our main contributions are twofold; first, we show that recurrence is crucial for this task; second, we show that adding temporal convolutions leads to significant improvements. We evaluate the different approaches on the Montalbano gesture recognition dataset, where we achieve state-of-the-art results.","http://arxiv.org/pdf/1506.01911v3","cs.CV	cs.AI	cs.LG	cs.NE	stat.ML","Beyond Temporal Pooling: Recurrence and Temporal Convolutions for   Gesture Recognition in Video",""
"Rakesh Achanta	Trevor Hastie","20","9","2015","In this paper, we address the task of Optical Character Recognition(OCR) for the Telugu script. We present an end-to-end framework that segments the text image, classifies the characters and extracts lines using a language model. The segmentation is based on mathematical morphology. The classification module, which is the most challenging task of the three, is a deep convolutional neural network. The language is modelled as a third degree markov chain at the glyph level. Telugu script is a complex alphasyllabary and the language is agglutinative, making the problem hard. In this paper we apply the latest advances in neural networks to achieve state-of-the-art error rates. We also review convolutional neural networks in great detail and expound the statistical justification behind the many tricks needed to make Deep Learning work.","http://arxiv.org/pdf/1509.05962v2","stat.ML	cs.AI	cs.CV	cs.LG	cs.NE","Telugu OCR Framework using Deep Learning",""
"Jeff Donahue	Philipp Krähenbühl	Trevor Darrell","31","5","2016","The ability of the Generative Adversarial Networks (GANs) framework to learn generative models mapping from simple latent distributions to arbitrarily complex data distributions has been demonstrated empirically, with compelling results showing that the latent space of such generators captures semantic variation in the data distribution. Intuitively, models trained to predict these semantic latent representations given data may serve as useful feature representations for auxiliary problems where semantics are relevant. However, in their existing form, GANs have no means of learning the inverse mapping -- projecting data back into the latent space. We propose Bidirectional Generative Adversarial Networks (BiGANs) as a means of learning this inverse mapping, and demonstrate that the resulting learned feature representation is useful for auxiliary supervised discrimination tasks, competitive with contemporary approaches to unsupervised and self-supervised feature learning.","http://arxiv.org/pdf/1605.09782v7","cs.LG	cs.AI	cs.CV	cs.NE	stat.ML","Adversarial Feature Learning","jdonahue@cs.berkeley.edu	philkr@utexas.edu	trevor@eecs.berkeley.edu"
"Zachary C. Lipton","10","6","2016","Supervised machine learning models boast remarkable predictive capabilities. But can you trust your model? Will it work in deployment? What else can it tell you about the world? We want models to be not only good, but interpretable. And yet the task of interpretation appears underspecified. Papers provide diverse and sometimes non-overlapping motivations for interpretability, and offer myriad notions of what attributes render models interpretable. Despite this ambiguity, many papers proclaim interpretability axiomatically, absent further explanation. In this paper, we seek to refine the discourse on interpretability. First, we examine the motivations underlying interest in interpretability, finding them to be diverse and occasionally discordant. Then, we address model properties and techniques thought to confer interpretability, identifying transparency to humans and post-hoc explanations as competing notions. Throughout, we discuss the feasibility and desirability of different notions, and question the oft-made assertions that linear models are interpretable and that deep neural networks are not.","http://arxiv.org/pdf/1606.03490v3","cs.LG	cs.AI	cs.CV	cs.NE	stat.ML","The Mythos of Model Interpretability",""
"Sahil Garg	Irina Rish	Guillermo Cecchi	Aurelie Lozano","22","1","2017","In this paper, we focus on online representation learning in non-stationary environments which may require continuous adaptation of model architecture. We propose a novel online dictionary-learning (sparse-coding) framework which incorporates the addition and deletion of hidden units (dictionary elements), and is inspired by the adult neurogenesis phenomenon in the dentate gyrus of the hippocampus, known to be associated with improved cognitive function and adaptation to new environments. In the online learning setting, where new input instances arrive sequentially in batches, the neuronal-birth is implemented by adding new units with random initial weights (random dictionary elements); the number of new units is determined by the current performance (representation error) of the dictionary, higher error causing an increase in the birth rate. Neuronal-death is implemented by imposing l1/l2-regularization (group sparsity) on the dictionary within the block-coordinate descent optimization at each iteration of our online alternating minimization scheme, which iterates between the code and dictionary updates. Finally, hidden unit connectivity adaptation is facilitated by introducing sparsity in dictionary elements. Our empirical evaluation on several real-life datasets (images and language) as well as on synthetic data demonstrates that the proposed approach can considerably outperform the state-of-art fixed-size (nonadaptive) online sparse coding of Mairal et al. (2009) in the presence of nonstationary data. Moreover, we identify certain properties of the data (e.g., sparse inputs with nearly non-overlapping supports) and of the model (e.g., dictionary sparsity) associated with such improvements.","http://arxiv.org/pdf/1701.06106v2","cs.LG	cs.AI	cs.CV	cs.NE	stat.ML","Neurogenesis-Inspired Dictionary Learning: Online Model Adaption in a   Changing World","sahilgar@usc.edu	rish@us.ibm.com	gcecchi@us.ibm.com	aclozano@us.ibm.com"
"Weifeng Ge	Yizhou Yu","28","2","2017","Deep neural networks require a large amount of labeled training data during supervised learning. However, collecting and labeling so much data might be infeasible in many cases. In this paper, we introduce a source-target selective joint fine-tuning scheme for improving the performance of deep learning tasks with insufficient training data. In this scheme, a target learning task with insufficient training data is carried out simultaneously with another source learning task with abundant training data. However, the source learning task does not use all existing training data. Our core idea is to identify and use a subset of training images from the original source learning task whose low-level characteristics are similar to those from the target learning task, and jointly fine-tune shared convolutional layers for both tasks. Specifically, we compute descriptors from linear or nonlinear filter bank responses on training images from both tasks, and use such descriptors to search for a desired subset of training samples for the source learning task.   Experiments demonstrate that our selective joint fine-tuning scheme achieves state-of-the-art performance on multiple visual classification tasks with insufficient training data for deep learning. Such tasks include Caltech 256, MIT Indoor 67, Oxford Flowers 102 and Stanford Dogs 120. In comparison to fine-tuning without a source domain, the proposed method can improve the classification accuracy by 2% - 10% using a single model.","http://arxiv.org/pdf/1702.08690v2","cs.CV	cs.AI	cs.LG	cs.NE	stat.ML","Borrowing Treasures from the Wealthy: Deep Transfer Learning through   Selective Joint Fine-tuning",""
"Tanmay Gupta	Kevin Shih	Saurabh Singh	Derek Hoiem","2","4","2017","An important goal of computer vision is to build systems that learn visual representations over time that can be applied to many tasks. In this paper, we investigate a vision-language embedding as a core representation and show that it leads to better cross-task transfer than standard multi-task learning. In particular, the task of visual recognition is aligned to the task of visual question answering by forcing each to use the same word-region embeddings. We show this leads to greater inductive transfer from recognition to VQA than standard multitask learning. Visual recognition also improves, especially for categories that have relatively few recognition training labels but appear often in the VQA setting. Thus, our paper takes a small step towards creating more general vision systems by showing the benefit of interpretable, flexible, and trainable core representations.","http://arxiv.org/pdf/1704.00260v2","cs.CV	cs.AI	cs.LG	cs.NE	stat.ML","Aligned Image-Word Representations Improve Inductive Transfer Across   Vision-Language Tasks","tgupta6@illinois.edu	kjshih2@illinois.edu	dhoiem@illinois.edu	saurabhsingh@google.com"
"Jan Hendrik Metzen	Mummadi Chaithanya Kumar	Thomas Brox	Volker Fischer","19","4","2017","While deep learning is remarkably successful on perceptual tasks, it was also shown to be vulnerable to adversarial perturbations of the input. These perturbations denote noise added to the input that was generated specifically to fool the system while being quasi-imperceptible for humans. More severely, there even exist universal perturbations that are input-agnostic but fool the network on the majority of inputs. While recent work has focused on image classification, this work proposes attacks against semantic image segmentation: we present an approach for generating (universal) adversarial perturbations that make the network yield a desired target segmentation as output. We show empirically that there exist barely perceptible universal noise patterns which result in nearly the same predicted segmentation for arbitrary inputs. Furthermore, we also show the existence of universal noise which removes a target class (e.g., all pedestrians) from the segmentation while leaving the segmentation mostly unchanged otherwise.","http://arxiv.org/pdf/1704.05712v3","stat.ML	cs.AI	cs.CV	cs.LG	cs.NE","Universal Adversarial Perturbations Against Semantic Image Segmentation","janhendrik.metzen@de.bosch.com	chaithu0536@gmail.com	brox@cs.uni-freiburg.de	volker.fischer@de.bosch.com"
"Quynh Nguyen	Matthias Hein","26","4","2017","While the optimization problem behind deep neural networks is highly non-convex, it is frequently observed in practice that training deep networks seems possible without getting stuck in suboptimal points. It has been argued that this is the case as all local minima are close to being globally optimal. We show that this is (almost) true, in fact almost all local minima are globally optimal, for a fully connected network with squared loss and analytic activation function given that the number of hidden units of one layer of the network is larger than the number of training points and the network structure from this layer on is pyramidal.","http://arxiv.org/pdf/1704.08045v2","cs.LG	cs.AI	cs.CV	cs.NE	stat.ML","The loss surface of deep and wide neural networks",""
"Chris Donahue	Zachary C. Lipton	Akshay Balsubramani	Julian McAuley","22","5","2017","We propose a new algorithm for training generative adversarial networks that jointly learns latent codes for both identities (e.g. individual humans) and observations (e.g. specific photographs). By fixing the identity portion of the latent codes, we can generate diverse images of the same subject, and by fixing the observation portion, we can traverse the manifold of subjects while maintaining contingent aspects such as lighting and pose. Our algorithm features a pairwise training scheme in which each sample from the generator consists of two images with a common identity code. Corresponding samples from the real dataset consist of two distinct photographs of the same subject. In order to fool the discriminator, the generator must produce pairs that are photorealistic, distinct, and appear to depict the same individual. We augment both the DCGAN and BEGAN approaches with Siamese discriminators to facilitate pairwise training. Experiments with human judges and an off-the-shelf face verification system demonstrate our algorithm's ability to generate convincing, identity-matched photographs.","http://arxiv.org/pdf/1705.07904v3","cs.LG	cs.AI	cs.CV	cs.NE	stat.ML","Semantically Decomposing the Latent Spaces of Generative Adversarial   Networks","cdonahue@ucsd.edu	zlipton@cmu.edu	abalsubr@stanford.edu	jmcauley@eng.ucsd.edu"
"Mahesh Chandra Mukkamala	Matthias Hein","17","6","2017","Adaptive gradient methods have become recently very popular, in particular as they have been shown to be useful in the training of deep neural networks. In this paper we have analyzed RMSProp, originally proposed for the training of deep neural networks, in the context of online convex optimization and show $\sqrt{T}$-type regret bounds. Moreover, we propose two variants SC-Adagrad and SC-RMSProp for which we show logarithmic regret bounds for strongly convex functions. Finally, we demonstrate in the experiments that these new variants outperform other adaptive gradient techniques or stochastic gradient descent in the optimization of strongly convex functions as well as in training of deep neural networks.","http://arxiv.org/pdf/1706.05507v2","cs.LG	cs.AI	cs.CV	cs.NE	stat.ML","Variants of RMSProp and Adagrad with Logarithmic Regret Bounds",""
"Chunyuan Li	Hao Liu	Changyou Chen	Yunchen Pu	Liqun Chen	Ricardo Henao	Lawrence Carin","5","9","2017","We investigate the non-identifiability issues associated with bidirectional adversarial training for joint distribution matching. Within a framework of conditional entropy, we propose both adversarial and non-adversarial approaches to learn desirable matched joint distributions for unsupervised and supervised tasks. We unify a broad family of adversarial models as joint distribution matching problems. Our approach stabilizes learning of unsupervised bidirectional adversarial learning methods. Further, we introduce an extension for semi-supervised learning tasks. Theoretical results are validated in synthetic data and real-world applications.","http://arxiv.org/pdf/1709.01215v2","stat.ML	cs.AI	cs.CV	cs.LG	cs.NE","ALICE: Towards Understanding Adversarial Learning for Joint Distribution   Matching","cl319@duke.edu"
"Mateusz Buda	Atsuto Maki	Maciej A. Mazurowski","15","10","2017","In this study, we systematically investigate the impact of class imbalance on classification performance of convolutional neural networks (CNNs) and compare frequently used methods to address the issue. Class imbalance is a common problem that has been comprehensively studied in classical machine learning, yet very limited systematic research is available in the context of deep learning. In our study, we use three benchmark datasets of increasing complexity, MNIST, CIFAR-10 and ImageNet, to investigate the effects of imbalance on classification and perform an extensive comparison of several methods to address the issue: oversampling, undersampling, two-phase training, and thresholding that compensates for prior class probabilities. Our main evaluation metric is area under the receiver operating characteristic curve (ROC AUC) adjusted to multi-class tasks since overall accuracy metric is associated with notable difficulties in the context of imbalanced data. Based on results from our experiments we conclude that (i) the effect of class imbalance on classification performance is detrimental; (ii) the method of addressing class imbalance that emerged as dominant in almost all analyzed scenarios was oversampling; (iii) oversampling should be applied to the level that totally eliminates the imbalance, whereas undersampling can perform better when the imbalance is only removed to some extent; (iv) as opposed to some classical machine learning models, oversampling does not necessarily cause overfitting of CNNs; (v) thresholding should be applied to compensate for prior class probabilities when overall number of properly classified cases is of interest.","http://arxiv.org/pdf/1710.05381v1","cs.CV	cs.AI	cs.LG	cs.NE	stat.ML","A systematic study of the class imbalance problem in convolutional   neural networks","buda	atsuto}@kth.se	maciej.mazurowski@duke.edu"
"Jan Kukačka	Vladimir Golkov	Daniel Cremers","29","10","2017","Regularization is one of the crucial ingredients of deep learning, yet the term regularization has various definitions, and regularization methods are often studied separately from each other. In our work we present a systematic, unifying taxonomy to categorize existing methods. We distinguish methods that affect data, network architectures, error terms, regularization terms, and optimization procedures. We do not provide all details about the listed methods; instead, we present an overview of how the methods can be sorted into meaningful categories and sub-categories. This helps revealing links and fundamental similarities between them. Finally, we include practical recommendations both for users and for developers of new regularization methods.","http://arxiv.org/pdf/1710.10686v1","cs.LG	cs.AI	cs.CV	cs.NE	stat.ML	62M45	I.2.6; I.5","Regularization for Deep Learning: A Taxonomy","jan.kukacka@tum.de	vladimir.golkov@tum.de	cremers@tum.de"
"Elie Aljalbout	Vladimir Golkov	Yawar Siddiqui	Daniel Cremers","23","1","2018","Clustering is a fundamental machine learning method. The quality of its results is dependent on the data distribution. For this reason, deep neural networks can be used for learning better representations of the data. In this paper, we propose a systematic taxonomy for clustering with deep learning, in addition to a review of methods from the field. Based on our taxonomy, creating new methods is more straightforward. We also propose a new approach which is built on the taxonomy and surpasses some of the limitations of some previous work. Our experimental evaluation on image datasets shows that the method approaches state-of-the-art clustering quality, and performs better in some cases.","http://arxiv.org/pdf/1801.07648v1","cs.LG	cs.AI	cs.CV	cs.NE	stat.ML	62H30, 62M45, 91C20	H.3.3; I.2.6; I.5; I.5.3; I.5.4","Clustering with Deep Learning: Taxonomy and New Methods","firstname.lastname@tum.de	cremers@tum.de"
"Armand Zampieri	Guillaume Charpiat	Yuliya Tarabalka","27","2","2018","We tackle here the problem of multimodal image non-rigid registration, which is of prime importance in remote sensing and medical imaging. The difficulties encountered by classical registration approaches include feature design and slow optimization by gradient descent. By analyzing these methods, we note the significance of the notion of scale. We design easy-to-train, fully-convolutional neural networks able to learn scale-specific features. Once chained appropriately, they perform global registration in linear time, getting rid of gradient descent schemes by predicting directly the deformation.We show their performance in terms of quality and speed through various tasks of remote sensing multimodal image alignment. In particular, we are able to register correctly cadastral maps of buildings as well as road polylines onto RGB images, and outperform current keypoint matching methods.","http://arxiv.org/pdf/1802.09816v1","cs.CV	cs.AI	cs.LG	cs.NE	stat.ML","Coarse to fine non-rigid registration: a chain of scale-specific neural   networks for multimodal image alignment with application to remote sensing","guillaume.charpiat@inria.fr	yuliya.tarabalka@inria.fr"
"Li Yao	Atousa Torabi	Kyunghyun Cho	Nicolas Ballas	Christopher Pal	Hugo Larochelle	Aaron Courville","27","2","2015","Recent progress in using recurrent neural networks (RNNs) for image description has motivated the exploration of their application for video description. However, while images are static, working with videos requires modeling their dynamic temporal structure and then properly integrating that information into a natural language description. In this context, we propose an approach that successfully takes into account both the local and global temporal structure of videos to produce descriptions. First, our approach incorporates a spatial temporal 3-D convolutional neural network (3-D CNN) representation of the short temporal dynamics. The 3-D CNN representation is trained on video action recognition tasks, so as to produce a representation that is tuned to human motion and behavior. Second we propose a temporal attention mechanism that allows to go beyond local temporal modeling and learns to automatically select the most relevant temporal segments given the text-generating RNN. Our approach exceeds the current state-of-art for both BLEU and METEOR metrics on the Youtube2Text dataset. We also present results on a new, larger and more challenging dataset of paired video and natural language descriptions.","http://arxiv.org/pdf/1502.08029v5","stat.ML	cs.AI	cs.CL	cs.CV	cs.LG","Describing Videos by Exploiting Temporal Structure","li.yao@umontreal.ca	atousa.torabi@umontreal.ca	kyunghyun.cho@umontreal.ca	nicolas.ballas@umontreal.ca	christopher.pal@polymtl.ca	hugo.larochelle@usherbrooke.ca	aaron.courville@umontreal.ca"
"Hao Wang	Xingjian Shi	Dit-Yan Yeung","2","11","2016","Hybrid methods that utilize both content and rating information are commonly used in many recommender systems. However, most of them use either handcrafted features or the bag-of-words representation as a surrogate for the content information but they are neither effective nor natural enough. To address this problem, we develop a collaborative recurrent autoencoder (CRAE) which is a denoising recurrent autoencoder (DRAE) that models the generation of content sequences in the collaborative filtering (CF) setting. The model generalizes recent advances in recurrent deep learning from i.i.d. input to non-i.i.d. (CF-based) input and provides a new denoising scheme along with a novel learnable pooling scheme for the recurrent autoencoder. To do this, we first develop a hierarchical Bayesian model for the DRAE and then generalize it to the CF setting. The synergy between denoising and CF enables CRAE to make accurate recommendations while learning to fill in the blanks in sequences. Experiments on real-world datasets from different domains (CiteULike and Netflix) show that, by jointly modeling the order-aware generation of sequences for the content information and performing CF for the ratings, CRAE is able to significantly outperform the state of the art on both the recommendation task based on ratings and the sequence generation task based on content information.","http://arxiv.org/pdf/1611.00454v1","cs.LG	cs.AI	cs.CL	cs.CV	stat.ML","Collaborative Recurrent Autoencoder: Recommend while Learning to Fill in   the Blanks","hwangaz@cse.ust.hk	xshiab@cse.ust.hk	dyyeung@cse.ust.hk"
"Laura Graesser	Abhinav Gupta	Lakshay Sharma	Evelina Bakhturina","3","12","2017","In this project we analysed how much semantic information images carry, and how much value image data can add to sentiment analysis of the text associated with the images. To better understand the contribution from images, we compared models which only made use of image data, models which only made use of text data, and models which combined both data types. We also analysed if this approach could help sentiment classifiers generalize to unknown sentiments.","http://arxiv.org/pdf/1712.00725v1","cs.CL	cs.AI	cs.CV	cs.LG	stat.ML","Sentiment Classification using Images and Label Embeddings","lhg256@nyu.edu	abhinavg@nyu.edu	ls4170@nyu.edu	eb2992@nyu.edu"
"Hao Wang	Xingjian Shi	Dit-Yan Yeung","2","11","2016","Neural networks (NN) have achieved state-of-the-art performance in various applications. Unfortunately in applications where training data is insufficient, they are often prone to overfitting. One effective way to alleviate this problem is to exploit the Bayesian approach by using Bayesian neural networks (BNN). Another shortcoming of NN is the lack of flexibility to customize different distributions for the weights and neurons according to the data, as is often done in probabilistic graphical models. To address these problems, we propose a class of probabilistic neural networks, dubbed natural-parameter networks (NPN), as a novel and lightweight Bayesian treatment of NN. NPN allows the usage of arbitrary exponential-family distributions to model the weights and neurons. Different from traditional NN and BNN, NPN takes distributions as input and goes through layers of transformation before producing distributions to match the target output distributions. As a Bayesian treatment, efficient backpropagation (BP) is performed to learn the natural parameters for the distributions over both the weights and neurons. The output distributions of each layer, as byproducts, may be used as second-order representations for the associated tasks such as link prediction. Experiments on real-world datasets show that NPN can achieve state-of-the-art performance.","http://arxiv.org/pdf/1611.00448v1","cs.LG	cs.AI	cs.CL	cs.CV	stat.ML","Natural-Parameter Networks: A Class of Probabilistic Neural Networks","hwangaz@cse.ust.hk	xshiab@cse.ust.hk	dyyeung@cse.ust.hk"
"Misha Denil	Pulkit Agrawal	Tejas D Kulkarni	Tom Erez	Peter Battaglia	Nando de Freitas","6","11","2016","When encountering novel objects, humans are able to infer a wide range of physical properties such as mass, friction and deformability by interacting with them in a goal driven way. This process of active interaction is in the same spirit as a scientist performing experiments to discover hidden facts. Recent advances in artificial intelligence have yielded machines that can achieve superhuman performance in Go, Atari, natural language processing, and complex control problems; however, it is not clear that these systems can rival the scientific intuition of even a young child. In this work we introduce a basic set of tasks that require agents to estimate properties such as mass and cohesion of objects in an interactive simulated environment where they can manipulate the objects and observe the consequences. We found that state of art deep reinforcement learning methods can learn to perform the experiments necessary to discover such hidden properties. By systematically manipulating the problem difficulty and the cost incurred by the agent for performing experiments, we found that agents learn different strategies that balance the cost of gathering information against the cost of making mistakes in different situations.","http://arxiv.org/pdf/1611.01843v3","stat.ML	cs.AI	cs.CV	cs.LG	cs.NE	physics.soc-ph","Learning to Perform Physics Experiments via Deep Reinforcement Learning","mdenil@google.com	tkulkarni@google.com	etom@google.com	peterbattaglia@google.com	nandodefreitas@google.com	pulkitag@berkeley.edu"
"Tsung-Hsien Wen	David Vandyke	Nikola Mrksic	Milica Gasic	Lina M. Rojas-Barahona	Pei-Hao Su	Stefan Ultes	Steve Young","15","4","2016","Teaching machines to accomplish tasks by conversing naturally with humans is challenging. Currently, developing task-oriented dialogue systems requires creating multiple components and typically this involves either a large amount of handcrafting, or acquiring costly labelled datasets to solve a statistical learning problem for each component. In this work we introduce a neural network-based text-in, text-out end-to-end trainable goal-oriented dialogue system along with a new way of collecting dialogue data based on a novel pipe-lined Wizard-of-Oz framework. This approach allows us to develop dialogue systems easily and without making too many assumptions about the task at hand. The results show that the model can converse with human subjects naturally whilst helping them to accomplish tasks in a restaurant search domain.","http://arxiv.org/pdf/1604.04562v3","cs.CL	cs.AI	cs.NE	stat.ML","A Network-based End-to-End Trainable Task-oriented Dialogue System","thw28@cam.ac.uk	djv27@cam.ac.uk	nm480@cam.ac.uk	mg436@cam.ac.uk	lmr46@cam.ac.uk	phs26@cam.ac.uk	su259@cam.ac.uk	sjy11@cam.ac.uk"
"Johannes Welbl	Guillaume Bouchard	Sebastian Riedel","20","4","2016","Embedding-based Knowledge Base Completion models have so far mostly combined distributed representations of individual entities or relations to compute truth scores of missing links. Facts can however also be represented using pairwise embeddings, i.e. embeddings for pairs of entities and relations. In this paper we explore such bigram embeddings with a flexible Factorization Machine model and several ablations from it. We investigate the relevance of various bigram types on the fb15k237 dataset and find relative improvements compared to a compositional model.","http://arxiv.org/pdf/1604.05878v1","cs.CL	cs.AI	cs.NE	stat.ML","A Factorization Machine Framework for Testing Bigram Embeddings in   Knowledgebase Completion","j.welbl@cs.ucl.ac.uk	g.bouchard@cs.ucl.ac.uk	s.riedel@cs.ucl.ac.uk"
"Franck Dernoncourt	Ji Young Lee	Peter Szolovits","15","12","2016","Existing models based on artificial neural networks (ANNs) for sentence classification often do not incorporate the context in which sentences appear, and classify sentences individually. However, traditional sentence classification approaches have been shown to greatly benefit from jointly classifying subsequent sentences, such as with conditional random fields. In this work, we present an ANN architecture that combines the effectiveness of typical ANN models to classify sentences in isolation, with the strength of structured prediction. Our model achieves state-of-the-art results on two different datasets for sequential sentence classification in medical abstracts.","http://arxiv.org/pdf/1612.05251v1","cs.CL	cs.AI	cs.NE	stat.ML","Neural Networks for Joint Sentence Classification in Medical Paper   Abstracts",""
"Franck Dernoncourt	Ji Young Lee	Ozlem Uzuner	Peter Szolovits","10","6","2016","Objective: Patient notes in electronic health records (EHRs) may contain critical information for medical investigations. However, the vast majority of medical investigators can only access de-identified notes, in order to protect the confidentiality of patients. In the United States, the Health Insurance Portability and Accountability Act (HIPAA) defines 18 types of protected health information (PHI) that needs to be removed to de-identify patient notes. Manual de-identification is impractical given the size of EHR databases, the limited number of researchers with access to the non-de-identified notes, and the frequent mistakes of human annotators. A reliable automated de-identification system would consequently be of high value.   Materials and Methods: We introduce the first de-identification system based on artificial neural networks (ANNs), which requires no handcrafted features or rules, unlike existing systems. We compare the performance of the system with state-of-the-art systems on two datasets: the i2b2 2014 de-identification challenge dataset, which is the largest publicly available de-identification dataset, and the MIMIC de-identification dataset, which we assembled and is twice as large as the i2b2 2014 dataset.   Results: Our ANN model outperforms the state-of-the-art systems. It yields an F1-score of 97.85 on the i2b2 2014 dataset, with a recall 97.38 and a precision of 97.32, and an F1-score of 99.23 on the MIMIC de-identification dataset, with a recall 99.25 and a precision of 99.06.   Conclusion: Our findings support the use of ANNs for de-identification of patient notes, as they show better performance than previously published systems while requiring no feature engineering.","http://arxiv.org/pdf/1606.03475v1","cs.CL	cs.AI	cs.NE	stat.ML","De-identification of Patient Notes with Recurrent Neural Networks","francky@mit.edu	jjylee@mit.edu	psz@mit.edu	ouzuner@albany.edu"
"Tsendsuren Munkhdalai	Hong Yu","20","10","2016","Hypothesis testing is an important cognitive process that supports human reasoning. In this paper, we introduce a computational hypothesis testing approach based on memory augmented neural networks. Our approach involves a hypothesis testing loop that reconsiders and progressively refines a previously formed hypothesis in order to generate new hypotheses to test. We apply the proposed approach to language comprehension task by using Neural Semantic Encoders (NSE). Our NSE models achieve the state-of-the-art results showing an absolute improvement of 1.2% to 2.6% accuracy over previous results obtained by single and ensemble systems on standard machine comprehension benchmarks such as the Children's Book Test (CBT) and Who-Did-What (WDW) news article datasets.","http://arxiv.org/pdf/1610.06454v2","cs.CL	cs.AI	cs.NE	stat.ML","Reasoning with Memory Augmented Neural Networks for Language   Comprehension","tsendsuren.munkhdalai@umassmed.edu	hong.yu@umassmed.edu"
"W. James Murdoch	Arthur Szlam","8","2","2017","Although deep learning models have proven effective at solving problems in natural language processing, the mechanism by which they come to their conclusions is often unclear. As a result, these models are generally treated as black boxes, yielding no insight of the underlying learned patterns. In this paper we consider Long Short Term Memory networks (LSTMs) and demonstrate a new approach for tracking the importance of a given input to the LSTM for a given output. By identifying consistently important patterns of words, we are able to distill state of the art LSTMs on sentiment analysis and question answering into a set of representative phrases. This representation is then quantitatively validated by using the extracted phrases to construct a simple, rule-based classifier which approximates the output of the LSTM.","http://arxiv.org/pdf/1702.02540v2","cs.CL	cs.AI	cs.NE	stat.ML","Automatic Rule Extraction from Long Short Term Memory Networks","aszlam@fb.com	jmurdoch@berkeley.edu"
"Sebastian Gehrmann	Franck Dernoncourt	Yeran Li	Eric T. Carlson	Joy T. Wu	Jonathan Welt	John Foote Jr.	Edward T. Moseley	David W. Grant	Patrick D. Tyler	Leo Anthony Celi","25","3","2017","Objective: We investigate whether deep learning techniques for natural language processing (NLP) can be used efficiently for patient phenotyping. Patient phenotyping is a classification task for determining whether a patient has a medical condition, and is a crucial part of secondary analysis of healthcare data. We assess the performance of deep learning algorithms and compare them with classical NLP approaches.   Materials and Methods: We compare convolutional neural networks (CNNs), n-gram models, and approaches based on cTAKES that extract pre-defined medical concepts from clinical notes and use them to predict patient phenotypes. The performance is tested on 10 different phenotyping tasks using 1,610 discharge summaries extracted from the MIMIC-III database.   Results: CNNs outperform other phenotyping algorithms in all 10 tasks. The average F1-score of our model is 76 (PPV of 83, and sensitivity of 71) with our model having an F1-score up to 37 points higher than alternative approaches. We additionally assess the interpretability of our model by presenting a method that extracts the most salient phrases for a particular prediction.   Conclusion: We show that NLP methods based on deep learning improve the performance of patient phenotyping. Our CNN-based algorithm automatically learns the phrases associated with each patient phenotype. As such, it reduces the annotation complexity for clinical domain experts, who are normally required to develop task-specific annotation rules and identify relevant phrases. Our method performs well in terms of both performance and interpretability, which indicates that deep learning is an effective approach to patient phenotyping based on clinicians' notes.","http://arxiv.org/pdf/1703.08705v1","cs.CL	cs.AI	cs.NE	stat.ML","Comparing Rule-Based and Deep Learning Models for Patient Phenotyping",""
"Ji Young Lee	Franck Dernoncourt	Peter Szolovits","5","4","2017","Over 50 million scholarly articles have been published: they constitute a unique repository of knowledge. In particular, one may infer from them relations between scientific concepts, such as synonyms and hyponyms. Artificial neural networks have been recently explored for relation extraction. In this work, we continue this line of work and present a system based on a convolutional neural network to extract relations. Our model ranked first in the SemEval-2017 task 10 (ScienceIE) for relation extraction in scientific articles (subtask C).","http://arxiv.org/pdf/1704.01523v1","cs.CL	cs.AI	cs.NE	stat.ML","MIT at SemEval-2017 Task 10: Relation Extraction with Convolutional   Neural Networks","jjylee@mit.edu	francky@mit.edu	psz@mit.edu"
"Ji Young Lee	Franck Dernoncourt	Peter Szolovits","17","5","2017","Recent approaches based on artificial neural networks (ANNs) have shown promising results for named-entity recognition (NER). In order to achieve high performances, ANNs need to be trained on a large labeled dataset. However, labels might be difficult to obtain for the dataset on which the user wants to perform NER: label scarcity is particularly pronounced for patient note de-identification, which is an instance of NER. In this work, we analyze to what extent transfer learning may address this issue. In particular, we demonstrate that transferring an ANN model trained on a large labeled dataset to another dataset with a limited number of labels improves upon the state-of-the-art results on two different datasets for patient note de-identification.","http://arxiv.org/pdf/1705.06273v1","cs.CL	cs.AI	cs.NE	stat.ML","Transfer Learning for Named-Entity Recognition with Neural Networks","jjylee@mit.edu	francky@mit.edu	psz@mit.edu"
"Sai Rajeswar	Sandeep Subramanian	Francis Dutil	Christopher Pal	Aaron Courville","31","5","2017","Generative Adversarial Networks (GANs) have gathered a lot of attention from the computer vision community, yielding impressive results for image generation. Advances in the adversarial generation of natural language from noise however are not commensurate with the progress made in generating images, and still lag far behind likelihood based methods. In this paper, we take a step towards generating natural language with a GAN objective alone. We introduce a simple baseline that addresses the discrete output space problem without relying on gradient estimators and show that it is able to achieve state-of-the-art results on a Chinese poem generation dataset. We present quantitative results on generating sentences from context-free and probabilistic context-free grammars, and qualitative language modeling results. A conditional version is also described that can generate sequences conditioned on sentence characteristics.","http://arxiv.org/pdf/1705.10929v1","cs.CL	cs.AI	cs.NE	stat.ML","Adversarial Generation of Natural Language","sai.rajeswar.mudumba	sandeep.subramanian.1	aaron.courville}@umontreal.ca	frdutil@gmail.com,	christopher.pal@polymtl.ca"
"Leila Arras	Grégoire Montavon	Klaus-Robert Müller	Wojciech Samek","22","6","2017","Recently, a technique called Layer-wise Relevance Propagation (LRP) was shown to deliver insightful explanations in the form of input space relevances for understanding feed-forward neural network classification decisions. In the present work, we extend the usage of LRP to recurrent neural networks. We propose a specific propagation rule applicable to multiplicative connections as they arise in recurrent network architectures such as LSTMs and GRUs. We apply our technique to a word-based bi-directional LSTM model on a five-class sentiment prediction task, and evaluate the resulting LRP relevances both qualitatively and quantitatively, obtaining better results than a gradient-based related method which was used in previous work.","http://arxiv.org/pdf/1706.07206v2","cs.CL	cs.AI	cs.NE	stat.ML","Explaining Recurrent Neural Network Predictions in Sentiment Analysis","leila.arras@hhi.fraunhofer.de	wojciech.samek@hhi.fraunhofer.de"
"Emmanuel Dufourq	Bruce A. Bassett","20","9","2017","Can textual data be compressed intelligently without losing accuracy in evaluating sentiment? In this study, we propose a novel evolutionary compression algorithm, PARSEC (PARts-of-Speech for sEntiment Compression), which makes use of Parts-of-Speech tags to compress text in a way that sacrifices minimal classification accuracy when used in conjunction with sentiment analysis algorithms. An analysis of PARSEC with eight commercial and non-commercial sentiment analysis algorithms on twelve English sentiment data sets reveals that accurate compression is possible with (0%, 1.3%, 3.3%) loss in sentiment classification accuracy for (20%, 50%, 75%) data compression with PARSEC using LingPipe, the most accurate of the sentiment algorithms. Other sentiment analysis algorithms are more severely affected by compression. We conclude that significant compression of text data is possible for sentiment analysis depending on the accuracy demands of the specific application and the specific sentiment analysis algorithm used.","http://arxiv.org/pdf/1709.06990v1","cs.NE	cs.AI	cs.CL	stat.ML","Text Compression for Sentiment Analysis via Evolutionary Algorithms","edufourq@gmail.com	bruce.a.bassett@gmail.com"
"Kartik Audhkhasi	Brian Kingsbury	Bhuvana Ramabhadran	George Saon	Michael Picheny","8","12","2017","Direct acoustics-to-word (A2W) models in the end-to-end paradigm have received increasing attention compared to conventional sub-word based automatic speech recognition models using phones, characters, or context-dependent hidden Markov model states. This is because A2W models recognize words from speech without any decoder, pronunciation lexicon, or externally-trained language model, making training and decoding with such models simple. Prior work has shown that A2W models require orders of magnitude more training data in order to perform comparably to conventional models. Our work also showed this accuracy gap when using the English Switchboard-Fisher data set. This paper describes a recipe to train an A2W model that closes this gap and is at-par with state-of-the-art sub-word based models. We achieve a word error rate of 8.8%/13.9% on the Hub5-2000 Switchboard/CallHome test sets without any decoder or language model. We find that model initialization, training data order, and regularization have the most impact on the A2W model performance. Next, we present a joint word-character A2W model that learns to first spell the word and then recognize it. This model provides a rich output to the user instead of simple word hypotheses, making it especially useful in the case of words unseen or rarely-seen during training.","http://arxiv.org/pdf/1712.03133v1","cs.CL	cs.AI	cs.NE	stat.ML","Building competitive direct acoustics-to-word models for English   conversational speech recognition",""
"Huijuan Xu	Kate Saenko","17","11","2015","We address the problem of Visual Question Answering (VQA), which requires joint image and language understanding to answer a question about a given photograph. Recent approaches have applied deep image captioning methods based on convolutional-recurrent networks to this problem, but have failed to model spatial inference. To remedy this, we propose a model we call the Spatial Memory Network and apply it to the VQA task. Memory networks are recurrent neural networks with an explicit attention mechanism that selects certain parts of the information stored in memory. Our Spatial Memory Network stores neuron activations from different spatial regions of the image in its memory, and uses the question to choose relevant regions for computing the answer, a process of which constitutes a single ""hop"" in the network. We propose a novel spatial attention architecture that aligns words with image patches in the first hop, and obtain improved results by adding a second attention hop which considers the whole question to choose visual evidence based on the results of the first hop. To better understand the inference process learned by the network, we design synthetic questions that specifically require spatial inference and visualize the attention weights. We evaluate our model on two published visual question answering datasets, DAQUAR [1] and VQA [2], and obtain improved results compared to a strong deep baseline model (iBOWIMG) which concatenates image and question features to predict the answer [3].","http://arxiv.org/pdf/1511.05234v2","cs.CV	cs.AI	cs.CL	cs.NE","Ask, Attend and Answer: Exploring Question-Guided Spatial Attention for   Visual Question Answering","hxu1@cs.uml.edu,	saenko@cs.uml.edu"
"Yuetan Lin	Zhangyang Pang	Donghui Wang	Yueting Zhuang","22","2","2017","Visual question answering (VQA) has witnessed great progress since May, 2015 as a classic problem unifying visual and textual data into a system. Many enlightening VQA works explore deep into the image and question encodings and fusing methods, of which attention is the most effective and infusive mechanism. Current attention based methods focus on adequate fusion of visual and textual features, but lack the attention to where people focus to ask questions about the image. Traditional attention based methods attach a single value to the feature at each spatial location, which losses many useful information. To remedy these problems, we propose a general method to perform saliency-like pre-selection on overlapped region features by the interrelation of bidirectional LSTM (BiLSTM), and use a novel element-wise multiplication based attention method to capture more competent correlation information between visual and textual features. We conduct experiments on the large-scale COCO-VQA dataset and analyze the effectiveness of our model demonstrated by strong empirical results.","http://arxiv.org/pdf/1702.06700v1","cs.CV	cs.AI	cs.CL	cs.NE","Task-driven Visual Saliency and Attention-based Visual Question   Answering","linyuetan@zju.edu.cn	pzy@zju.edu.cn	dhwang@zju.edu.cn	yzhuang@zju.edu.cn"
"Akash Kumar Dhaka	Giampiero Salvi","29","6","2016","We present a systematic analysis on the performance of a phonetic recogniser when the window of input features is not symmetric with respect to the current frame. The recogniser is based on Context Dependent Deep Neural Networks (CD-DNNs) and Hidden Markov Models (HMMs). The objective is to reduce the latency of the system by reducing the number of future feature frames required to estimate the current output. Our tests performed on the TIMIT database show that the performance does not degrade when the input window is shifted up to 5 frames in the past compared to common practice (no future frame). This corresponds to improving the latency by 50 ms in our settings. Our tests also show that the best results are not obtained with the symmetric window commonly employed, but with an asymmetric window with eight past and two future context frames, although this observation should be confirmed on other data sets. The reduction in latency suggested by our results is critical for specific applications such as real-time lip synchronisation for tele-presence, but may also be beneficial in general applications to improve the lag in human-machine spoken interaction.","http://arxiv.org/pdf/1606.09163v1","cs.CL	cs.CV	cs.NE	stat.ML","Optimising The Input Window Alignment in CD-DNN Based Phoneme   Recognition for Low Latency Processing","akashd@kth.se	giampi@kth.se"
"Peng Qian	Xipeng Qiu	Xuanjing Huang","22","4","2016","Recently, the long short-term memory neural network (LSTM) has attracted wide interest due to its success in many tasks. LSTM architecture consists of a memory cell and three gates, which looks similar to the neuronal networks in the brain. However, there still lacks the evidence of the cognitive plausibility of LSTM architecture as well as its working mechanism. In this paper, we study the cognitive plausibility of LSTM by aligning its internal architecture with the brain activity observed via fMRI when the subjects read a story. Experiment results show that the artificial memory vector in LSTM can accurately predict the observed sequential brain activities, indicating the correlation between LSTM architecture and the cognitive process of story reading.","http://arxiv.org/pdf/1604.06635v1","cs.CL	cs.AI	cs.LG	cs.NE","Bridging LSTM Architecture and the Neural Dynamics during Reading","pqian11@fudan.edu.cn	xpqiu@fudan.edu.cn	xjhuang@fudan.edu.cn"
"Jiwei Li","11","12","2014","This paper addresses how a recursive neural network model can automatically leave out useless information and emphasize important evidence, in other words, to perform ""weight tuning"" for higher-level representation acquisition. We propose two models, Weighted Neural Network (WNN) and Binary-Expectation Neural Network (BENN), which automatically control how much one specific unit contributes to the higher-level representation. The proposed model can be viewed as incorporating a more powerful compositional function for embedding acquisition in recursive neural networks. Experimental results demonstrate the significant improvement over standard neural models.","http://arxiv.org/pdf/1412.3714v2","cs.NE	cs.AI	cs.CL	cs.LG","Feature Weight Tuning for Recursive Neural Networks","jiweil@stanford.edu"
"Sadikin Mujiono	Mohamad Ivan Fanany	Chan Basaruddin","6","10","2016","One essential task in information extraction from the medical corpus is drug name recognition. Compared with text sources come from other domains, the medical text is special and has unique characteristics. In addition, the medical text mining poses more challenges, e.g., more unstructured text, the fast growing of new terms addition, a wide range of name variation for the same drug. The mining is even more challenging due to the lack of labeled dataset sources and external knowledge, as well as multiple token representations for a single drug name that is more common in the real application setting. Although many approaches have been proposed to overwhelm the task, some problems remained with poor F-score performance (less than 0.75). This paper presents a new treatment in data representation techniques to overcome some of those challenges. We propose three data representation techniques based on the characteristics of word distribution and word similarities as a result of word embedding training. The first technique is evaluated with the standard NN model, i.e., MLP (Multi-Layer Perceptrons). The second technique involves two deep network classifiers, i.e., DBN (Deep Belief Networks), and SAE (Stacked Denoising Encoders). The third technique represents the sentence as a sequence that is evaluated with a recurrent NN model, i.e., LSTM (Long Short Term Memory). In extracting the drug name entities, the third technique gives the best F-score performance compared to the state of the art, with its average F-score being 0.8645.","http://arxiv.org/pdf/1610.01891v1","cs.CL	cs.AI	cs.LG	cs.NE	68Txx	I.2.4","A New Data Representation Based on Training Data Characteristics to   Extract Drug Named-Entity in Medical Text","mujiono.sadikin@mercubuana.ac.id"
"Eric Malmi	Pyry Takala	Hannu Toivonen	Tapani Raiko	Aristides Gionis","18","5","2015","Writing rap lyrics requires both creativity to construct a meaningful, interesting story and lyrical skills to produce complex rhyme patterns, which form the cornerstone of good flow. We present a rap lyrics generation method that captures both of these aspects. First, we develop a prediction model to identify the next line of existing lyrics from a set of candidate next lines. This model is based on two machine-learning techniques: the RankSVM algorithm and a deep neural network model with a novel structure. Results show that the prediction model can identify the true next line among 299 randomly selected lines with an accuracy of 17%, i.e., over 50 times more likely than by random. Second, we employ the prediction model to combine lines from existing songs, producing lyrics with rhyme and a meaning. An evaluation of the produced lyrics shows that in terms of quantitative rhyme density, the method outperforms the best human rappers by 21%. The rap lyrics generator has been deployed as an online tool called DeepBeat, and the performance of the tool has been assessed by analyzing its usage logs. This analysis shows that machine-learned rankings correlate with user preferences.","http://arxiv.org/pdf/1505.04771v2","cs.LG	cs.AI	cs.CL	cs.NE	I.2.7; H.3.3","DopeLearning: A Computational Approach to Rap Lyrics Generation","eric.malmi@aalto.fi	pyry.takala@aalto.fi	hannu.toivonen@cs.helsinki.fi	tapani.raiko@aalto.fi	aristides.gionis@aalto.fi"
"Shengxian Wan	Yanyan Lan	Jun Xu	Jiafeng Guo	Liang Pang	Xueqi Cheng","15","4","2016","Semantic matching, which aims to determine the matching degree between two texts, is a fundamental problem for many NLP applications. Recently, deep learning approach has been applied to this problem and significant improvements have been achieved. In this paper, we propose to view the generation of the global interaction between two texts as a recursive process: i.e. the interaction of two texts at each position is a composition of the interactions between their prefixes as well as the word level interaction at the current position. Based on this idea, we propose a novel deep architecture, namely Match-SRNN, to model the recursive matching structure. Firstly, a tensor is constructed to capture the word level interactions. Then a spatial RNN is applied to integrate the local interactions recursively, with importance determined by four types of gates. Finally, the matching score is calculated based on the global interaction. We show that, after degenerated to the exact matching scenario, Match-SRNN can approximate the dynamic programming process of longest common subsequence. Thus, there exists a clear interpretation for Match-SRNN. Our experiments on two semantic matching tasks showed the effectiveness of Match-SRNN, and its ability of visualizing the learned matching structure.","http://arxiv.org/pdf/1604.04378v1","cs.CL	cs.AI	cs.LG	cs.NE","Match-SRNN: Modeling the Recursive Matching Structure with Spatial RNN","wanshengxian	pangliang}@software.ict.ac.cn	lanyanyan@ict.ac.cn	junxu@ict.ac.cn	guojiafeng@ict.ac.cn	cxq@ict.ac.cn"
"Iulian V. Serban	Alexander G. Ororbia II	Joelle Pineau	Aaron Courville","1","12","2016","Advances in neural variational inference have facilitated the learning of powerful directed graphical models with continuous latent variables, such as variational autoencoders. The hope is that such models will learn to represent rich, multi-modal latent factors in real-world data, such as natural language text. However, current models often assume simplistic priors on the latent variables - such as the uni-modal Gaussian distribution - which are incapable of representing complex latent factors efficiently. To overcome this restriction, we propose the simple, but highly flexible, piecewise constant distribution. This distribution has the capacity to represent an exponential number of modes of a latent target distribution, while remaining mathematically tractable. Our results demonstrate that incorporating this new latent distribution into different models yields substantial improvements in natural language processing tasks such as document modeling and natural language generation for dialogue.","http://arxiv.org/pdf/1612.00377v4","cs.CL	cs.AI	cs.LG	cs.NE	I.5.1; I.2.7","Piecewise Latent Variables for Neural Variational Text Processing",""
"Baolin Peng	Kaisheng Yao","31","5","2015","Recurrent Neural Networks (RNNs) have become increasingly popular for the task of language understanding. In this task, a semantic tagger is deployed to associate a semantic label to each word in an input sequence. The success of RNN may be attributed to its ability to memorize long-term dependence that relates the current-time semantic label prediction to the observations many time instances away. However, the memory capacity of simple RNNs is limited because of the gradient vanishing and exploding problem. We propose to use an external memory to improve memorization capability of RNNs. We conducted experiments on the ATIS dataset, and observed that the proposed model was able to achieve the state-of-the-art results. We compare our proposed model with alternative models and report analysis results that may provide insights for future research.","http://arxiv.org/pdf/1506.00195v1","cs.CL	cs.AI	cs.LG	cs.NE","Recurrent Neural Networks with External Memory for Language   Understanding","blpeng@se.cuhk.edu.hk,	kaisheny@microsoft.com"
"Alessandro Sordoni	Michel Galley	Michael Auli	Chris Brockett	Yangfeng Ji	Margaret Mitchell	Jian-Yun Nie	Jianfeng Gao	Bill Dolan","22","6","2015","We present a novel response generation system that can be trained end to end on large quantities of unstructured Twitter conversations. A neural network architecture is used to address sparsity issues that arise when integrating contextual information into classic statistical models, allowing the system to take into account previous dialog utterances. Our dynamic-context generative models show consistent gains over both context-sensitive and non-context-sensitive Machine Translation and Information Retrieval baselines.","http://arxiv.org/pdf/1506.06714v1","cs.CL	cs.AI	cs.LG	cs.NE","A Neural Network Approach to Context-Sensitive Generation of   Conversational Responses",""
"Ryan Lowe	Nissan Pow	Iulian Serban	Joelle Pineau","30","6","2015","This paper introduces the Ubuntu Dialogue Corpus, a dataset containing almost 1 million multi-turn dialogues, with a total of over 7 million utterances and 100 million words. This provides a unique resource for research into building dialogue managers based on neural language models that can make use of large amounts of unlabeled data. The dataset has both the multi-turn property of conversations in the Dialog State Tracking Challenge datasets, and the unstructured nature of interactions from microblog services such as Twitter. We also describe two neural learning architectures suitable for analyzing this dataset, and provide benchmark performance on the task of selecting the best next response.","http://arxiv.org/pdf/1506.08909v3","cs.CL	cs.AI	cs.LG	cs.NE","The Ubuntu Dialogue Corpus: A Large Dataset for Research in Unstructured   Multi-Turn Dialogue Systems",""
"Iulian V. Serban	Alessandro Sordoni	Yoshua Bengio	Aaron Courville	Joelle Pineau","17","7","2015","We investigate the task of building open domain, conversational dialogue systems based on large dialogue corpora using generative models. Generative models produce system responses that are autonomously generated word-by-word, opening up the possibility for realistic, flexible interactions. In support of this goal, we extend the recently proposed hierarchical recurrent encoder-decoder neural network to the dialogue domain, and demonstrate that this model is competitive with state-of-the-art neural language models and back-off n-gram models. We investigate the limitations of this and similar approaches, and show how its performance can be improved by bootstrapping the learning from a larger question-answer pair corpus and from pretrained word embeddings.","http://arxiv.org/pdf/1507.04808v3","cs.CL	cs.AI	cs.LG	cs.NE	I.5.1; I.2.7","Building End-To-End Dialogue Systems Using Generative Hierarchical   Neural Network Models",""
"Dzmitry Bahdanau	Jan Chorowski	Dmitriy Serdyuk	Philemon Brakel	Yoshua Bengio","18","8","2015","Many of the current state-of-the-art Large Vocabulary Continuous Speech Recognition Systems (LVCSR) are hybrids of neural networks and Hidden Markov Models (HMMs). Most of these systems contain separate components that deal with the acoustic modelling, language modelling and sequence decoding. We investigate a more direct approach in which the HMM is replaced with a Recurrent Neural Network (RNN) that performs sequence prediction directly at the character level. Alignment between the input features and the desired character sequence is learned automatically by an attention mechanism built into the RNN. For each predicted character, the attention mechanism scans the input sequence and chooses relevant frames. We propose two methods to speed up this operation: limiting the scan to a subset of most promising frames and pooling over time the information contained in neighboring frames, thereby reducing source sequence length. Integrating an n-gram language model into the decoding process yields recognition accuracies similar to other HMM-free RNN-based approaches.","http://arxiv.org/pdf/1508.04395v2","cs.CL	cs.AI	cs.LG	cs.NE","End-to-End Attention-based Large Vocabulary Speech Recognition",""
"Baolin Peng	Zhengdong Lu	Hang Li	Kam-Fai Wong","22","8","2015","We propose Neural Reasoner, a framework for neural network-based reasoning over natural language sentences. Given a question, Neural Reasoner can infer over multiple supporting facts and find an answer to the question in specific forms. Neural Reasoner has 1) a specific interaction-pooling mechanism, allowing it to examine multiple facts, and 2) a deep architecture, allowing it to model the complicated logical relations in reasoning tasks. Assuming no particular structure exists in the question and facts, Neural Reasoner is able to accommodate different types of reasoning and different forms of language expressions. Despite the model complexity, Neural Reasoner can still be trained effectively in an end-to-end manner. Our empirical studies show that Neural Reasoner can outperform existing neural reasoning systems with remarkable margins on two difficult artificial tasks (Positional Reasoning and Path Finding) proposed in [8]. For example, it improves the accuracy on Path Finding(10K) from 33.4% [6] to over 98%.","http://arxiv.org/pdf/1508.05508v1","cs.AI	cs.CL	cs.LG	cs.NE","Towards Neural Network-based Reasoning","blpeng@se.cuhk.edu.hk	kfwong@se.cuhk.edu.hk	Lu.Zhengdong@huawei.com	HangLi.HL@huawei.com"
"Hongyuan Mei	Mohit Bansal	Matthew R. Walter","2","9","2015","We propose an end-to-end, domain-independent neural encoder-aligner-decoder model for selective generation, i.e., the joint task of content selection and surface realization. Our model first encodes a full set of over-determined database event records via an LSTM-based recurrent neural network, then utilizes a novel coarse-to-fine aligner to identify the small subset of salient records to talk about, and finally employs a decoder to generate free-form descriptions of the aligned, selected records. Our model achieves the best selection and generation results reported to-date (with 59% relative improvement in generation) on the benchmark WeatherGov dataset, despite using no specialized features or linguistic resources. Using an improved k-nearest neighbor beam filter helps further. We also perform a series of ablations and visualizations to elucidate the contributions of our key model components. Lastly, we evaluate the generalizability of our model on the RoboCup dataset, and get results that are competitive with or better than the state-of-the-art, despite being severely data-starved.","http://arxiv.org/pdf/1509.00838v2","cs.CL	cs.AI	cs.LG	cs.NE","What to talk about and how? Selective Generation using LSTMs with   Coarse-to-Fine Alignment","hongyuan@ttic.edu	mbansal@ttic.edu	mwalter@ttic.edu"
"Tim Rocktäschel	Edward Grefenstette	Karl Moritz Hermann	Tomáš Kočiský	Phil Blunsom","22","9","2015","While most approaches to automatically recognizing entailment relations have used classifiers employing hand engineered features derived from complex natural language processing pipelines, in practice their performance has been only slightly better than bag-of-word pair classifiers using only lexical similarity. The only attempt so far to build an end-to-end differentiable neural network for entailment failed to outperform such a simple similarity classifier. In this paper, we propose a neural model that reads two sentences to determine entailment using long short-term memory units. We extend this model with a word-by-word neural attention mechanism that encourages reasoning over entailments of pairs of words and phrases. Furthermore, we present a qualitative analysis of attention weights produced by this model, demonstrating such reasoning capabilities. On a large entailment dataset this model outperforms the previous best neural model and a classifier with engineered features by a substantial margin. It is the first generic end-to-end differentiable system that achieves state-of-the-art accuracy on a textual entailment dataset.","http://arxiv.org/pdf/1509.06664v4","cs.CL	cs.AI	cs.LG	cs.NE	68T50	I.2.6; I.2.7","Reasoning about Entailment with Neural Attention","t.rocktaschel@cs.ucl.ac.uk	etg@google.com	kmh@google.com	tkocisky@google.com	pblunsom@google.com"
"Yu Zhang	Guoguo Chen	Dong Yu	Kaisheng Yao	Sanjeev Khudanpur	James Glass","30","10","2015","In this paper, we extend the deep long short-term memory (DLSTM) recurrent neural networks by introducing gated direct connections between memory cells in adjacent layers. These direct links, called highway connections, enable unimpeded information flow across different layers and thus alleviate the gradient vanishing problem when building deeper LSTMs. We further introduce the latency-controlled bidirectional LSTMs (BLSTMs) which can exploit the whole history while keeping the latency under control. Efficient algorithms are proposed to train these novel networks using both frame and sequence discriminative criteria. Experiments on the AMI distant speech recognition (DSR) task indicate that we can train deeper LSTMs and achieve better improvement from sequence training with highway LSTMs (HLSTMs). Our novel model obtains $43.9/47.7\%$ WER on AMI (SDM) dev and eval sets, outperforming all previous works. It beats the strong DNN and DLSTM baselines with $15.7\%$ and $5.3\%$ relative improvement respectively.","http://arxiv.org/pdf/1510.08983v2","cs.NE	cs.AI	cs.CL	cs.LG","Highway Long Short-Term Memory RNNs for Distant Speech Recognition","yzhang87	glass}@mit.edu	guoguo	khudanpur}@jhu.edu	dongyu@microsoft.com	Kaisheng.YAO@microsoft.com"
"Pengcheng Yin	Zhengdong Lu	Hang Li	Ben Kao","3","12","2015","We proposed Neural Enquirer as a neural network architecture to execute a natural language (NL) query on a knowledge-base (KB) for answers. Basically, Neural Enquirer finds the distributed representation of a query and then executes it on knowledge-base tables to obtain the answer as one of the values in the tables. Unlike similar efforts in end-to-end training of semantic parsers, Neural Enquirer is fully ""neuralized"": it not only gives distributional representation of the query and the knowledge-base, but also realizes the execution of compositional queries as a series of differentiable operations, with intermediate results (consisting of annotations of the tables at different levels) saved on multiple layers of memory. Neural Enquirer can be trained with gradient descent, with which not only the parameters of the controlling components and semantic parsing component, but also the embeddings of the tables and query words can be learned from scratch. The training can be done in an end-to-end fashion, but it can take stronger guidance, e.g., the step-by-step supervision for complicated queries, and benefit from it. Neural Enquirer is one step towards building neural network systems which seek to understand language by executing it on real-world. Our experiments show that Neural Enquirer can learn to execute fairly complicated NL queries on tables with rich structures.","http://arxiv.org/pdf/1512.00965v2","cs.AI	cs.CL	cs.LG	cs.NE","Neural Enquirer: Learning to Query Tables with Natural Language","pcyin@cs.hku.hk	kao@cs.hku.hk	Lu.Zhengdong@huawei.com	HangLi.HL@huawei.com"
"Petr Baudiš	Jan Pichl	Tomáš Vyskočil	Jan Šedivý","19","3","2016","We review the task of Sentence Pair Scoring, popular in the literature in various forms - viewed as Answer Sentence Selection, Semantic Text Scoring, Next Utterance Ranking, Recognizing Textual Entailment, Paraphrasing or e.g. a component of Memory Networks.   We argue that all such tasks are similar from the model perspective and propose new baselines by comparing the performance of common IR metrics and popular convolutional, recurrent and attention-based neural models across many Sentence Pair Scoring tasks and datasets. We discuss the problem of evaluating randomized models, propose a statistically grounded methodology, and attempt to improve comparisons by releasing new datasets that are much harder than some of the currently used well explored benchmarks. We introduce a unified open source software framework with easily pluggable models and tasks, which enables us to experiment with multi-task reusability of trained sentence model. We set a new state-of-art in performance on the Ubuntu Dialogue dataset.","http://arxiv.org/pdf/1603.06127v4","cs.CL	cs.AI	cs.LG	cs.NE","Sentence Pair Scoring: Towards Unified Framework for Text Comprehension","baudipet@fel.cvut.cz"
"Jiatao Gu	Zhengdong Lu	Hang Li	Victor O. K. Li","21","3","2016","We address an important problem in sequence-to-sequence (Seq2Seq) learning referred to as copying, in which certain segments in the input sequence are selectively replicated in the output sequence. A similar phenomenon is observable in human language communication. For example, humans tend to repeat entity names or even long phrases in conversation. The challenge with regard to copying in Seq2Seq is that new machinery is needed to decide when to perform the operation. In this paper, we incorporate copying into neural network-based Seq2Seq learning and propose a new model called CopyNet with encoder-decoder structure. CopyNet can nicely integrate the regular way of word generation in the decoder with the new copying mechanism which can choose sub-sequences in the input sequence and put them at proper places in the output sequence. Our empirical study on both synthetic data sets and real world data sets demonstrates the efficacy of CopyNet. For example, CopyNet can outperform regular RNN-based model with remarkable margins on text summarization tasks.","http://arxiv.org/pdf/1603.06393v3","cs.CL	cs.AI	cs.LG	cs.NE","Incorporating Copying Mechanism in Sequence-to-Sequence Learning","jiataogu@eee.hku.hk	vli@eee.hku.hk	lu.zhengdong@huawei.com	hangli.hl@huawei.com"
"Iulian Vlad Serban	Alberto García-Durán	Caglar Gulcehre	Sungjin Ahn	Sarath Chandar	Aaron Courville	Yoshua Bengio","22","3","2016","Over the past decade, large-scale supervised learning corpora have enabled machine learning researchers to make substantial advances. However, to this date, there are no large-scale question-answer corpora available. In this paper we present the 30M Factoid Question-Answer Corpus, an enormous question answer pair corpus produced by applying a novel neural network architecture on the knowledge base Freebase to transduce facts into natural language questions. The produced question answer pairs are evaluated both by human evaluators and using automatic evaluation metrics, including well-established machine translation and sentence similarity metrics. Across all evaluation criteria the question-generation model outperforms the competing template-based baseline. Furthermore, when presented to human evaluators, the generated questions appear comparable in quality to real human-generated questions.","http://arxiv.org/pdf/1603.06807v2","cs.CL	cs.AI	cs.LG	cs.NE	H.3.4; I.5.1; I.2.6; I.2.7","Generating Factoid Questions With Recurrent Neural Networks: The 30M   Factoid Question-Answer Corpus",""
"Chia-Wei Liu	Ryan Lowe	Iulian V. Serban	Michael Noseworthy	Laurent Charlin	Joelle Pineau","25","3","2016","We investigate evaluation metrics for dialogue response generation systems where supervised labels, such as task completion, are not available. Recent works in response generation have adopted metrics from machine translation to compare a model's generated response to a single target response. We show that these metrics correlate very weakly with human judgements in the non-technical Twitter domain, and not at all in the technical Ubuntu domain. We provide quantitative and qualitative results highlighting specific weaknesses in existing metrics, and provide recommendations for future development of better automatic evaluation metrics for dialogue systems.","http://arxiv.org/pdf/1603.08023v2","cs.CL	cs.AI	cs.LG	cs.NE","How NOT To Evaluate Your Dialogue System: An Empirical Study of   Unsupervised Evaluation Metrics for Dialogue Response Generation","chia-wei.liu@mail.mcgill.ca	ryan.lowe@mail.mcgill.ca	michael.noseworthy@mail.mcgill.ca	lcharlin@cs.mcgill.ca	jpineau@cs.mcgill.ca	iulian.vlad.serban@umontreal.ca"
"Iulian Vlad Serban	Alessandro Sordoni	Ryan Lowe	Laurent Charlin	Joelle Pineau	Aaron Courville	Yoshua Bengio","19","5","2016","Sequential data often possesses a hierarchical structure with complex dependencies between subsequences, such as found between the utterances in a dialogue. In an effort to model this kind of generative process, we propose a neural network-based generative architecture, with latent stochastic variables that span a variable number of time steps. We apply the proposed model to the task of dialogue response generation and compare it with recent neural network architectures. We evaluate the model performance through automatic evaluation metrics and by carrying out a human evaluation. The experiments demonstrate that our model improves upon recently proposed models and that the latent variables facilitate the generation of long outputs and maintain the context.","http://arxiv.org/pdf/1605.06069v3","cs.CL	cs.AI	cs.LG	cs.NE	I.5.1; I.2.7","A Hierarchical Latent Variable Encoder-Decoder Model for Generating   Dialogues","iulian.vlad.serban@umontreal.ca	aaron.courville@umontreal.ca	yoshua.bengio@umontreal.ca	alessandro.sordoni@maluuba.com	ryan.lowe@cs.mcgill.ca	lcharlin@cs.mcgill.ca	jpineau@cs.mcgill.ca"
"Dirk Weissenborn","13","6","2016","Many important NLP problems can be posed as dual-sequence or sequence-to-sequence modeling tasks. Recent advances in building end-to-end neural architectures have been highly successful in solving such tasks. In this work we propose a new architecture for dual-sequence modeling that is based on associative memory. We derive AM-RNNs, a recurrent associative memory (AM) which augments generic recurrent neural networks (RNN). This architecture is extended to the Dual AM-RNN which operates on two AMs at once. Our models achieve very competitive results on textual entailment. A qualitative analysis demonstrates that long range dependencies between source and target-sequence can be bridged effectively using Dual AM-RNNs. However, an initial experiment on auto-encoding reveals that these benefits are not exploited by the system when learning to solve sequence-to-sequence tasks which indicates that additional supervision or regularization is needed.","http://arxiv.org/pdf/1606.03864v2","cs.NE	cs.AI	cs.CL	cs.LG","Neural Associative Memory for Dual-Sequence Modeling","dirk.weissenborn@dfki.de"
"Marc Dymetman	Chunyang Xiao","8","7","2016","We introduce LL-RNNs (Log-Linear RNNs), an extension of Recurrent Neural Networks that replaces the softmax output layer by a log-linear output layer, of which the softmax is a special case. This conceptually simple move has two main advantages. First, it allows the learner to combat training data sparsity by allowing it to model words (or more generally, output symbols) as complex combinations of attributes without requiring that each combination is directly observed in the training data (as the softmax does). Second, it permits the inclusion of flexible prior knowledge in the form of a priori specified modular features, where the neural network component learns to dynamically control the weights of a log-linear distribution exploiting these features.   We conduct experiments in the domain of language modelling of French, that exploit morphological prior knowledge and show an important decrease in perplexity relative to a baseline RNN.   We provide other motivating iillustrations, and finally argue that the log-linear and the neural-network components contribute complementary strengths to the LL-RNN: the LL aspect allows the model to incorporate rich prior knowledge, while the NN aspect, according to the ""representation learning"" paradigm, allows the model to discover novel combination of characteristics.","http://arxiv.org/pdf/1607.02467v2","cs.AI	cs.CL	cs.LG	cs.NE","Log-Linear RNNs: Towards Recurrent Neural Networks with Flexible Prior   Knowledge","marc.dymetman@xrce.xerox.com	chunyang.xiao@xrce.xerox.com"
"Ondrej Bajgar	Rudolf Kadlec	Jan Kleindienst","4","10","2016","There is a practically unlimited amount of natural language data available. Still, recent work in text comprehension has focused on datasets which are small relative to current computing possibilities. This article is making a case for the community to move to larger data and as a step in that direction it is proposing the BookTest, a new dataset similar to the popular Children's Book Test (CBT), however more than 60 times larger. We show that training on the new data improves the accuracy of our Attention-Sum Reader model on the original CBT test data by a much larger margin than many recent attempts to improve the model architecture. On one version of the dataset our ensemble even exceeds the human baseline provided by Facebook. We then show in our own human study that there is still space for further improvement.","http://arxiv.org/pdf/1610.00956v1","cs.CL	cs.AI	cs.LG	cs.NE","Embracing data abundance: BookTest Dataset for Reading Comprehension","obajgar@cz.ibm.com	rudolf kadlec@cz.ibm.com	jankle@cz.ibm.com"
"James Bradbury	Stephen Merity	Caiming Xiong	Richard Socher","5","11","2016","Recurrent neural networks are a powerful tool for modeling sequential data, but the dependence of each timestep's computation on the previous timestep's output limits parallelism and makes RNNs unwieldy for very long sequences. We introduce quasi-recurrent neural networks (QRNNs), an approach to neural sequence modeling that alternates convolutional layers, which apply in parallel across timesteps, and a minimalist recurrent pooling function that applies in parallel across channels. Despite lacking trainable recurrent layers, stacked QRNNs have better predictive accuracy than stacked LSTMs of the same hidden size. Due to their increased parallelism, they are up to 16 times faster at train and test time. Experiments on language modeling, sentiment classification, and character-level neural machine translation demonstrate these advantages and underline the viability of QRNNs as a basic building block for a variety of sequence tasks.","http://arxiv.org/pdf/1611.01576v2","cs.NE	cs.AI	cs.CL	cs.LG","Quasi-Recurrent Neural Networks","james.bradbury@salesforce.com	smerity@salesforce.com	cxiong@salesforce.com	rsocher@salesforce.com"
"Jakob N. Foerster	Justin Gilmer	Jan Chorowski	Jascha Sohl-Dickstein	David Sussillo","28","11","2016","There exist many problem domains where the interpretability of neural network models is essential for deployment. Here we introduce a recurrent architecture composed of input-switched affine transformations - in other words an RNN without any explicit nonlinearities, but with input-dependent recurrent weights. This simple form allows the RNN to be analyzed via straightforward linear methods: we can exactly characterize the linear contribution of each input to the model predictions; we can use a change-of-basis to disentangle input, output, and computational hidden unit subspaces; we can fully reverse-engineer the architecture's solution to a simple task. Despite this ease of interpretation, the input switched affine network achieves reasonable performance on a text modeling tasks, and allows greater computational efficiency than networks with standard nonlinearities.","http://arxiv.org/pdf/1611.09434v2","cs.AI	cs.CL	cs.LG	cs.NE","Input Switched Affine Networks: An RNN Architecture Designed for   Interpretability",""
"Michał Daniluk	Tim Rocktäschel	Johannes Welbl	Sebastian Riedel","15","2","2017","Neural language models predict the next token using a latent representation of the immediate token history. Recently, various methods for augmenting neural language models with an attention mechanism over a differentiable memory have been proposed. For predicting the next token, these models query information from a memory of the recent history which can facilitate learning mid- and long-range dependencies. However, conventional attention mechanisms used in memory-augmented neural language models produce a single output vector per time step. This vector is used both for predicting the next token as well as for the key and value of a differentiable memory of a token history. In this paper, we propose a neural language model with a key-value attention mechanism that outputs separate representations for the key and value of a differentiable memory, as well as for encoding the next-word distribution. This model outperforms existing memory-augmented neural language models on two corpora. Yet, we found that our method mainly utilizes a memory of the five most recent output representations. This led to the unexpected main finding that a much simpler model based only on the concatenation of recent output representations from previous time steps is on par with more sophisticated memory-augmented neural language models.","http://arxiv.org/pdf/1702.04521v1","cs.CL	cs.AI	cs.LG	cs.NE","Frustratingly Short Attention Spans in Neural Language Modeling","michal.daniluk.15@ucl.ac.uk,	t.rocktaschel@cs.ucl.ac.uk	j.welbl@cs.ucl.ac.uk	s.riedel@cs.ucl.ac.uk"
"Zhouhan Lin	Minwei Feng	Cicero Nogueira dos Santos	Mo Yu	Bing Xiang	Bowen Zhou	Yoshua Bengio","9","3","2017","This paper proposes a new model for extracting an interpretable sentence embedding by introducing self-attention. Instead of using a vector, we use a 2-D matrix to represent the embedding, with each row of the matrix attending on a different part of the sentence. We also propose a self-attention mechanism and a special regularization term for the model. As a side effect, the embedding comes with an easy way of visualizing what specific parts of the sentence are encoded into the embedding. We evaluate our model on 3 different tasks: author profiling, sentiment classification, and textual entailment. Results show that our model yields a significant performance gain compared to other sentence embedding methods in all of the 3 tasks.","http://arxiv.org/pdf/1703.03130v1","cs.CL	cs.AI	cs.LG	cs.NE","A Structured Self-attentive Sentence Embedding","lin.zhouhan@gmail.com	mfeng@us.ibm.com	cicerons@us.ibm.com	yum@us.ibm.com	bingxia@us.ibm.com	zhou@us.ibm.com"
"Samuel Rönnqvist	Niko Schenk	Christian Chiarcos","26","4","2017","We introduce an attention-based Bi-LSTM for Chinese implicit discourse relations and demonstrate that modeling argument pairs as a joint sequence can outperform word order-agnostic approaches. Our model benefits from a partial sampling scheme and is conceptually simple, yet achieves state-of-the-art performance on the Chinese Discourse Treebank. We also visualize its attention activity to illustrate the model's ability to selectively focus on the relevant parts of an input sequence.","http://arxiv.org/pdf/1704.08092v1","cs.CL	cs.AI	cs.LG	cs.NE","A Recurrent Neural Model with Attention for the Recognition of Chinese   Implicit Discourse Relations","sronnqvi@abo.fi	schenk@informatik.uni-frankfurt.de	chiarcos@informatik.uni-frankfurt.de"
"Lara J. Martin	Prithviraj Ammanabrolu	Xinyu Wang	William Hancock	Shruti Singh	Brent Harrison	Mark O. Riedl","5","6","2017","Automated story generation is the problem of automatically selecting a sequence of events, actions, or words that can be told as a story. We seek to develop a system that can generate stories by learning everything it needs to know from textual story corpora. To date, recurrent neural networks that learn language models at character, word, or sentence levels have had little success generating coherent stories. We explore the question of event representations that provide a mid-level of abstraction between words and sentences in order to retain the semantic information of the original data while minimizing event sparsity. We present a technique for preprocessing textual story data into event sequences. We then present a technique for automated story generation whereby we decompose the problem into the generation of successive events (event2event) and the generation of natural language sentences from events (event2sentence). We give empirical results comparing different event representations and their effects on event successor generation and the translation of events to natural language.","http://arxiv.org/pdf/1706.01331v3","cs.CL	cs.AI	cs.LG	cs.NE","Event Representations for Automated Story Generation with Deep Neural   Nets","riedl]@gatech.edu"
"Tong Wang	Xingdi Yuan	Adam Trischler","5","6","2017","We propose a generative machine comprehension model that learns jointly to ask and answer questions based on documents. The proposed model uses a sequence-to-sequence framework that encodes the document and generates a question (answer) given an answer (question). Significant improvement in model performance is observed empirically on the SQuAD corpus, confirming our hypothesis that the model benefits from jointly learning to perform both tasks. We believe the joint model's novelty offers a new perspective on machine comprehension beyond architectural engineering, and serves as a first step towards autonomous information seeking.","http://arxiv.org/pdf/1706.01450v1","cs.CL	cs.AI	cs.LG	cs.NE","A Joint Model for Question Answering and Question Generation",""
"Wei Wen	Yuxiong He	Samyam Rajbhandari	Minjia Zhang	Wenhan Wang	Fang Liu	Bin Hu	Yiran Chen	Hai Li","15","9","2017","Model compression is significant for the wide adoption of Recurrent Neural Networks (RNNs) in both user devices possessing limited resources and business clusters requiring quick responses to large-scale service requests. This work aims to learn structurally-sparse Long Short-Term Memory (LSTM) by reducing the sizes of basic structures within LSTM units, including input updates, gates, hidden states, cell states and outputs. Independently reducing the sizes of basic structures can result in inconsistent dimensions among them, and consequently, end up with invalid LSTM units. To overcome the problem, we propose Intrinsic Sparse Structures (ISS) in LSTMs. Removing a component of ISS will simultaneously decrease the sizes of all basic structures by one and thereby always maintain the dimension consistency. By learning ISS within LSTM units, the obtained LSTMs remain regular while having much smaller basic structures. Based on group Lasso regularization, our method achieves 10.59x speedup without losing any perplexity of a language modeling of Penn TreeBank dataset. It is also successfully evaluated through a compact model with only 2.69M weights for machine Question Answering of SQuAD dataset. Our approach is successfully extended to non- LSTM RNNs, like Recurrent Highway Networks (RHNs). Our source code is publicly available at https://github.com/wenwei202/iss-rnns","http://arxiv.org/pdf/1709.05027v7","cs.LG	cs.AI	cs.CL	cs.NE","Learning Intrinsic Sparse Structures within Long Short-Term Memory","wei.wen@duke.edu	yiran.chen@duke.edu	hai.li@duke.edu	yuxhe@microsoft.com	samyamr@microsoft.com	minjiaz@microsoft.com	wenhanw@microsoft.com	fangliu@microsoft.com	binhu@microsoft.com"
"Huda Hakami	Danushka Bollegala	Hayashi Kohei","19","9","2017","Representing the semantic relations that exist between two given words (or entities) is an important first step in a wide-range of NLP applications such as analogical reasoning, knowledge base completion and relational information retrieval. A simple, yet surprisingly accurate method for representing a relation between two words is to compute the vector offset (\PairDiff) between their corresponding word embeddings. Despite the empirical success, it remains unclear as to whether \PairDiff is the best operator for obtaining a relational representation from word embeddings. We conduct a theoretical analysis of generalised bilinear operators that can be used to measure the $\ell_{2}$ relational distance between two word-pairs. We show that, if the word embeddings are standardised and uncorrelated, such an operator will be independent of bilinear terms, and can be simplified to a linear form, where \PairDiff is a special case. For numerous word embedding types, we empirically verify the uncorrelation assumption, demonstrating the general applicability of our theoretical result. Moreover, we experimentally discover \PairDiff from the bilinear relation composition operator on several benchmark analogy datasets.","http://arxiv.org/pdf/1709.06673v2","cs.CL	cs.AI	cs.LG	cs.NE","Why PairDiff works? -- A Mathematical Analysis of Bilinear Relational   Compositional Operators for Analogy Detection","H.A.Hakami@liverpool.ac.uk	danushka@liverpool.ac.uk	hayashi.kohei@gmail.com"
"Zhengdong Lu	Haotian Cui	Xianggen Liu	Yukun Yan	Daqi Zheng","26","9","2017","We propose Object-oriented Neural Programming (OONP), a framework for semantically parsing documents in specific domains. Basically, OONP reads a document and parses it into a predesigned object-oriented data structure (referred to as ontology in this paper) that reflects the domain-specific semantics of the document. An OONP parser models semantic parsing as a decision process: a neural net-based Reader sequentially goes through the document, and during the process it builds and updates an intermediate ontology to summarize its partial understanding of the text it covers. OONP supports a rich family of operations (both symbolic and differentiable) for composing the ontology, and a big variety of forms (both symbolic and differentiable) for representing the state and the document. An OONP parser can be trained with supervision of different forms and strength, including supervised learning (SL) , reinforcement learning (RL) and hybrid of the two. Our experiments on both synthetic and real-world document parsing tasks have shown that OONP can learn to handle fairly complicated ontology with training data of modest sizes.","http://arxiv.org/pdf/1709.08853v4","cs.LG	cs.AI	cs.CL	cs.NE","Object-oriented Neural Programming (OONP) for Document Understanding","luz@deeplycurious.ai	da@deeplycurious.ai	cht15@mails.tsinghua.edu.cn	liuxg16@mails.tsinghua.edu.cn	yanyk13@mails.tsinghua.edu.cn"
"Bin Bi	Hao Ma","29","9","2017","This paper proposes a novel neural machine reading model for open-domain question answering at scale. Existing machine comprehension models typically assume that a short piece of relevant text containing answers is already identified and given to the models, from which the models are designed to extract answers. This assumption, however, is not realistic for building a large-scale open-domain question answering system which requires both deep text understanding and identifying relevant text from corpus simultaneously.   In this paper, we introduce Neural Comprehensive Ranker (NCR) that integrates both passage ranking and answer extraction in one single framework. A Q&A system based on this framework allows users to issue an open-domain question without needing to provide a piece of text that must contain the answer. Experiments show that the unified NCR model is able to outperform the states-of-the-art in both retrieval of relevant text and answer extraction.","http://arxiv.org/pdf/1709.10204v2","cs.CL	cs.AI	cs.LG	cs.NE","A Neural Comprehensive Ranker (NCR) for Open-Domain Question Answering",""
"Mirco Ravanelli	Philemon Brakel	Maurizio Omologo	Yoshua Bengio","29","9","2017","Speech recognition is largely taking advantage of deep learning, showing that substantial benefits can be obtained by modern Recurrent Neural Networks (RNNs). The most popular RNNs are Long Short-Term Memory (LSTMs), which typically reach state-of-the-art performance in many tasks thanks to their ability to learn long-term dependencies and robustness to vanishing gradients. Nevertheless, LSTMs have a rather complex design with three multiplicative gates, that might impair their efficient implementation. An attempt to simplify LSTMs has recently led to Gated Recurrent Units (GRUs), which are based on just two multiplicative gates.   This paper builds on these efforts by further revising GRUs and proposing a simplified architecture potentially more suitable for speech recognition. The contribution of this work is two-fold. First, we suggest to remove the reset gate in the GRU design, resulting in a more efficient single-gate architecture. Second, we propose to replace tanh with ReLU activations in the state update equations. Results show that, in our implementation, the revised architecture reduces the per-epoch training time with more than 30% and consistently improves recognition performance across different tasks, input features, and noisy conditions when compared to a standard GRU.","http://arxiv.org/pdf/1710.00641v1","cs.CL	cs.AI	cs.LG	cs.NE","Improving speech recognition by revising gated recurrent units","mravanelli@fbk.eu,	pbpop3@gmail.com,	omologo@fbk.eu,	yoshua.umontreal@gmail.com"
"Baolin Peng	Xiujun Li	Jianfeng Gao	Jingjing Liu	Kam-Fai Wong","18","1","2018","Training a task-completion dialogue agent with real users via reinforcement learning (RL) could be prohibitively expensive, because it requires many interactions with users. One alternative is to resort to a user simulator, while the discrepancy of between simulated and real users makes the learned policy unreliable in practice. This paper addresses these challenges by integrating planning into the dialogue policy learning based on Dyna-Q framework, and provides a more sample-efficient approach to learn the dialogue polices. The proposed agent consists of a planner trained on-line with limited real user experience that can generate large amounts of simulated experience to supplement with limited real user experience, and a policy model trained on these hybrid experiences. The effectiveness of our approach is validated on a movie-booking task in both a simulation setting and a human-in-the-loop setting.","http://arxiv.org/pdf/1801.06176v1","cs.CL	cs.AI	cs.LG	cs.NE","Integrating planning for task-completion dialogue policy learning","blpeng@se.cuhk.edu.hk	kfwong@se.cuhk.edu.hk	xiul@microsoft.com	jfgao@microsoft.com	jingjl@microsoft.com"
"Andrew L. Maas	Peng Qi	Ziang Xie	Awni Y. Hannun	Christopher T. Lengerich	Daniel Jurafsky	Andrew Y. Ng","30","6","2014","Deep neural networks (DNNs) are now a central component of nearly all state-of-the-art speech recognition systems. Building neural network acoustic models requires several design decisions including network architecture, size, and training loss function. This paper offers an empirical investigation on which aspects of DNN acoustic model design are most important for speech recognition system performance. We report DNN classifier performance and final speech recognizer word error rates, and compare DNNs using several metrics to quantify factors influencing differences in task performance. Our first set of experiments use the standard Switchboard benchmark corpus, which contains approximately 300 hours of conversational telephone speech. We compare standard DNNs to convolutional networks, and present the first experiments using locally-connected, untied neural networks for acoustic modeling. We additionally build systems on a corpus of 2,100 hours of training data by combining the Switchboard and Fisher corpora. This larger corpus allows us to more thoroughly examine performance of large DNN models -- with up to ten times more parameters than those typically used in speech recognition systems. Our results suggest that a relatively simple DNN architecture and optimization technique produces strong results. These findings, along with previous work, help establish a set of best practices for building DNN hybrid speech recognition systems with maximum likelihood training. Our experiments in DNN optimization additionally serve as a case study for training DNNs with discriminative loss functions for speech tasks, as well as DNN classifiers more generally.","http://arxiv.org/pdf/1406.7806v2","cs.CL	cs.LG	cs.NE	stat.ML","Building DNN Acoustic Models for Large Vocabulary Speech Recognition",""
"William Chan	Ian Lane","7","4","2015","We present a novel deep Recurrent Neural Network (RNN) model for acoustic modelling in Automatic Speech Recognition (ASR). We term our contribution as a TC-DNN-BLSTM-DNN model, the model combines a Deep Neural Network (DNN) with Time Convolution (TC), followed by a Bidirectional Long Short-Term Memory (BLSTM), and a final DNN. The first DNN acts as a feature processor to our model, the BLSTM then generates a context from the sequence acoustic signal, and the final DNN takes the context and models the posterior probabilities of the acoustic states. We achieve a 3.47 WER on the Wall Street Journal (WSJ) eval92 task or more than 8% relative improvement over the baseline DNN models.","http://arxiv.org/pdf/1504.01482v1","cs.LG	cs.CL	cs.NE	stat.ML","Deep Recurrent Neural Networks for Acoustic Modelling","williamchan@cmu.edu,	lane@cmu.edu"
"David Krueger	Roland Memisevic","26","11","2015","We stabilize the activations of Recurrent Neural Networks (RNNs) by penalizing the squared distance between successive hidden states' norms.   This penalty term is an effective regularizer for RNNs including LSTMs and IRNNs, improving performance on character-level language modeling and phoneme recognition, and outperforming weight noise and dropout.   We achieve competitive performance (18.6\% PER) on the TIMIT phoneme recognition task for RNNs evaluated without beam search or an RNN transducer.   With this penalty term, IRNN can achieve similar performance to LSTM on language modeling, although adding the penalty term to the LSTM results in superior performance.   Our penalty term also prevents the exponential growth of IRNN's activations outside of their training horizon, allowing them to generalize to much longer sequences.","http://arxiv.org/pdf/1511.08400v7","cs.NE	cs.CL	cs.LG	stat.ML","Regularizing RNNs by Stabilizing Activations",""
"Noam Shazeer	Azalia Mirhoseini	Krzysztof Maziarz	Andy Davis	Quoc Le	Geoffrey Hinton	Jeff Dean","23","1","2017","The capacity of a neural network to absorb information is limited by its number of parameters. Conditional computation, where parts of the network are active on a per-example basis, has been proposed in theory as a way of dramatically increasing model capacity without a proportional increase in computation. In practice, however, there are significant algorithmic and performance challenges. In this work, we address these challenges and finally realize the promise of conditional computation, achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters. We introduce a Sparsely-Gated Mixture-of-Experts layer (MoE), consisting of up to thousands of feed-forward sub-networks. A trainable gating network determines a sparse combination of these experts to use for each example. We apply the MoE to the tasks of language modeling and machine translation, where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora. We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers. On large language modeling and machine translation benchmarks, these models achieve significantly better results than state-of-the-art at lower computational cost.","http://arxiv.org/pdf/1701.06538v1","cs.LG	cs.CL	cs.NE	stat.ML","Outrageously Large Neural Networks: The Sparsely-Gated   Mixture-of-Experts Layer","noam@google.com	azalia@google.com	andydavis@google.com	qvl@google.com	geoffhinton@google.com	jeff@google.com	krzysztof.maziarz@student.uj.edu.pl"
"Yacine Jernite	Samuel R. Bowman	David Sontag","23","4","2017","This work presents a novel objective function for the unsupervised training of neural network sentence encoders. It exploits signals from paragraph-level discourse coherence to train these models to understand text. Our objective is purely discriminative, allowing us to train models many times faster than was possible under prior methods, and it yields models which perform well in extrinsic evaluations.","http://arxiv.org/pdf/1705.00557v1","cs.CL	cs.LG	cs.NE	stat.ML","Discourse-Based Objectives for Fast Unsupervised Sentence Representation   Learning","yacine.jernite@nyu.edu	bowman@nyu.edu	dsontag@mit.edu"
"Zhengyang Wang	Shuiwang Ji","18","5","2017","Visual question answering is a recently proposed artificial intelligence task that requires a deep understanding of both images and texts. In deep learning, images are typically modeled through convolutional neural networks, and texts are typically modeled through recurrent neural networks. While the requirement for modeling images is similar to traditional computer vision tasks, such as object recognition and image classification, visual question answering raises a different need for textual representation as compared to other natural language processing tasks. In this work, we perform a detailed analysis on natural language questions in visual question answering. Based on the analysis, we propose to rely on convolutional neural networks for learning textual representations. By exploring the various properties of convolutional neural networks specialized for text data, such as width and depth, we present our ""CNN Inception + Gate"" model. We show that our model improves question representations and thus the overall accuracy of visual question answering models. We also show that the text representation requirement in visual question answering is more complicated and comprehensive than that in conventional natural language processing tasks, making it a better task to evaluate textual representation methods. Shallow models like fastText, which can obtain comparable results with deep learning models in tasks like text classification, are not suitable in visual question answering.","http://arxiv.org/pdf/1705.06824v1","cs.LG	cs.CL	cs.NE	stat.ML","Learning Convolutional Text Representations for Visual Question   Answering","zwang6@eecs.wsu.edu	sji@eecs.wsu.edu"
"Kyunghyun Cho	Bart van Merrienboer	Caglar Gulcehre	Dzmitry Bahdanau	Fethi Bougares	Holger Schwenk	Yoshua Bengio","3","6","2014","In this paper, we propose a novel neural network model called RNN Encoder-Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixed-length vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder-Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.","http://arxiv.org/pdf/1406.1078v3","cs.CL	cs.LG	cs.NE	stat.ML","Learning Phrase Representations using RNN Encoder-Decoder for   Statistical Machine Translation","firstname.lastname@umontreal.ca	d.bahdanau@jacobs-university.de	firstname.lastname@lium.univ-lemans.fr	find.me@on.the.web"
"Zhiyuan Tang	Dong Wang	Zhiyong Zhang","18","5","2015","Recurrent neural networks (RNNs), particularly long short-term memory (LSTM), have gained much attention in automatic speech recognition (ASR). Although some successful stories have been reported, training RNNs remains highly challenging, especially with limited training data. Recent research found that a well-trained model can be used as a teacher to train other child models, by using the predictions generated by the teacher model as supervision. This knowledge transfer learning has been employed to train simple neural nets with a complex one, so that the final performance can reach a level that is infeasible to obtain by regular training. In this paper, we employ the knowledge transfer learning approach to train RNNs (precisely LSTM) using a deep neural network (DNN) model as the teacher. This is different from most of the existing research on knowledge transfer learning, since the teacher (DNN) is assumed to be weaker than the child (RNN); however, our experiments on an ASR task showed that it works fairly well: without applying any tricks on the learning scheme, this approach can train RNNs successfully even with limited training data.","http://arxiv.org/pdf/1505.04630v5","stat.ML	cs.CL	cs.LG	cs.NE","Recurrent Neural Network Training with Dark Knowledge Transfer","tangzy@cslt.riit.tsinghua.edu.cn	zhangzy@cslt.riit.tsinghua.edu.cn	Author:wangdong99@mails.tsinghua.edu.cn"
"Haşim Sak	Andrew Senior	Françoise Beaufays","5","2","2014","Long Short-Term Memory (LSTM) is a recurrent neural network (RNN) architecture that has been designed to address the vanishing and exploding gradient problems of conventional RNNs. Unlike feedforward neural networks, RNNs have cyclic connections making them powerful for modeling sequences. They have been successfully used for sequence labeling and sequence prediction tasks, such as handwriting recognition, language modeling, phonetic labeling of acoustic frames. However, in contrast to the deep neural networks, the use of RNNs in speech recognition has been limited to phone recognition in small scale tasks. In this paper, we present novel LSTM based RNN architectures which make more effective use of model parameters to train acoustic models for large vocabulary speech recognition. We train and compare LSTM, RNN and DNN models at various numbers of parameters and configurations. We show that LSTM models converge quickly and give state of the art speech recognition performance for relatively small sized models.","http://arxiv.org/pdf/1402.1128v1","cs.NE	cs.CL	cs.LG	stat.ML","Long Short-Term Memory Based Recurrent Neural Network Architectures for   Large Vocabulary Speech Recognition","hasim@google.com}	andrewsenior@google.com}	fsb@google.com@google.com}"
"Peter Wittek	Sándor Darányi	Efstratios Kontopoulos	Theodoros Moysiadis	Ioannis Kompatsiaris","5","2","2015","Based on the Aristotelian concept of potentiality vs. actuality allowing for the study of energy and dynamics in language, we propose a field approach to lexical analysis. Falling back on the distributional hypothesis to statistically model word meaning, we used evolving fields as a metaphor to express time-dependent changes in a vector space model by a combination of random indexing and evolving self-organizing maps (ESOM). To monitor semantic drifts within the observation period, an experiment was carried out on the term space of a collection of 12.8 million Amazon book reviews. For evaluation, the semantic consistency of ESOM term clusters was compared with their respective neighbourhoods in WordNet, and contrasted with distances among term vectors by random indexing. We found that at 0.05 level of significance, the terms in the clusters showed a high level of semantic consistency. Tracking the drift of distributional patterns in the term space across time periods, we found that consistency decreased, but not at a statistically significant level. Our method is highly scalable, with interpretations in philosophy.","http://arxiv.org/pdf/1502.01753v1","cs.CL	cs.LG	cs.NE	stat.ML","Monitoring Term Drift Based on Semantic Consistency in an Evolving   Vector Field",""
"Jan Chorowski	Navdeep Jaitly","8","12","2016","The recently proposed Sequence-to-Sequence (seq2seq) framework advocates replacing complex data processing pipelines, such as an entire automatic speech recognition system, with a single neural network trained in an end-to-end fashion. In this contribution, we analyse an attention-based seq2seq speech recognition system that directly transcribes recordings into characters. We observe two shortcomings: overconfidence in its predictions and a tendency to produce incomplete transcriptions when language models are used. We propose practical solutions to both problems achieving competitive speaker independent word error rates on the Wall Street Journal dataset: without separate language models we reach 10.6% WER, while together with a trigram language model, we reach 6.7% WER.","http://arxiv.org/pdf/1612.02695v1","cs.NE	cs.CL	cs.LG	stat.ML","Towards better decoding and language model integration in sequence to   sequence models","jan.chorowski@cs.uni.wroc.pl,ndjaitly@google.com"
"Dzmitry Bahdanau	Kyunghyun Cho	Yoshua Bengio","1","9","2014","Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.","http://arxiv.org/pdf/1409.0473v7","cs.CL	cs.LG	cs.NE	stat.ML","Neural Machine Translation by Jointly Learning to Align and Translate",""
"Jean Pouget-Abadie	Dzmitry Bahdanau	Bart van Merrienboer	Kyunghyun Cho	Yoshua Bengio","3","9","2014","The authors of (Cho et al., 2014a) have shown that the recently introduced neural network translation systems suffer from a significant drop in translation quality when translating long sentences, unlike existing phrase-based translation systems. In this paper, we propose a way to address this issue by automatically segmenting an input sentence into phrases that can be easily translated by the neural network translation model. Once each segment has been independently translated by the neural machine translation model, the translated clauses are concatenated to form a final translation. Empirical results show a significant improvement in translation quality for long sentences.","http://arxiv.org/pdf/1409.1257v2","cs.CL	cs.LG	cs.NE	stat.ML","Overcoming the Curse of Sentence Length for Neural Machine Translation   using Automatic Segmentation",""
"William Chan	Nan Rosemary Ke	Ian Lane","7","4","2015","Deep Neural Network (DNN) acoustic models have yielded many state-of-the-art results in Automatic Speech Recognition (ASR) tasks. More recently, Recurrent Neural Network (RNN) models have been shown to outperform DNNs counterparts. However, state-of-the-art DNN and RNN models tend to be impractical to deploy on embedded systems with limited computational capacity. Traditionally, the approach for embedded platforms is to either train a small DNN directly, or to train a small DNN that learns the output distribution of a large DNN. In this paper, we utilize a state-of-the-art RNN to transfer knowledge to small DNN. We use the RNN model to generate soft alignments and minimize the Kullback-Leibler divergence against the small DNN. The small DNN trained on the soft RNN alignments achieved a 3.93 WER on the Wall Street Journal (WSJ) eval92 task compared to a baseline 4.54 WER or more than 13% relative improvement.","http://arxiv.org/pdf/1504.01483v1","cs.LG	cs.CL	cs.NE	stat.ML","Transferring Knowledge from a RNN to a DNN","williamchan@cmu.edu,	rosemary.ke@sv.cmu.edu,	lane@cmu.edu"
"Sarath Chandar	Mitesh M. Khapra	Hugo Larochelle	Balaraman Ravindran","27","4","2015","Common Representation Learning (CRL), wherein different descriptions (or views) of the data are embedded in a common subspace, is receiving a lot of attention recently. Two popular paradigms here are Canonical Correlation Analysis (CCA) based approaches and Autoencoder (AE) based approaches. CCA based approaches learn a joint representation by maximizing correlation of the views when projected to the common subspace. AE based methods learn a common representation by minimizing the error of reconstructing the two views. Each of these approaches has its own advantages and disadvantages. For example, while CCA based approaches outperform AE based approaches for the task of transfer learning, they are not as scalable as the latter. In this work we propose an AE based approach called Correlational Neural Network (CorrNet), that explicitly maximizes correlation among the views when projected to the common subspace. Through a series of experiments, we demonstrate that the proposed CorrNet is better than the above mentioned approaches with respect to its ability to learn correlated common representations. Further, we employ CorrNet for several cross language tasks and show that the representations learned using CorrNet perform better than the ones learned using other state of the art approaches.","http://arxiv.org/pdf/1504.07225v3","cs.CL	cs.LG	cs.NE	stat.ML","Correlational Neural Networks","apsarathchandar@gmail.com	mikhapra@in.ibm.com	hugo.larochelle@usherbrooke.ca	ravi@cse.iitm.ac.in"
"Jan Chorowski	Dzmitry Bahdanau	Dmitriy Serdyuk	Kyunghyun Cho	Yoshua Bengio","24","6","2015","Recurrent sequence generators conditioned on input data through an attention mechanism have recently shown very good performance on a range of tasks in- cluding machine translation, handwriting synthesis and image caption gen- eration. We extend the attention-mechanism with features needed for speech recognition. We show that while an adaptation of the model used for machine translation in reaches a competitive 18.7% phoneme error rate (PER) on the TIMIT phoneme recognition task, it can only be applied to utterances which are roughly as long as the ones it was trained on. We offer a qualitative explanation of this failure and propose a novel and generic method of adding location-awareness to the attention mechanism to alleviate this issue. The new method yields a model that is robust to long inputs and achieves 18% PER in single utterances and 20% in 10-times longer (repeated) utterances. Finally, we propose a change to the at- tention mechanism that prevents it from concentrating too much on single frames, which further reduces PER to 17.6% level.","http://arxiv.org/pdf/1506.07503v1","cs.CL	cs.LG	cs.NE	stat.ML","Attention-Based Models for Speech Recognition","jan.chorowski@ii.uni.wroc.pl"
"Haşim Sak	Andrew Senior	Kanishka Rao	Françoise Beaufays","24","7","2015","We have recently shown that deep Long Short-Term Memory (LSTM) recurrent neural networks (RNNs) outperform feed forward deep neural networks (DNNs) as acoustic models for speech recognition. More recently, we have shown that the performance of sequence trained context dependent (CD) hidden Markov model (HMM) acoustic models using such LSTM RNNs can be equaled by sequence trained phone models initialized with connectionist temporal classification (CTC). In this paper, we present techniques that further improve performance of LSTM RNN acoustic models for large vocabulary speech recognition. We show that frame stacking and reduced frame rate lead to more accurate models and faster decoding. CD phone modeling leads to further improvements. We also present initial results for LSTM RNN models outputting words directly.","http://arxiv.org/pdf/1507.06947v1","cs.CL	cs.LG	cs.NE	stat.ML","Fast and Accurate Recurrent Neural Network Acoustic Models for Speech   Recognition","hasim@google.com	andrewsenior@google.com	kanishkarao@google.com	fsb@google.com"
"William Chan	Navdeep Jaitly	Quoc V. Le	Oriol Vinyals","5","8","2015","We present Listen, Attend and Spell (LAS), a neural network that learns to transcribe speech utterances to characters. Unlike traditional DNN-HMM models, this model learns all the components of a speech recognizer jointly. Our system has two components: a listener and a speller. The listener is a pyramidal recurrent network encoder that accepts filter bank spectra as inputs. The speller is an attention-based recurrent network decoder that emits characters as outputs. The network produces character sequences without making any independence assumptions between the characters. This is the key improvement of LAS over previous end-to-end CTC models. On a subset of the Google voice search task, LAS achieves a word error rate (WER) of 14.1% without a dictionary or a language model, and 10.3% with language model rescoring over the top 32 beams. By comparison, the state-of-the-art CLDNN-HMM model achieves a WER of 8.0%.","http://arxiv.org/pdf/1508.01211v2","cs.CL	cs.LG	cs.NE	stat.ML","Listen, Attend and Spell","williamchan@cmu.edu	ndjaitly@google.com	qvl@google.com	vinyals@google.com"
"Shihao Ji	S. V. N. Vishwanathan	Nadathur Satish	Michael J. Anderson	Pradeep Dubey","21","11","2015","We propose BlackOut, an approximation algorithm to efficiently train massive recurrent neural network language models (RNNLMs) with million word vocabularies. BlackOut is motivated by using a discriminative loss, and we describe a new sampling strategy which significantly reduces computation while improving stability, sample efficiency, and rate of convergence. One way to understand BlackOut is to view it as an extension of the DropOut strategy to the output layer, wherein we use a discriminative training loss and a weighted sampling scheme. We also establish close connections between BlackOut, importance sampling, and noise contrastive estimation (NCE). Our experiments, on the recently released one billion word language modeling benchmark, demonstrate scalability and accuracy of BlackOut; we outperform the state-of-the art, and achieve the lowest perplexity scores on this dataset. Moreover, unlike other established methods which typically require GPUs or CPU clusters, we show that a carefully implemented version of BlackOut requires only 1-10 days on a single machine to train a RNNLM with a million word vocabulary and billions of parameters on one billion words. Although we describe BlackOut in the context of RNNLM training, it can be used to any networks with large softmax output layers.","http://arxiv.org/pdf/1511.06909v7","cs.LG	cs.CL	cs.NE	stat.ML","BlackOut: Speeding up Recurrent Neural Network Language Models With Very   Large Vocabularies","shihao.ji@intel.com	vishy@ucsc.edu	nadathur.rajagopalan.satish@intel.com	michael.j.anderson@intel.com	pradeep.dubey@intel.com"
"Marta R. Costa-Jussà	José A. R. Fonollosa","2","3","2016","Neural Machine Translation (MT) has reached state-of-the-art results. However, one of the main challenges that neural MT still faces is dealing with very large vocabularies and morphologically rich languages. In this paper, we propose a neural MT system using character-based embeddings in combination with convolutional and highway layers to replace the standard lookup-based word representations. The resulting unlimited-vocabulary and affix-aware source word embeddings are tested in a state-of-the-art neural MT based on an attention-based bidirectional recurrent neural network. The proposed MT scheme provides improved results even when the source language is not morphologically rich. Improvements up to 3 BLEU points are obtained in the German-English WMT task.","http://arxiv.org/pdf/1603.00810v3","cs.CL	cs.LG	cs.NE	stat.ML","Character-based Neural Machine Translation","marta.ruiz@upc.edu	jose.fonollosa@upc.edu"
"Yangfeng Ji	Gholamreza Haffari	Jacob Eisenstein","7","3","2016","This paper presents a novel latent variable recurrent neural network architecture for jointly modeling sequences of words and (possibly latent) discourse relations between adjacent sentences. A recurrent neural network generates individual words, thus reaping the benefits of discriminatively-trained vector representations. The discourse relations are represented with a latent variable, which can be predicted or marginalized, depending on the task. The resulting model can therefore employ a training objective that includes not only discourse relation classification, but also word prediction. As a result, it outperforms state-of-the-art alternatives for two tasks: implicit discourse relation classification in the Penn Discourse Treebank, and dialog act classification in the Switchboard corpus. Furthermore, by marginalizing over latent discourse relations at test time, we obtain a discourse informed language model, which improves over a strong LSTM baseline.","http://arxiv.org/pdf/1603.01913v2","cs.CL	cs.LG	cs.NE	stat.ML","A Latent Variable Recurrent Neural Network for Discourse Relation   Language Models","jiyfeng@gatech.edu	jacobe@gatech.edu"
"Zhiyuan Tang	Lantian Li	Dong Wang","31","3","2016","Although highly correlated, speech and speaker recognition have been regarded as two independent tasks and studied by two communities. This is certainly not the way that people behave: we decipher both speech content and speaker traits at the same time. This paper presents a unified model to perform speech and speaker recognition simultaneously and altogether. The model is based on a unified neural network where the output of one task is fed to the input of the other, leading to a multi-task recurrent network. Experiments show that the joint model outperforms the task-specific models on both the two tasks.","http://arxiv.org/pdf/1603.09643v4","cs.CL	cs.LG	cs.NE	stat.ML","Multi-task Recurrent Model for Speech and Speaker Recognition","tangzy@cslt.riit.tsinghua.edu.cn	lilt@cslt.riit.tsinghua.edu.cn	wangdong99@mails.tsinghua.edu.cn"
"Sarath Chandar	Sungjin Ahn	Hugo Larochelle	Pascal Vincent	Gerald Tesauro	Yoshua Bengio","24","5","2016","Memory networks are neural networks with an explicit memory component that can be both read and written to by the network. The memory is often addressed in a soft way using a softmax function, making end-to-end training with backpropagation possible. However, this is not computationally scalable for applications which require the network to read from extremely large memories. On the other hand, it is well known that hard attention mechanisms based on reinforcement learning are challenging to train successfully. In this paper, we explore a form of hierarchical memory network, which can be considered as a hybrid between hard and soft attention memory networks. The memory is organized in a hierarchical structure such that reading from it is done with less computation than soft attention over a flat memory, while also being easier to train than hard attention over a flat memory. Specifically, we propose to incorporate Maximum Inner Product Search (MIPS) in the training and inference procedures for our hierarchical memory network. We explore the use of various state-of-the art approximate MIPS techniques and report results on SimpleQuestions, a challenging large scale factoid question answering task.","http://arxiv.org/pdf/1605.07427v1","stat.ML	cs.CL	cs.LG	cs.NE","Hierarchical Memory Networks",""
"Sam Wiseman	Alexander M. Rush","9","6","2016","Sequence-to-Sequence (seq2seq) modeling has rapidly become an important general-purpose NLP tool that has proven effective for many text-generation and sequence-labeling tasks. Seq2seq builds on deep neural language modeling and inherits its remarkable accuracy in estimating local, next-word distributions. In this work, we introduce a model and beam-search training scheme, based on the work of Daume III and Marcu (2005), that extends seq2seq to learn global sequence scores. This structured approach avoids classical biases associated with local training and unifies the training loss with the test-time usage, while preserving the proven model architecture of seq2seq and its efficient training approach. We show that our system outperforms a highly-optimized attention-based seq2seq system and other baselines on three different sequence to sequence tasks: word ordering, parsing, and machine translation.","http://arxiv.org/pdf/1606.02960v2","cs.CL	cs.LG	cs.NE	stat.ML","Sequence-to-Sequence Learning as Beam-Search Optimization","swiseman@seas.harvard.edu	srush@seas.harvard.edu"
"Ankit Vani	Yacine Jernite	David Sontag","23","5","2017","In this work, we present the Grounded Recurrent Neural Network (GRNN), a recurrent neural network architecture for multi-label prediction which explicitly ties labels to specific dimensions of the recurrent hidden state (we call this process ""grounding""). The approach is particularly well-suited for extracting large numbers of concepts from text. We apply the new model to address an important problem in healthcare of understanding what medical concepts are discussed in clinical text. Using a publicly available dataset derived from Intensive Care Units, we learn to label a patient's diagnoses and procedures from their discharge summary. Our evaluation shows a clear advantage to using our proposed architecture over a variety of strong baselines.","http://arxiv.org/pdf/1705.08557v1","stat.ML	cs.CL	cs.LG	cs.NE","Grounded Recurrent Neural Networks","ankit.vani@nyu.edu,	jernite@cs.nyu.edu,	dsontag@csail.mit.edu"
"Tsung-Hsien Wen	Yishu Miao	Phil Blunsom	Steve Young","29","5","2017","Developing a dialogue agent that is capable of making autonomous decisions and communicating by natural language is one of the long-term goals of machine learning research. Traditional approaches either rely on hand-crafting a small state-action set for applying reinforcement learning that is not scalable or constructing deterministic models for learning dialogue sentences that fail to capture natural conversational variability. In this paper, we propose a Latent Intention Dialogue Model (LIDM) that employs a discrete latent variable to learn underlying dialogue intentions in the framework of neural variational inference. In a goal-oriented dialogue scenario, these latent intentions can be interpreted as actions guiding the generation of machine responses, which can be further refined autonomously by reinforcement learning. The experimental evaluation of LIDM shows that the model out-performs published benchmarks for both corpus-based and human evaluation, demonstrating the effectiveness of discrete latent variable models for learning goal-oriented dialogues.","http://arxiv.org/pdf/1705.10229v1","cs.CL	cs.LG	cs.NE	stat.ML","Latent Intention Dialogue Models",""
"Julius Kunze	Louis Kirsch	Ilia Kurenkov	Andreas Krug	Jens Johannsmeier	Sebastian Stober","1","6","2017","End-to-end training of automated speech recognition (ASR) systems requires massive data and compute resources. We explore transfer learning based on model adaptation as an approach for training ASR models under constrained GPU memory, throughput and training data. We conduct several systematic experiments adapting a Wav2Letter convolutional neural network originally trained for English ASR to the German language. We show that this technique allows faster training on consumer-grade resources while requiring less training data in order to achieve the same accuracy, thereby lowering the cost of training ASR models in other languages. Model introspection revealed that small adaptations to the network's weights were sufficient for good performance, especially for inner layers.","http://arxiv.org/pdf/1706.00290v1","cs.LG	cs.CL	cs.NE	stat.ML","Transfer Learning for Speech Recognition on a Budget","juliuskunze@gmail.com,	mail@louiskirsch.com	kurenkov@uni-potsdam.de	ankrug@uni-potsdam.de	johannsmeier@uni-potsdam.de	sstober@uni-potsdam.de"
"Matt Shannon","8","6","2017","State-level minimum Bayes risk (sMBR) training has become the de facto standard for sequence-level training of speech recognition acoustic models. It has an elegant formulation using the expectation semiring, and gives large improvements in word error rate (WER) over models trained solely using cross-entropy (CE) or connectionist temporal classification (CTC). sMBR training optimizes the expected number of frames at which the reference and hypothesized acoustic states differ. It may be preferable to optimize the expected WER, but WER does not interact well with the expectation semiring, and previous approaches based on computing expected WER exactly involve expanding the lattices used during training. In this paper we show how to perform optimization of the expected WER by sampling paths from the lattices used during conventional sMBR training. The gradient of the expected WER is itself an expectation, and so may be approximated using Monte Carlo sampling. We show experimentally that optimizing WER during acoustic model training gives 5% relative improvement in WER over a well-tuned sMBR baseline on a 2-channel query recognition task (Google Home).","http://arxiv.org/pdf/1706.02776v1","cs.CL	cs.LG	cs.NE	stat.ML","Optimizing expected word error rate via sampling for speech recognition","mattshannon@google.com"
"Artem M. Grachev	Dmitry I. Ignatov	Andrey V. Savchenko","20","8","2017","In this paper, we consider several compression techniques for the language modeling problem based on recurrent neural networks (RNNs). It is known that conventional RNNs, e.g, LSTM-based networks in language modeling, are characterized with either high space complexity or substantial inference time. This problem is especially crucial for mobile applications, in which the constant interaction with the remote server is inappropriate. By using the Penn Treebank (PTB) dataset we compare pruning, quantization, low-rank factorization, tensor train decomposition for LSTM networks in terms of model size and suitability for fast inference.","http://arxiv.org/pdf/1708.05963v1","stat.ML	cs.CL	cs.LG	cs.NE	62M45, 68T50	I.2.7, I.2.6, I.5.1, I.5.4","Neural Networks Compression for Language Modeling","grachev.art@gmail.com"
"Mostafa Dehghani	Aliaksei Severyn	Sascha Rothe	Jaap Kamps","1","11","2017","Training deep neural networks requires massive amounts of training data, but for many tasks only limited labeled data is available. This makes weak supervision attractive, using weak or noisy signals like the output of heuristic methods or user click-through data for training. In a semi-supervised setting, we can use a large set of data with weak labels to pretrain a neural network and then fine-tune the parameters with a small amount of data with true labels. This feels intuitively sub-optimal as these two independent stages leave the model unaware about the varying label quality. What if we could somehow inform the model about the label quality? In this paper, we propose a semi-supervised learning method where we train two neural networks in a multi-task fashion: a ""target network"" and a ""confidence network"". The target network is optimized to perform a given task and is trained using a large set of unlabeled data that are weakly annotated. We propose to weight the gradient updates to the target network using the scores provided by the second confidence network, which is trained on a small amount of supervised data. Thus we avoid that the weight updates computed from noisy labels harm the quality of the target network model. We evaluate our learning strategy on two different tasks: document ranking and sentiment classification. The results demonstrate that our approach not only enhances the performance compared to the baselines but also speeds up the learning process from weak labels.","http://arxiv.org/pdf/1711.00313v2","cs.LG	cs.CL	cs.NE	stat.ML","Avoiding Your Teacher's Mistakes: Training Neural Networks with   Controlled Weak Supervision","dehghani@uva.nl,	severyn@google.com,	rothe@google.com,	kamps@uva.nl"
"Christopher Tegho	Paweł Budzianowski	Milica Gašić","30","11","2017","In statistical dialogue management, the dialogue manager learns a policy that maps a belief state to an action for the system to perform. Efficient exploration is key to successful policy optimisation. Current deep reinforcement learning methods are very promising but rely on epsilon-greedy exploration, thus subjecting the user to a random choice of action during learning. Alternative approaches such as Gaussian Process SARSA (GPSARSA) estimate uncertainties and are sample efficient, leading to better user experience, but on the expense of a greater computational complexity. This paper examines approaches to extract uncertainty estimates from deep Q-networks (DQN) in the context of dialogue management. We perform an extensive benchmark of deep Bayesian methods to extract uncertainty estimates, namely Bayes-By-Backprop, dropout, its concrete variation, bootstrapped ensemble and alpha-divergences, combining it with DQN algorithm.","http://arxiv.org/pdf/1711.11486v1","stat.ML	cs.CL	cs.LG	cs.NE","Uncertainty Estimates for Efficient Neural Network-based Dialogue Policy   Optimisation","christegho@gmail.com	pfb30@cam.ac.uk	mg436@cam.ac.uk"
"Yuanhang Su	Yuzhong Huang	C. -C. Jay Kuo","27","2","2018","In this work, we investigate the memory capability of recurrent neural networks (RNNs), where this capability is defined as a function that maps an element in a sequence to the current output. We first analyze the system function of a recurrent neural network (RNN) cell, and provide analytical results for three RNNs. They are the simple recurrent neural network (SRN), the long short-term memory (LSTM), and the gated recurrent unit (GRU). Based on the analysis, we propose a new design to extend the memory length of a cell, and call it the extended long short-term memory (ELSTM). Next, we present a dependent bidirectional recurrent neural network (DBRNN) for the sequence-in-sequence-out (SISO) problem, which is more robust to previous erroneous predictions. Extensive experiments are carried out on different language tasks to demonstrate the superiority of our proposed ELSTM and DBRNN solutions.","http://arxiv.org/pdf/1803.01686v1","cs.LG	cs.CL	cs.NE	stat.ML","On Extended Long Short-term Memory and Dependent Bidirectional Recurrent   Neural Network",""
"Lin Ma	Zhengdong Lu	Hang Li","1","6","2015","In this paper, we propose to employ the convolutional neural network (CNN) for the image question answering (QA). Our proposed CNN provides an end-to-end framework with convolutional architectures for learning not only the image and question representations, but also their inter-modal interactions to produce the answer. More specifically, our model consists of three CNNs: one image CNN to encode the image content, one sentence CNN to compose the words of the question, and one multimodal convolution layer to learn their joint representation for the classification in the space of candidate answer words. We demonstrate the efficacy of our proposed model on the DAQUAR and COCO-QA datasets, which are two benchmark datasets for the image QA, with the performances significantly outperforming the state-of-the-art.","http://arxiv.org/pdf/1506.00333v2","cs.CL	cs.CV	cs.LG	cs.NE","Learning to Answer Questions From Image Using Convolutional Neural   Network","forest.linma@gmail.com	Lu.Zhengdong@huawei.com	HangLi,HL@huawei.com"
"Zichao Yang	Xiaodong He	Jianfeng Gao	Li Deng	Alex Smola","7","11","2015","This paper presents stacked attention networks (SANs) that learn to answer natural language questions from images. SANs use semantic representation of a question as query to search for the regions in an image that are related to the answer. We argue that image question answering (QA) often requires multiple steps of reasoning. Thus, we develop a multiple-layer SAN in which we query an image multiple times to infer the answer progressively. Experiments conducted on four image QA data sets demonstrate that the proposed SANs significantly outperform previous state-of-the-art approaches. The visualization of the attention layers illustrates the progress that the SAN locates the relevant visual clues that lead to the answer of the question layer-by-layer.","http://arxiv.org/pdf/1511.02274v2","cs.LG	cs.CL	cs.CV	cs.NE","Stacked Attention Networks for Image Question Answering","zichaoy@cs.cmu.edu,	xiaohe	jfgao	deng}@microsoft.com	alex@smola.org"
"Jacob Andreas	Marcus Rohrbach	Trevor Darrell	Dan Klein","9","11","2015","Visual question answering is fundamentally compositional in nature---a question like ""where is the dog?"" shares substructure with questions like ""what color is the dog?"" and ""where is the cat?"" This paper seeks to simultaneously exploit the representational capacity of deep networks and the compositional linguistic structure of questions. We describe a procedure for constructing and learning *neural module networks*, which compose collections of jointly-trained neural ""modules"" into deep networks for question answering. Our approach decomposes questions into their linguistic substructures, and uses these structures to dynamically instantiate modular networks (with reusable components for recognizing dogs, classifying colors, etc.). The resulting compound networks are jointly trained. We evaluate our approach on two challenging datasets for visual question answering, achieving state-of-the-art results on both the VQA natural image dataset and a new dataset of complex questions about abstract shapes.","http://arxiv.org/pdf/1511.02799v4","cs.CV	cs.CL	cs.LG	cs.NE","Neural Module Networks","jdau	rohrbachu	trevoru	klein}@{csu	eecsu	eecsu	csu"
"Federico Raue	Andreas Dengel	Thomas M. Breuel	Marcus Liwicki","13","11","2015","In this paper, we extend a symbolic association framework for being able to handle missing elements in multimodal sequences. The general scope of the work is the symbolic associations of object-word mappings as it happens in language development in infants. In other words, two different representations of the same abstract concepts can associate in both directions. This scenario has been long interested in Artificial Intelligence, Psychology, and Neuroscience. In this work, we extend a recent approach for multimodal sequences (visual and audio) to also cope with missing elements in one or both modalities. Our method uses two parallel Long Short-Term Memories (LSTMs) with a learning rule based on EM-algorithm. It aligns both LSTM outputs via Dynamic Time Warping (DTW). We propose to include an extra step for the combination with the max operation for exploiting the common elements between both sequences. The motivation behind is that the combination acts as a condition selector for choosing the best representation from both LSTMs. We evaluated the proposed extension in the following scenarios: missing elements in one modality (visual or audio) and missing elements in both modalities (visual and sound). The performance of our extension reaches better results than the original model and similar results to individual LSTM trained in each modality.","http://arxiv.org/pdf/1511.04401v5","cs.CV	cs.CL	cs.LG	cs.NE","Symbol Grounding Association in Multimodal Sequences with Missing   Elements","federico.raue@dfki.de	andreas.dengel@dfki.de	tmb@cs.uni-kl.de	liwicki@cs.uni-kl.de"
"Dan Hendrycks	Mantas Mazeika	Duncan Wilson	Kevin Gimpel","14","2","2018","The growing importance of massive datasets with the advent of deep learning makes robustness to label noise a critical property for classifiers to have. Sources of label noise include automatic labeling for large datasets, non-expert labeling, and label corruption by data poisoning adversaries. In the latter case, corruptions may be arbitrarily bad, even so bad that a classifier predicts the wrong labels with high confidence. To protect against such sources of noise, we leverage the fact that a small set of clean labels is often easy to procure. We demonstrate that robustness to label noise up to severe strengths can be achieved by using a set of trusted data with clean labels, and propose a loss correction that utilizes trusted examples in a data-efficient manner to mitigate the effects of label noise on deep neural network classifiers. Across vision and natural language processing tasks, we experiment with various label noises at several strengths, and show that our method significantly outperforms existing methods.","http://arxiv.org/pdf/1802.05300v1","cs.LG	cs.CL	cs.CV	cs.NE","Using Trusted Data to Train Deep Networks on Labels Corrupted by Severe   Noise",""
"Kyunghyun Cho	Aaron Courville	Yoshua Bengio","4","7","2015","Whereas deep neural networks were first mostly used for classification tasks, they are rapidly expanding in the realm of structured output problems, where the observed target is composed of multiple random variables that have a rich joint distribution, given the input. We focus in this paper on the case where the input also has a rich structure and the input and output structures are somehow related. We describe systems that learn to attend to different places in the input, for each element of the output, for a variety of tasks: machine translation, image caption generation, video clip description and speech recognition. All these systems are based on a shared set of building blocks: gated recurrent neural networks and convolutional neural networks, along with trained attention mechanisms. We report on experimental results with these systems, showing impressively good performance and the advantage of the attention mechanism.","http://arxiv.org/pdf/1507.01053v1","cs.NE	cs.CL	cs.CV	cs.LG","Describing Multimedia Content using Attention-based Encoder--Decoder   Networks",""
"Desmond Elliott	Stella Frank	Eva Hasler","15","10","2015","In this paper we present an approach to multi-language image description bringing together insights from neural machine translation and neural image description. To create a description of an image for a given target language, our sequence generation models condition on feature vectors from the image, the description from the source language, and/or a multimodal vector computed over the image and a description in the source language. In image description experiments on the IAPR-TC12 dataset of images aligned with English and German sentences, we find significant and substantial improvements in BLEU4 and Meteor scores for models trained over multiple languages, compared to a monolingual baseline.","http://arxiv.org/pdf/1510.04709v2","cs.CL	cs.CV	cs.LG	cs.NE","Multilingual Image Description with Neural Sequence Models","d.elliott@uva.nl	s.c.frank@uva.nl	ech57@cam.ac.uk"
"Oswaldo Ludwig	Xiao Liu	Parisa Kordjamshidi	Marie-Francine Moens","28","3","2016","This paper introduces the visually informed embedding of word (VIEW), a continuous vector representation for a word extracted from a deep neural model trained using the Microsoft COCO data set to forecast the spatial arrangements between visual objects, given a textual description. The model is composed of a deep multilayer perceptron (MLP) stacked on the top of a Long Short Term Memory (LSTM) network, the latter being preceded by an embedding layer. The VIEW is applied to transferring multimodal background knowledge to Spatial Role Labeling (SpRL) algorithms, which recognize spatial relations between objects mentioned in the text. This work also contributes with a new method to select complementary features and a fine-tuning method for MLP that improves the $F1$ measure in classifying the words into spatial roles. The VIEW is evaluated with the Task 3 of SemEval-2013 benchmark data set, SpaceEval.","http://arxiv.org/pdf/1603.08474v1","cs.CL	cs.CV	cs.LG	cs.NE","Deep Embedding for Spatial Role Labeling",""
"Yuntian Deng	Anssi Kanervisto	Jeffrey Ling	Alexander M. Rush","16","9","2016","We present a neural encoder-decoder model to convert images into presentational markup based on a scalable coarse-to-fine attention mechanism. Our method is evaluated in the context of image-to-LaTeX generation, and we introduce a new dataset of real-world rendered mathematical expressions paired with LaTeX markup. We show that unlike neural OCR techniques using CTC-based models, attention-based approaches can tackle this non-standard OCR task. Our approach outperforms classical mathematical OCR systems by a large margin on in-domain rendered data, and, with pretraining, also performs well on out-of-domain handwritten data. To reduce the inference complexity associated with the attention-based approaches, we introduce a new coarse-to-fine attention layer that selects a support region before applying attention.","http://arxiv.org/pdf/1609.04938v2","cs.CV	cs.CL	cs.LG	cs.NE","Image-to-Markup Generation with Coarse-to-Fine Attention",""
"Sumeet S. Singh","15","2","2018","We present a deep recurrent neural network model with soft visual attention that learns to generate LaTeX markup of real-world math formulas given their images. Applying neural sequence generation techniques that have been very successful in the fields of machine translation and image/handwriting/speech captioning, recognition, transcription and synthesis, we construct an image-to-markup model that learns to produce syntactically and semantically correct LaTeX markup code of over 150 words long and achieves a BLEU score of 89%; the best reported so far for the Im2Latex problem. We also visually demonstrate that the model learns to scan the image left-right / up-down much as a human would read it.","http://arxiv.org/pdf/1802.05415v1","cs.LG	cs.CL	cs.CV	cs.NE","Teaching Machines to Code: Neural Markup Generation with Visual   Attention",""
"Mohammad Javad Shafiee	Elnaz Barshan	Alexander Wong","7","4","2017","A promising paradigm for achieving highly efficient deep neural networks is the idea of evolutionary deep intelligence, which mimics biological evolution processes to progressively synthesize more efficient networks. A crucial design factor in evolutionary deep intelligence is the genetic encoding scheme used to simulate heredity and determine the architectures of offspring networks. In this study, we take a deeper look at the notion of synaptic cluster-driven evolution of deep neural networks which guides the evolution process towards the formation of a highly sparse set of synaptic clusters in offspring networks. Utilizing a synaptic cluster-driven genetic encoding, the probabilistic encoding of synaptic traits considers not only individual synaptic properties but also inter-synaptic relationships within a deep neural network. This process results in highly sparse offspring networks which are particularly tailored for parallel computational devices such as GPUs and deep neural network accelerator chips. Comprehensive experimental results using four well-known deep neural network architectures (LeNet-5, AlexNet, ResNet-56, and DetectNet) on two different tasks (object categorization and object detection) demonstrate the efficiency of the proposed method. Cluster-driven genetic encoding scheme synthesizes networks that can achieve state-of-the-art performance with significantly smaller number of synapses than that of the original ancestor network. ($\sim$125-fold decrease in synapses for MNIST). Furthermore, the improved cluster efficiency in the generated offspring networks ($\sim$9.71-fold decrease in clusters for MNIST and a $\sim$8.16-fold decrease in clusters for KITTI) is particularly useful for accelerated performance on parallel computing hardware architectures such as those in GPUs and deep neural network accelerator chips.","http://arxiv.org/pdf/1704.02081v1","cs.NE	cs.AI	cs.CV	stat.ML","Evolution in Groups: A deeper look at synaptic cluster driven evolution   of deep neural networks","mjshafiee@uwaterloo.ca	ebarshan@uwaterloo.ca	a28wong@uwaterloo.ca"
"Mete Ozay	Ilke Öztekin	Uygar Öztekin	Fatos T. Yarman Vural","10","5","2012","A relatively recent advance in cognitive neuroscience has been multi-voxel pattern analysis (MVPA), which enables researchers to decode brain states and/or the type of information represented in the brain during a cognitive operation. MVPA methods utilize machine learning algorithms to distinguish among types of information or cognitive states represented in the brain, based on distributed patterns of neural activity. In the current investigation, we propose a new approach for representation of neural data for pattern analysis, namely a Mesh Learning Model. In this approach, at each time instant, a star mesh is formed around each voxel, such that the voxel corresponding to the center node is surrounded by its p-nearest neighbors. The arc weights of each mesh are estimated from the voxel intensity values by least squares method. The estimated arc weights of all the meshes, called Mesh Arc Descriptors (MADs), are then used to train a classifier, such as Neural Networks, k-Nearest Neighbor, Na\""ive Bayes and Support Vector Machines. The proposed Mesh Model was tested on neuroimaging data acquired via functional magnetic resonance imaging (fMRI) during a recognition memory experiment using categorized word lists, employing a previously established experimental paradigm (\""Oztekin & Badre, 2011). Results suggest that the proposed Mesh Learning approach can provide an effective algorithm for pattern analysis of brain activity during cognitive processing.","http://arxiv.org/pdf/1205.2382v3","cs.NE	cs.AI	cs.CV	stat.ML","Mesh Learning for Classifying Cognitive Processes",""
"A. H. Karimi	M. J. Shafiee	A. Ghodsi	A. Wong","1","7","2017","In this work, we perform an exploratory study on synthesizing deep neural networks using biological synaptic strength distributions, and the potential influence of different distributions on modelling performance particularly for the scenario associated with small data sets. Surprisingly, a CNN with convolutional layer synaptic strengths drawn from biologically-inspired distributions such as log-normal or correlated center-surround distributions performed relatively well suggesting a possibility for designing deep neural network architectures that do not require many data samples to learn, and can sidestep current training procedures while maintaining or boosting modelling performance.","http://arxiv.org/pdf/1707.00081v1","cs.NE	cs.AI	cs.CV	stat.ML","Synthesizing Deep Neural Network Architectures using Biological Synaptic   Strength Distributions","(a6karimi@uwaterloo.ca)1	(mjshafiee@uwaterloo.ca)2	(aghodsib@uwaterloo.ca)3	(a28wong@uwaterloo.ca)2"
"Yukun Bao	Zhongyi Hu	Tao Xiong","9","1","2014","Addressing the issue of SVMs parameters optimization, this study proposes an efficient memetic algorithm based on Particle Swarm Optimization algorithm (PSO) and Pattern Search (PS). In the proposed memetic algorithm, PSO is responsible for exploration of the search space and the detection of the potential regions with optimum solutions, while pattern search (PS) is used to produce an effective exploitation on the potential regions obtained by PSO. Moreover, a novel probabilistic selection strategy is proposed to select the appropriate individuals among the current population to undergo local refinement, keeping a well balance between exploration and exploitation. Experimental results confirm that the local refinement with PS and our proposed selection strategy are effective, and finally demonstrate effectiveness and robustness of the proposed PSO-PS based MA for SVMs parameters optimization.","http://arxiv.org/pdf/1401.1926v1","cs.LG	cs.AI	cs.NE	stat.ML","A PSO and Pattern Search based Memetic Algorithm for SVMs Parameters   Optimization",""
"Laurent Dinh	Jascha Sohl-Dickstein	Samy Bengio","27","5","2016","Unsupervised learning of probabilistic models is a central yet challenging problem in machine learning. Specifically, designing models with tractable learning, sampling, inference and evaluation is crucial in solving this task. We extend the space of such models using real-valued non-volume preserving (real NVP) transformations, a set of powerful invertible and learnable transformations, resulting in an unsupervised learning algorithm with exact log-likelihood computation, exact sampling, exact inference of latent variables, and an interpretable latent space. We demonstrate its ability to model natural images on four datasets through sampling, log-likelihood evaluation and latent variable manipulations.","http://arxiv.org/pdf/1605.08803v3","cs.LG	cs.AI	cs.NE	stat.ML","Density estimation using Real NVP",""
"Tim Salimans	Jonathan Ho	Xi Chen	Szymon Sidor	Ilya Sutskever","10","3","2017","We explore the use of Evolution Strategies (ES), a class of black box optimization algorithms, as an alternative to popular MDP-based RL techniques such as Q-learning and Policy Gradients. Experiments on MuJoCo and Atari show that ES is a viable solution strategy that scales extremely well with the number of CPUs available: By using a novel communication strategy based on common random numbers, our ES implementation only needs to communicate scalars, making it possible to scale to over a thousand parallel workers. This allows us to solve 3D humanoid walking in 10 minutes and obtain competitive results on most Atari games after one hour of training. In addition, we highlight several advantages of ES as a black box optimization technique: it is invariant to action frequency and delayed rewards, tolerant of extremely long horizons, and does not need temporal discounting or value function approximation.","http://arxiv.org/pdf/1703.03864v2","stat.ML	cs.AI	cs.LG	cs.NE","Evolution Strategies as a Scalable Alternative to Reinforcement Learning",""
"Peter Karkus	David Hsu	Wee Sun Lee","20","3","2017","This paper introduces the QMDP-net, a neural network architecture for planning under partial observability. The QMDP-net combines the strengths of model-free learning and model-based planning. It is a recurrent policy network, but it represents a policy for a parameterized set of tasks by connecting a model with a planning algorithm that solves the model, thus embedding the solution structure of planning in a network learning architecture. The QMDP-net is fully differentiable and allows for end-to-end training. We train a QMDP-net on different tasks so that it can generalize to new ones in the parameterized task set and ""transfer"" to other similar tasks beyond the set. In preliminary experiments, QMDP-net showed strong performance on several robotic tasks in simulation. Interestingly, while QMDP-net encodes the QMDP algorithm, it sometimes outperforms the QMDP algorithm in the experiments, as a result of end-to-end learning.","http://arxiv.org/pdf/1703.06692v3","cs.AI	cs.LG	cs.NE	stat.ML","QMDP-Net: Deep Learning for Planning under Partial Observability","karkus@comp.nus.edu.sg	dyhsu@comp.nus.edu.sg	leews@comp.nus.edu.sg"
"Gregory Farquhar	Tim Rocktäschel	Maximilian Igl	Shimon Whiteson","31","10","2017","Combining deep model-free reinforcement learning with on-line planning is a promising approach to building on the successes of deep RL. On-line planning with look-ahead trees has proven successful in environments where transition models are known a priori. However, in complex environments where transition models need to be learned from data, the deficiencies of learned models have limited their utility for planning. To address these challenges, we propose TreeQN, a differentiable, recursive, tree-structured model that serves as a drop-in replacement for any value function network in deep RL with discrete actions. TreeQN dynamically constructs a tree by recursively applying a transition model in a learned abstract state space and then aggregating predicted rewards and state-values using a tree backup to estimate Q-values. We also propose ATreeC, an actor-critic variant that augments TreeQN with a softmax layer to form a stochastic policy network. Both approaches are trained end-to-end, such that the learned model is optimised for its actual use in the tree. We show that TreeQN and ATreeC outperform n-step DQN and A2C on a box-pushing task, as well as n-step DQN and value prediction networks (Oh et al. 2017) on multiple Atari games. Furthermore, we present ablation studies that demonstrate the effect of different auxiliary losses on learning transition models.","http://arxiv.org/pdf/1710.11417v2","cs.AI	cs.LG	cs.NE	stat.ML","TreeQN and ATreeC: Differentiable Tree-Structured Models for Deep   Reinforcement Learning","gregory.farquhar@cs.ox.ac.uk	tim.rocktaschel@cs.ox.ac.uk	maximilian.igl@cs.ox.ac.uk	shimon.whiteson@cs.ox.ac.uk"
"Nan Rosemary Ke	Anirudh Goyal	Olexa Bilaniuk	Jonathan Binas	Laurent Charlin	Chris Pal	Yoshua Bengio","7","11","2017","A major drawback of backpropagation through time (BPTT) is the difficulty of learning long-term dependencies, coming from having to propagate credit information backwards through every single step of the forward computation. This makes BPTT both computationally impractical and biologically implausible. For this reason, full backpropagation through time is rarely used on long sequences, and truncated backpropagation through time is used as a heuristic. However, this usually leads to biased estimates of the gradient in which longer term dependencies are ignored. Addressing this issue, we propose an alternative algorithm, Sparse Attentive Backtracking, which might also be related to principles used by brains to learn long-term dependencies. Sparse Attentive Backtracking learns an attention mechanism over the hidden states of the past and selectively backpropagates through paths with high attention weights. This allows the model to learn long term dependencies while only backtracking for a small number of time steps, not just from the recent past but also from attended relevant past states.","http://arxiv.org/pdf/1711.02326v1","cs.AI	cs.LG	cs.NE	stat.ML","Sparse Attentive Backtracking: Long-Range Credit Assignment in Recurrent   Networks",""
"Anakha V Babu	Bipin Rajendran","9","11","2017","We study the performance of stochastically trained deep neural networks (DNNs) whose synaptic weights are implemented using emerging memristive devices that exhibit limited dynamic range, resolution, and variability in their programming characteristics. We show that a key device parameter to optimize the learning efficiency of DNNs is the variability in its programming characteristics. DNNs with such memristive synapses, even with dynamic range as low as $15$ and only $32$ discrete levels, when trained based on stochastic updates suffer less than $3\%$ loss in accuracy compared to floating point software baseline. We also study the performance of stochastic memristive DNNs when used as inference engines with noise corrupted data and find that if the device variability can be minimized, the relative degradation in performance for the Stochastic DNN is better than that of the software baseline. Hence, our study presents a new optimization corner for memristive devices for building large noise-immune deep learning systems.","http://arxiv.org/pdf/1711.03640v1","stat.ML	cs.AI	cs.LG	cs.NE","Stochastic Deep Learning in Memristive Networks","av442@njit.edu	bipin@njit.edu"
"Yukun Bao	Tao Xiong	Zhongyi Hu","31","12","2013","Multi-step-ahead time series prediction is one of the most challenging research topics in the field of time series modeling and prediction, and is continually under research. Recently, the multiple-input several multiple-outputs (MISMO) modeling strategy has been proposed as a promising alternative for multi-step-ahead time series prediction, exhibiting advantages compared with the two currently dominating strategies, the iterated and the direct strategies. Built on the established MISMO strategy, this study proposes a particle swarm optimization (PSO)-based MISMO modeling strategy, which is capable of determining the number of sub-models in a self-adaptive mode, with varying prediction horizons. Rather than deriving crisp divides with equal-size s prediction horizons from the established MISMO, the proposed PSO-MISMO strategy, implemented with neural networks, employs a heuristic to create flexible divides with varying sizes of prediction horizons and to generate corresponding sub-models, providing considerable flexibility in model construction, which has been validated with simulated and real datasets.","http://arxiv.org/pdf/1401.0104v1","cs.AI	cs.LG	cs.NE	stat.ML","PSO-MISMO Modeling Strategy for Multi-Step-Ahead Time Series Prediction",""
"Behnam Neyshabur	Ryota Tomioka	Nathan Srebro","27","2","2015","We investigate the capacity, convexity and characterization of a general family of norm-constrained feed-forward networks.","http://arxiv.org/pdf/1503.00036v2","cs.LG	cs.AI	cs.NE	stat.ML","Norm-Based Capacity Control in Neural Networks",""
"Konrad Zolna","5","12","2016","The method presented extends a given regression neural network to make its performance improve. The modification affects the learning procedure only, hence the extension may be easily omitted during evaluation without any change in prediction. It means that the modified model may be evaluated as quickly as the original one but tends to perform better.   This improvement is possible because the modification gives better expressive power, provides better behaved gradients and works as a regularization. The knowledge gained by the temporarily extended neural network is contained in the parameters shared with the original neural network.   The only cost is an increase in learning time.","http://arxiv.org/pdf/1612.01589v1","cs.LG	cs.AI	cs.NE	stat.ML","Improving the Performance of Neural Networks in Regression Tasks Using   Drawering","konrad.zolna@im.uj.edu.pl,	konrad.zolna@rtbhouse.com"
"Yujia Li	Kevin Swersky	Richard Zemel","17","12","2014","A key element in transfer learning is representation learning; if representations can be developed that expose the relevant factors underlying the data, then new tasks and domains can be learned readily based on mappings of these salient factors. We propose that an important aim for these representations are to be unbiased. Different forms of representation learning can be derived from alternative definitions of unwanted bias, e.g., bias to particular tasks, domains, or irrelevant underlying data dimensions. One very useful approach to estimating the amount of bias in a representation comes from maximum mean discrepancy (MMD) [5], a measure of distance between probability distributions. We are not the first to suggest that MMD can be a useful criterion in developing representations that apply across multiple domains or tasks [1]. However, in this paper we describe a number of novel applications of this criterion that we have devised, all based on the idea of developing unbiased representations. These formulations include: a standard domain adaptation framework; a method of learning invariant representations; an approach based on noise-insensitive autoencoders; and a novel form of generative model.","http://arxiv.org/pdf/1412.5244v1","cs.LG	cs.AI	cs.NE	stat.ML","Learning unbiased features","yujiali@cs.toronto.edu	kswersky@cs.toronto.edu	zemel@cs.toronto.edu"
"David Balduzzi	Muhammad Ghifary","10","9","2015","This paper proposes GProp, a deep reinforcement learning algorithm for continuous policies with compatible function approximation. The algorithm is based on two innovations. Firstly, we present a temporal-difference based method for learning the gradient of the value-function. Secondly, we present the deviator-actor-critic (DAC) model, which comprises three neural networks that estimate the value function, its gradient, and determine the actor's policy respectively. We evaluate GProp on two challenging tasks: a contextual bandit problem constructed from nonparametric regression datasets that is designed to probe the ability of reinforcement learning algorithms to accurately estimate gradients; and the octopus arm, a challenging reinforcement learning benchmark. GProp is competitive with fully supervised methods on the bandit task and achieves the best performance to date on the octopus arm.","http://arxiv.org/pdf/1509.03005v1","cs.LG	cs.AI	cs.NE	stat.ML","Compatible Value Gradients for Reinforcement Learning of Continuous Deep   Policies","david.balduzzi@vuw.ac.nz	muhammad.ghifary@ecs.vuw.ac.nz"
"Takayuki Osogami	Makoto Otsuka","29","9","2015","We propose a particularly structured Boltzmann machine, which we refer to as a dynamic Boltzmann machine (DyBM), as a stochastic model of a multi-dimensional time-series. The DyBM can have infinitely many layers of units but allows exact and efficient inference and learning when its parameters have a proposed structure. This proposed structure is motivated by postulates and observations, from biological neural networks, that the synaptic weight is strengthened or weakened, depending on the timing of spikes (i.e., spike-timing dependent plasticity or STDP). We show that the learning rule of updating the parameters of the DyBM in the direction of maximizing the likelihood of given time-series can be interpreted as STDP with long term potentiation and long term depression. The learning rule has a guarantee of convergence and can be performed in a distributed matter (i.e., local in space) with limited memory (i.e., local in time).","http://arxiv.org/pdf/1509.08634v1","cs.NE	cs.AI	cs.LG	stat.ML","Learning dynamic Boltzmann machines with spike-timing dependent   plasticity",""
"Yujia Li	Daniel Tarlow	Marc Brockschmidt	Richard Zemel","17","11","2015","Graph-structured data appears frequently in domains including chemistry, natural language semantics, social networks, and knowledge bases. In this work, we study feature learning techniques for graph-structured inputs. Our starting point is previous work on Graph Neural Networks (Scarselli et al., 2009), which we modify to use gated recurrent units and modern optimization techniques and then extend to output sequences. The result is a flexible and broadly useful class of neural network models that has favorable inductive biases relative to purely sequence-based models (e.g., LSTMs) when the problem is graph-structured. We demonstrate the capabilities on some simple AI (bAbI) and graph algorithm learning tasks. We then show it achieves state-of-the-art performance on a problem from program verification, in which subgraphs need to be matched to abstract data structures.","http://arxiv.org/pdf/1511.05493v4","cs.LG	cs.AI	cs.NE	stat.ML","Gated Graph Sequence Neural Networks","yujiali@cs.toronto.edu	zemel@cs.toronto.edu	mabrocks@microsoft.com	dtarlow@microsoft.com"
"Gabriel Dulac-Arnold	Richard Evans	Hado van Hasselt	Peter Sunehag	Timothy Lillicrap	Jonathan Hunt	Timothy Mann	Theophane Weber	Thomas Degris	Ben Coppin","24","12","2015","Being able to reason in an environment with a large number of discrete actions is essential to bringing reinforcement learning to a larger class of problems. Recommender systems, industrial plants and language models are only some of the many real-world tasks involving large numbers of discrete actions for which current methods are difficult or even often impossible to apply. An ability to generalize over the set of actions as well as sub-linear complexity relative to the size of the set are both necessary to handle such tasks. Current approaches are not able to provide both of these, which motivates the work in this paper. Our proposed approach leverages prior information about the actions to embed them in a continuous space upon which it can generalize. Additionally, approximate nearest-neighbor methods allow for logarithmic-time lookup complexity relative to the number of actions, which is necessary for time-wise tractable training. This combined approach allows reinforcement learning methods to be applied to large-scale learning problems previously intractable with current methods. We demonstrate our algorithm's abilities on a series of tasks having up to one million actions.","http://arxiv.org/pdf/1512.07679v2","cs.AI	cs.LG	cs.NE	stat.ML","Deep Reinforcement Learning in Large Discrete Action Spaces",""
"Aviv Tamar	Yi Wu	Garrett Thomas	Sergey Levine	Pieter Abbeel","9","2","2016","We introduce the value iteration network (VIN): a fully differentiable neural network with a `planning module' embedded within. VINs can learn to plan, and are suitable for predicting outcomes that involve planning-based reasoning, such as policies for reinforcement learning. Key to our approach is a novel differentiable approximation of the value-iteration algorithm, which can be represented as a convolutional neural network, and trained end-to-end using standard backpropagation. We evaluate VIN based policies on discrete and continuous path-planning domains, and on a natural-language based search task. We show that by learning an explicit planning computation, VIN policies generalize better to new, unseen domains.","http://arxiv.org/pdf/1602.02867v4","cs.AI	cs.LG	cs.NE	stat.ML","Value Iteration Networks",""
"Mikael Henaff	Arthur Szlam	Yann LeCun","22","2","2016","Although RNNs have been shown to be powerful tools for processing sequential data, finding architectures or optimization strategies that allow them to model very long term dependencies is still an active area of research. In this work, we carefully analyze two synthetic datasets originally outlined in (Hochreiter and Schmidhuber, 1997) which are used to evaluate the ability of RNNs to store information over many time steps. We explicitly construct RNN solutions to these problems, and using these constructions, illuminate both the problems themselves and the way in which RNNs store different types of information in their hidden states. These constructions furthermore explain the success of recent methods that specify unitary initializations or constraints on the transition matrices.","http://arxiv.org/pdf/1602.06662v2","cs.NE	cs.AI	cs.LG	stat.ML","Recurrent Orthogonal Networks and Long-Memory Tasks",""
"Hado van Hasselt	Arthur Guez	Matteo Hessel	Volodymyr Mnih	David Silver","24","2","2016","Most learning algorithms are not invariant to the scale of the function that is being approximated. We propose to adaptively normalize the targets used in learning. This is useful in value-based reinforcement learning, where the magnitude of appropriate value approximations can change over time when we update the policy of behavior. Our main motivation is prior work on learning to play Atari games, where the rewards were all clipped to a predetermined range. This clipping facilitates learning across many different games with a single learning algorithm, but a clipped reward function can result in qualitatively different behavior. Using the adaptive normalization we can remove this domain-specific heuristic without diminishing overall performance.","http://arxiv.org/pdf/1602.07714v2","cs.LG	cs.AI	cs.NE	stat.ML","Learning values across many orders of magnitude",""
"Laura Deming	Sasha Targ	Nate Sauder	Diogo Almeida	Chun Jimmie Ye","23","5","2016","Each human genome is a 3 billion base pair set of encoding instructions. Decoding the genome using deep learning fundamentally differs from most tasks, as we do not know the full structure of the data and therefore cannot design architectures to suit it. As such, architectures that fit the structure of genomics should be learned not prescribed. Here, we develop a novel search algorithm, applicable across domains, that discovers an optimal architecture which simultaneously learns general genomic patterns and identifies the most important sequence motifs in predicting functional genomic outcomes. The architectures we find using this algorithm succeed at using only RNA expression data to predict gene regulatory structure, learn human-interpretable visualizations of key sequence motifs, and surpass state-of-the-art results on benchmark genomics challenges.","http://arxiv.org/pdf/1605.07156v1","cs.LG	cs.AI	cs.NE	stat.ML","Genetic Architect: Discovering Genomic Structure with Learned Neural   Architectures","ldeming.www@gmail.com,	sasha.targ@ucsf.edu	nate@enlitic.com,	diogo@enlitic.com,	jimmie.ye@ucsf.edu"
"Tejas D. Kulkarni	Ardavan Saeedi	Simanta Gautam	Samuel J. Gershman","8","6","2016","Learning robust value functions given raw observations and rewards is now possible with model-free and model-based deep reinforcement learning algorithms. There is a third alternative, called Successor Representations (SR), which decomposes the value function into two components -- a reward predictor and a successor map. The successor map represents the expected future state occupancy from any given state and the reward predictor maps states to scalar rewards. The value function of a state can be computed as the inner product between the successor map and the reward weights. In this paper, we present DSR, which generalizes SR within an end-to-end deep reinforcement learning framework. DSR has several appealing properties including: increased sensitivity to distal reward changes due to factorization of reward and world dynamics, and the ability to extract bottleneck states (subgoals) given successor maps trained under a random policy. We show the efficacy of our approach on two diverse environments given raw pixel observations -- simple grid-world domains (MazeBase) and the Doom game engine.","http://arxiv.org/pdf/1606.02396v1","stat.ML	cs.AI	cs.LG	cs.NE","Deep Successor Reinforcement Learning","tejask@mit.edu	ardavans@mit.edu	simanta@mit.edu	gershman@fas.harvard.edu"
"Yan Duan	John Schulman	Xi Chen	Peter L. Bartlett	Ilya Sutskever	Pieter Abbeel","9","11","2016","Deep reinforcement learning (deep RL) has been successful in learning sophisticated behaviors automatically; however, the learning process requires a huge number of trials. In contrast, animals can learn new tasks in just a few trials, benefiting from their prior knowledge about the world. This paper seeks to bridge this gap. Rather than designing a ""fast"" reinforcement learning algorithm, we propose to represent it as a recurrent neural network (RNN) and learn it from data. In our proposed method, RL$^2$, the algorithm is encoded in the weights of the RNN, which are learned slowly through a general-purpose (""slow"") RL algorithm. The RNN receives all information a typical RL algorithm would receive, including observations, actions, rewards, and termination flags; and it retains its state across episodes in a given Markov Decision Process (MDP). The activations of the RNN store the state of the ""fast"" RL algorithm on the current (previously unseen) MDP. We evaluate RL$^2$ experimentally on both small-scale and large-scale problems. On the small-scale side, we train it to solve randomly generated multi-arm bandit problems and finite MDPs. After RL$^2$ is trained, its performance on new MDPs is close to human-designed algorithms with optimality guarantees. On the large-scale side, we test RL$^2$ on a vision-based navigation task and show that it scales up to high-dimensional problems.","http://arxiv.org/pdf/1611.02779v2","cs.AI	cs.LG	cs.NE	stat.ML","RL$^2$: Fast Reinforcement Learning via Slow Reinforcement Learning","rocky	joschu	peter}@openai.com	peter@berkeley.edu,	ilyasu@openai.com	pieter@openai.com"
"Jasmine Collins	Jascha Sohl-Dickstein	David Sussillo","29","11","2016","Two potential bottlenecks on the expressiveness of recurrent neural networks (RNNs) are their ability to store information about the task in their parameters, and to store information about the input history in their units. We show experimentally that all common RNN architectures achieve nearly the same per-task and per-unit capacity bounds with careful training, for a variety of tasks and stacking depths. They can store an amount of task information which is linear in the number of parameters, and is approximately 5 bits per parameter. They can additionally store approximately one real number from their input history per hidden unit. We further find that for several tasks it is the per-task parameter capacity bound that determines performance. These results suggest that many previous results comparing RNN architectures are driven primarily by differences in training effectiveness, rather than differences in capacity. Supporting this observation, we compare training difficulty for several architectures, and show that vanilla RNNs are far more difficult to train, yet have slightly higher capacity. Finally, we propose two novel RNN architectures, one of which is easier to train than the LSTM or GRU for deeply stacked architectures.","http://arxiv.org/pdf/1611.09913v3","stat.ML	cs.AI	cs.LG	cs.NE","Capacity and Trainability in Recurrent Neural Networks","jlcollins@google.com	jaschasd@google.com	sussillo@google.com"
"Mohammad Taha Bahadori	Krzysztof Chalupka	Edward Choi	Robert Chen	Walter F. Stewart	Jimeng Sun","8","2","2017","In application domains such as healthcare, we want accurate predictive models that are also causally interpretable. In pursuit of such models, we propose a causal regularizer to steer predictive models towards causally-interpretable solutions and theoretically study its properties. In a large-scale analysis of Electronic Health Records (EHR), our causally-regularized model outperforms its L1-regularized counterpart in causal accuracy and is competitive in predictive performance. We perform non-linear causality analysis by causally regularizing a special neural network architecture. We also show that the proposed causal regularizer can be used together with neural representation learning algorithms to yield up to 20% improvement over multilayer perceptron in detecting multivariate causation, a situation common in healthcare, where many causal factors should occur simultaneously to have an effect on the target variable.","http://arxiv.org/pdf/1702.02604v2","cs.LG	cs.AI	cs.NE	stat.ML","Causal Regularization",""
"Dario Garcia-Gasulla	Ferran Parés	Armand Vilalta	Jonatan Moreno	Eduard Ayguadé	Jesús Labarta	Ulises Cortés	Toyotaro Suzumura","3","3","2017","Deep neural networks are representation learning techniques. During training, a deep net is capable of generating a descriptive language of unprecedented size and detail in machine learning. Extracting the descriptive language coded within a trained CNN model (in the case of image data), and reusing it for other purposes is a field of interest, as it provides access to the visual descriptors previously learnt by the CNN after processing millions of images, without requiring an expensive training phase. Contributions to this field (commonly known as feature representation transfer or transfer learning) have been purely empirical so far, extracting all CNN features from a single layer close to the output and testing their performance by feeding them to a classifier. This approach has provided consistent results, although its relevance is limited to classification tasks. In a completely different approach, in this paper we statistically measure the discriminative power of every single feature found within a deep CNN, when used for characterizing every class of 11 datasets. We seek to provide new insights into the behavior of CNN features, particularly the ones from convolutional layers, as this can be relevant for their application to knowledge representation and reasoning. Our results confirm that low and middle level features may behave differently to high level features, but only under certain conditions. We find that all CNN features can be used for knowledge representation purposes both by their presence or by their absence, doubling the information a single CNN feature may provide. We also study how much noise these features may include, and propose a thresholding approach to discard most of it. All these insights have a direct application to the generation of CNN embedding spaces.","http://arxiv.org/pdf/1703.01127v4","cs.NE	cs.AI	cs.LG	stat.ML","On the Behavior of Convolutional Nets for Feature Extraction","dario.garcia@bsc.es	ferran.pares@bsc.es	armand.vilalta@bsc.es"
"Aditya Grover	Manik Dhar	Stefano Ermon","24","5","2017","Adversarial learning of probabilistic models has recently emerged as a promising alternative to maximum likelihood. Implicit models such as generative adversarial networks (GAN) often generate better samples compared to explicit models trained by maximum likelihood. Yet, GANs sidestep the characterization of an explicit density which makes quantitative evaluations challenging. To bridge this gap, we propose Flow-GANs, a generative adversarial network for which we can perform exact likelihood evaluation, thus supporting both adversarial and maximum likelihood training. When trained adversarially, Flow-GANs generate high-quality samples but attain extremely poor log-likelihood scores, inferior even to a mixture model memorizing the training data; the opposite is true when trained by maximum likelihood. Results on MNIST and CIFAR-10 demonstrate that hybrid training can attain high held-out likelihoods while retaining visual fidelity in the generated samples.","http://arxiv.org/pdf/1705.08868v2","cs.LG	cs.AI	cs.NE	stat.ML","Flow-GAN: Combining Maximum Likelihood and Adversarial Learning in   Generative Models","adityag@cs.stanford.edu	dmanik@cs.stanford.edu	ermon@cs.stanford.edu"
"Chris J. Maddison	Dieterich Lawson	George Tucker	Nicolas Heess	Mohammad Norouzi	Andriy Mnih	Arnaud Doucet	Yee Whye Teh","25","5","2017","When used as a surrogate objective for maximum likelihood estimation in latent variable models, the evidence lower bound (ELBO) produces state-of-the-art results. Inspired by this, we consider the extension of the ELBO to a family of lower bounds defined by a particle filter's estimator of the marginal likelihood, the filtering variational objectives (FIVOs). FIVOs take the same arguments as the ELBO, but can exploit a model's sequential structure to form tighter bounds. We present results that relate the tightness of FIVO's bound to the variance of the particle filter's estimator by considering the generic case of bounds defined as log-transformed likelihood estimators. Experimentally, we show that training with FIVO results in substantial improvements over training the same model architecture with the ELBO on sequential data.","http://arxiv.org/pdf/1705.09279v3","cs.LG	cs.AI	cs.NE	stat.ML","Filtering Variational Objectives","cmaddis@google.com	dieterichl@google.com	gjt@google.com"
"Jiaxin Shi	Shengyang Sun	Jun Zhu","29","5","2017","Recent progress in variational inference has paid much attention to the flexibility of variational posteriors. One promising direction is to use implicit distributions, i.e., distributions without tractable densities as the variational posterior. However, existing methods on implicit posteriors still face challenges of noisy estimation and computational infeasibility when applied to models with high-dimensional latent variables. In this paper, we present a new approach named Kernel Implicit Variational Inference that addresses these challenges. As far as we know, for the first time implicit variational inference is successfully applied to Bayesian neural networks, which shows promising results on both regression and classification tasks.","http://arxiv.org/pdf/1705.10119v3","stat.ML	cs.AI	cs.LG	cs.NE","Kernel Implicit Variational Inference","shijx15@mails.tsinghua.edu.cn,	ssy@cs.toronto.edu,	dcszj@tsinghua.edu.cn"
"Julien Perez	Tomi Silander","31","5","2017","Partially observable environments present an important open challenge in the domain of sequential control learning with delayed rewards. Despite numerous attempts during the two last decades, the majority of reinforcement learning algorithms and associated approximate models, applied to this context, still assume Markovian state transitions. In this paper, we explore the use of a recently proposed attention-based model, the Gated End-to-End Memory Network, for sequential control. We call the resulting model the Gated End-to-End Memory Policy Network. More precisely, we use a model-free value-based algorithm to learn policies for partially observed domains using this memory-enhanced neural network. This model is end-to-end learnable and it features unbounded memory. Indeed, because of its attention mechanism and associated non-parametric memory, the proposed model allows us to define an attention mechanism over the observation stream unlike recurrent models. We show encouraging results that illustrate the capability of our attention-based model in the context of the continuous-state non-stationary control problem of stock trading. We also present an OpenAI Gym environment for simulated stock exchange and explain its relevance as a benchmark for the field of non-Markovian decision process learning.","http://arxiv.org/pdf/1705.10993v1","stat.ML	cs.AI	cs.LG	cs.NE","Non-Markovian Control with Gated End-to-End Memory Policy Networks","julien.perez@xrce.xerox.com	tomi.silander@xrce.xerox.com"
"Emmanuel Dufourq	Bruce A. Bassett","3","7","2017","Regression or classification? This is perhaps the most basic question faced when tackling a new supervised learning problem. We present an Evolutionary Deep Learning (EDL) algorithm that automatically solves this by identifying the question type with high accuracy, along with a proposed deep architecture. Typically, a significant amount of human insight and preparation is required prior to executing machine learning algorithms. For example, when creating deep neural networks, the number of parameters must be selected in advance and furthermore, a lot of these choices are made based upon pre-existing knowledge of the data such as the use of a categorical cross entropy loss function. Humans are able to study a dataset and decide whether it represents a classification or a regression problem, and consequently make decisions which will be applied to the execution of the neural network. We propose the Automated Problem Identification (API) algorithm, which uses an evolutionary algorithm interface to TensorFlow to manipulate a deep neural network to decide if a dataset represents a classification or a regression problem. We test API on 16 different classification, regression and sentiment analysis datasets with up to 10,000 features and up to 17,000 unique target values. API achieves an average accuracy of $96.3\%$ in identifying the problem type without hardcoding any insights about the general characteristics of regression or classification problems. For example, API successfully identifies classification problems even with 1000 target values. Furthermore, the algorithm recommends which loss function to use and also recommends a neural network architecture. Our work is therefore a step towards fully automated machine learning.","http://arxiv.org/pdf/1707.00703v1","cs.NE	cs.AI	cs.LG	stat.ML","Automated Problem Identification: Regression vs Classification via   Evolutionary Deep Networks","edufourq@gmail.com,	bruce.a.bassett@gmail.com"
"Nikhil Mishra	Mostafa Rohaninejad	Xi Chen	Pieter Abbeel","11","7","2017","Deep neural networks excel in regimes with large amounts of data, but tend to struggle when data is scarce or when they need to adapt quickly to changes in the task. In response, recent work in meta-learning proposes training a meta-learner on a distribution of similar tasks, in the hopes of generalization to novel but related tasks by learning a high-level strategy that captures the essence of the problem it is asked to solve. However, many recent meta-learning approaches are extensively hand-designed, either using architectures specialized to a particular application, or hard-coding algorithmic components that constrain how the meta-learner solves the task. We propose a class of simple and generic meta-learner architectures that use a novel combination of temporal convolutions and soft attention; the former to aggregate information from past experience and the latter to pinpoint specific pieces of information. In the most extensive set of meta-learning experiments to date, we evaluate the resulting Simple Neural AttentIve Learner (or SNAIL) on several heavily-benchmarked tasks. On all tasks, in both supervised and reinforcement learning, SNAIL attains state-of-the-art performance by significant margins.","http://arxiv.org/pdf/1707.03141v3","cs.AI	cs.LG	cs.NE	stat.ML","A Simple Neural Attentive Meta-Learner","nmishra@berkeley.edu	rohaninejadm@berkeley.edu	c.xi@berkeley.edu	pabbeel@berkeley.edu"
"Simone Scardapane	Steven Van Vaerenbergh	Simone Totaro	Aurelio Uncini","13","7","2017","Neural networks are generally built by interleaving (adaptable) linear layers with (fixed) nonlinear activation functions. To increase their flexibility, several authors have proposed methods for adapting the activation functions themselves, endowing them with varying degrees of flexibility. None of these approaches, however, have gained wide acceptance in practice, and research in this topic remains open. In this paper, we introduce a novel family of flexible activation functions that are based on an inexpensive kernel expansion at every neuron. Leveraging over several properties of kernel-based models, we propose multiple variations for designing and initializing these kernel activation functions (KAFs), including a multidimensional scheme allowing to nonlinearly combine information from different paths in the network. The resulting KAFs can approximate any mapping defined over a subset of the real line, either convex or nonconvex. Furthermore, they are smooth over their entire domain, linear in their parameters, and they can be regularized using any known scheme, including the use of $\ell_1$ penalties to enforce sparseness. To the best of our knowledge, no other known model satisfies all these properties simultaneously. In addition, we provide a relatively complete overview on alternative techniques for adapting the activation functions, which is currently lacking in the literature. A large set of experiments validates our proposal.","http://arxiv.org/pdf/1707.04035v2","stat.ML	cs.AI	cs.LG	cs.NE","Kafnets: kernel-based non-parametric activation functions for neural   networks",""
"Razvan Pascanu	Yujia Li	Oriol Vinyals	Nicolas Heess	Lars Buesing	Sebastien Racanière	David Reichert	Théophane Weber	Daan Wierstra	Peter Battaglia","19","7","2017","Conventional wisdom holds that model-based planning is a powerful approach to sequential decision-making. It is often very challenging in practice, however, because while a model can be used to evaluate a plan, it does not prescribe how to construct a plan. Here we introduce the ""Imagination-based Planner"", the first model-based, sequential decision-making agent that can learn to construct, evaluate, and execute plans. Before any action, it can perform a variable number of imagination steps, which involve proposing an imagined action and evaluating it with its model-based imagination. All imagined actions and outcomes are aggregated, iteratively, into a ""plan context"" which conditions future real and imagined actions. The agent can even decide how to imagine: testing out alternative imagined actions, chaining sequences of actions together, or building a more complex ""imagination tree"" by navigating flexibly among the previously imagined states using a learned policy. And our agent can learn to plan economically, jointly optimizing for external rewards and computational costs associated with using its imagination. We show that our architecture can learn to solve a challenging continuous control problem, and also learn elaborate planning strategies in a discrete maze-solving task. Our work opens a new direction toward learning the components of a model-based planning system and how to use them.","http://arxiv.org/pdf/1707.06170v1","cs.AI	cs.LG	cs.NE	stat.ML","Learning model-based planning from scratch","razp@google.com	yujiali@google.com	vinyals@google.com	heess@google.com	lbuesing@google.com	sracaniere@google.com	@google.com	 reichert@google.com	theophane@google.com	wierstra@google.com	peterbattaglia@google.com"
"Isabeau Prémont-Schwarz	Alexander Ilin	Tele Hotloo Hao	Antti Rasmus	Rinu Boney	Harri Valpola","28","7","2017","We propose a recurrent extension of the Ladder networks whose structure is motivated by the inference required in hierarchical latent variable models. We demonstrate that the recurrent Ladder is able to handle a wide variety of complex learning tasks that benefit from iterative inference and temporal modeling. The architecture shows close-to-optimal results on temporal modeling of video data, competitive results on music modeling, and improved perceptual grouping based on higher order abstractions, such as stochastic textures and motion cues. We present results for fully supervised, semi-supervised, and unsupervised tasks. The results suggest that the proposed architecture and principles are powerful tools for learning a hierarchy of abstractions, learning iterative inference and handling temporal information.","http://arxiv.org/pdf/1707.09219v4","cs.NE	cs.AI	cs.LG	stat.ML","Recurrent Ladder Networks","isabeau@cai.fi	alexilin@cai.fi	hotloo@cai.fi	antti@cai.fi	rinu@cai.fi	harri@cai.fi"
"Kenji Kawaguchi	Leslie Pack Kaelbling	Yoshua Bengio","16","10","2017","With a direct analysis of neural networks, this paper presents a mathematically tight generalization theory to partially address an open problem regarding the generalization of deep learning. Unlike previous bound-based theory, our main theory is quantitatively as tight as possible for every dataset individually, while producing qualitative insights competitively. Our results give insight into why and how deep learning can generalize well, despite its large capacity, complexity, possible algorithmic instability, nonrobustness, and sharp minima, answering to an open question in the literature. We also discuss limitations of our results and propose additional open problems.","http://arxiv.org/pdf/1710.05468v3","stat.ML	cs.AI	cs.LG	cs.NE","Generalization in Deep Learning",""
"Yannic Kilcher	Gary Becigneul	Thomas Hofmann","31","10","2017","It is commonly agreed that the use of relevant invariances as a good statistical bias is important in machine-learning. However, most approaches that explicitly incorporate invariances into a model architecture only make use of very simple transformations, such as translations and rotations. Hence, there is a need for methods to model and extract richer transformations that capture much higher-level invariances. To that end, we introduce a tool allowing to parametrize the set of filters of a trained convolutional neural network with the latent space of a generative adversarial network. We then show that the method can capture highly non-linear invariances of the data by visualizing their effect in the data space.","http://arxiv.org/pdf/1710.11386v1","cs.LG	cs.AI	cs.NE	stat.ML","Parametrizing filters of a CNN with a GAN","yannic.kilcher@inf.ethz.ch	gary.becigneul@inf.ethz.ch	thomas.hofmann@inf.ethz.ch"
"Zhen He	Shaobing Gao	Liang Xiao	Daxue Liu	Hangen He	David Barber","5","11","2017","Long Short-Term Memory (LSTM) is a popular approach to boosting the ability of Recurrent Neural Networks to store longer term temporal information. The capacity of an LSTM network can be increased by widening and adding layers. However, usually the former introduces additional parameters, while the latter increases the runtime. As an alternative we propose the Tensorized LSTM in which the hidden states are represented by tensors and updated via a cross-layer convolution. By increasing the tensor size, the network can be widened efficiently without additional parameters since the parameters are shared across different locations in the tensor; by delaying the output, the network can be deepened implicitly with little additional runtime since deep computations for each timestep are merged into temporal computations of the sequence. Experiments conducted on five challenging sequence learning tasks show the potential of the proposed model.","http://arxiv.org/pdf/1711.01577v3","stat.ML	cs.AI	cs.LG	cs.NE","Wider and Deeper, Cheaper and Faster: Tensorized LSTMs for Sequence   Learning",""
"Shruti R. Kulkarni	John M. Alexiades	Bipin Rajendran","9","11","2017","We describe a novel spiking neural network (SNN) for automated, real-time handwritten digit classification and its implementation on a GP-GPU platform. Information processing within the network, from feature extraction to classification is implemented by mimicking the basic aspects of neuronal spike initiation and propagation in the brain. The feature extraction layer of the SNN uses fixed synaptic weight maps to extract the key features of the image and the classifier layer uses the recently developed NormAD approximate gradient descent based supervised learning algorithm for spiking neural networks to adjust the synaptic weights. On the standard MNIST database images of handwritten digits, our network achieves an accuracy of 99.80% on the training set and 98.06% on the test set, with nearly 7x fewer parameters compared to the state-of-the-art spiking networks. We further use this network in a GPU based user-interface system demonstrating real-time SNN simulation to infer digits written by different users. On a test set of 500 such images, this real-time platform achieves an accuracy exceeding 97% while making a prediction within an SNN emulation time of less than 100ms.","http://arxiv.org/pdf/1711.03637v1","stat.ML	cs.AI	cs.LG	cs.NE","Learning and Real-time Classification of Hand-written Digits With   Spiking Neural Networks","srk68@njit.edu	jma59@njit.edu	bipin@njit.edu"
"Joan Serrà	Dídac Surís	Marius Miron	Alexandros Karatzoglou","4","1","2018","Catastrophic forgetting occurs when a neural network loses the information learned in a previous task after training on subsequent tasks. This problem remains a hurdle for artificial intelligence systems with sequential learning capabilities. In this paper, we propose a task-based hard attention mechanism that preserves previous tasks' information without affecting the current task's learning. A hard attention mask is learned concurrently to every task, through stochastic gradient descent, and previous masks are exploited to condition such learning. We show that the proposed mechanism is effective for reducing catastrophic forgetting, cutting current rates by 45 to 80%. We also show that it is robust to different hyperparameter choices, and that it offers a number of monitoring capabilities. The approach features the possibility to control both the stability and compactness of the learned knowledge, which we believe makes it also attractive for online learning or network compression applications.","http://arxiv.org/pdf/1801.01423v2","cs.LG	cs.AI	cs.NE	stat.ML","Overcoming catastrophic forgetting with hard attention to the task",""
"Zachary C. Lipton	Yu-Xiang Wang	Alex Smola","12","2","2018","Faced with distribution shift between training and test set, we wish to detect and quantify the shift, and to correct our classifiers without test set labels. Motivated by medical diagnosis, where diseases (targets), cause symptoms (observations), we focus on label shift, where the label marginal $p(y)$ changes but the conditional $p(x|y)$ does not. We propose Black Box Shift Estimation (BBSE) to estimate the test distribution $p(y)$. BBSE exploits arbitrary black box predictors to reduce dimensionality prior to shift correction. While better predictors give tighter estimates, BBSE works even when predictors are biased, inaccurate, or uncalibrated, so long as their confusion matrices are invertible. We prove BBSE's consistency, bound its error, and introduce a statistical test that uses BBSE to detect shift. We also leverage BBSE to correct classifiers. Experiments demonstrate accurate estimates and improved prediction, even on high-dimensional datasets of natural images.","http://arxiv.org/pdf/1802.03916v2","cs.LG	cs.AI	cs.NE	stat.ML","Detecting and Correcting for Label Shift with Black Box Predictors","zlipton@cmu.edu,	yuxiangw@amazon.com,	smola@amazon.com"
"Kenji Kawaguchi	Yoshua Bengio","21","2","2018","This paper introduces a novel measure-theoretic learning theory to analyze generalization behaviors of practical interest. The proposed learning theory has the following abilities: 1) to utilize the qualities of each learned representation on the path from raw inputs to outputs in representation learning, 2) to guarantee good generalization errors possibly with arbitrarily rich hypothesis spaces (e.g., arbitrarily large capacity and Rademacher complexity) and non-stable/non-robust learning algorithms, and 3) to clearly distinguish each individual problem instance from each other. Our generalization bounds are relative to a representation of the data, and hold true even if the representation is learned. We discuss several consequences of our results on deep learning, one-shot learning and curriculum learning. Unlike statistical learning theory, the proposed learning theory analyzes each problem instance individually via measure theory, rather than a set of problem instances via statistics. Because of the differences in the assumptions and the objectives, the proposed learning theory is meant to be complementary to previous learning theory and is not designed to compete with it.","http://arxiv.org/pdf/1802.07426v1","stat.ML	cs.AI	cs.LG	cs.NE","Generalization in Machine Learning via Analytical Learning Theory",""
"Roman Novak	Yasaman Bahri	Daniel A. Abolafia	Jeffrey Pennington	Jascha Sohl-Dickstein","23","2","2018","In practice it is often found that large over-parameterized neural networks generalize better than their smaller counterparts, an observation that appears to conflict with classical notions of function complexity, which typically favor smaller models. In this work, we investigate this tension between complexity and generalization through an extensive empirical exploration of two natural metrics of complexity related to sensitivity to input perturbations. Our experiments survey thousands of models with various fully-connected architectures, optimizers, and other hyper-parameters, as well as four different image classification datasets.   We find that trained neural networks are more robust to input perturbations in the vicinity of the training data manifold, as measured by the norm of the input-output Jacobian of the network, and that it correlates well with generalization. We further establish that factors associated with poor generalization $-$ such as full-batch training or using random labels $-$ correspond to lower robustness, while factors associated with good generalization $-$ such as data augmentation and ReLU non-linearities $-$ give rise to more robust functions. Finally, we demonstrate how the input-output Jacobian norm can be predictive of generalization at the level of individual test points.","http://arxiv.org/pdf/1802.08760v1","stat.ML	cs.AI	cs.LG	cs.NE","Sensitivity and Generalization in Neural Networks: an Empirical Study","romann@google.com	yasamanb@google.com	danabo@google.com	jpennin@google.com	jaschasd@google.com"
"Ari S. Morcos	David G. T. Barrett	Neil C. Rabinowitz	Matthew Botvinick","19","3","2018","Despite their ability to memorize large datasets, deep neural networks often achieve good generalization performance. However, the differences between the learned solutions of networks which generalize and those which do not remain unclear. Additionally, the tuning properties of single directions (defined as the activation of a single unit or some linear combination of units in response to some input) have been highlighted, but their importance has not been evaluated. Here, we connect these lines of inquiry to demonstrate that a network's reliance on single directions is a good predictor of its generalization performance, across networks trained on datasets with different fractions of corrupted labels, across ensembles of networks trained on datasets with unmodified labels, across different hyperparameters, and over the course of training. While dropout only regularizes this quantity up to a point, batch normalization implicitly discourages single direction reliance, in part by decreasing the class selectivity of individual units. Finally, we find that class selectivity is a poor predictor of task importance, suggesting not only that networks which generalize well minimize their dependence on individual units by reducing their selectivity, but also that individually selective units may not be necessary for strong network performance.","http://arxiv.org/pdf/1803.06959v1","stat.ML	cs.AI	cs.LG	cs.NE","On the importance of single directions for generalization","arimorcos@google.com	barrettdavid@google.com	ncr@google.com	botvinick@google.com	arimorcos@google.com"
"Srinivas C. Turaga	Kevin L. Briggman	Moritz Helmstaedter	Winfried Denk	H. Sebastian Seung","28","11","2009","Images can be segmented by first using a classifier to predict an affinity graph that reflects the degree to which image pixels must be grouped together and then partitioning the graph to yield a segmentation. Machine learning has been applied to the affinity classifier to produce affinity graphs that are good in the sense of minimizing edge misclassification rates. However, this error measure is only indirectly related to the quality of segmentations produced by ultimately partitioning the affinity graph. We present the first machine learning algorithm for training a classifier to produce affinity graphs that are good in the sense of producing segmentations that directly minimize the Rand index, a well known segmentation performance measure. The Rand index measures segmentation performance by quantifying the classification of the connectivity of image pixel pairs after segmentation. By using the simple graph partitioning algorithm of finding the connected components of the thresholded affinity graph, we are able to train an affinity classifier to directly minimize the Rand index of segmentations resulting from the graph partitioning. Our learning algorithm corresponds to the learning of maximin affinities between image pixel pairs, which are predictive of the pixel-pair connectivity.","http://arxiv.org/pdf/0911.5372v1","cs.CV	cs.AI	cs.LG	cs.NE","Maximin affinity learning of image segmentation",""
"Sergey S. Tarasenko","14","2","2011","This study is focused on the development of the cortex-like visual object recognition system. We propose a general framework, which consists of three hierarchical levels (modules). These modules functionally correspond to the V1, V4 and IT areas. Both bottom-up and top-down connections between the hierarchical levels V4 and IT are employed. The higher the degree of matching between the input and the preferred stimulus, the shorter the response time of the neuron. Therefore information about a single stimulus is distributed in time and is transmitted by the waves of spikes. The reciprocal connections and waves of spikes implement predictive coding: an initial hypothesis is generated on the basis of information delivered by the first wave of spikes and is tested with the information carried by the consecutive waves. The development is considered as extraction and accumulation of features in V4 and objects in IT. Once stored a feature can be disposed, if rarely activated. This cause update of feature repository. Consequently, objects in IT are also updated. This illustrates the growing process and dynamical change of topological structures of V4, IT and connections between these areas.","http://arxiv.org/pdf/1102.2739v1","cs.CV	cs.AI	cs.LG	cs.NE","A General Framework for Development of the Cortex-like Visual Object   Recognition System: Waves of Spikes, Predictive Coding and Universal   Dictionary of Features",""
"Dan C. Cireşan	Ueli Meier	Luca M. Gambardella	Jürgen Schmidhuber","23","3","2011","The competitive MNIST handwritten digit recognition benchmark has a long history of broken records since 1998. The most recent substantial improvement by others dates back 7 years (error rate 0.4%) . Recently we were able to significantly improve this result, using graphics cards to greatly speed up training of simple but deep MLPs, which achieved 0.35%, outperforming all the previous more complex methods. Here we report another substantial improvement: 0.31% obtained using a committee of MLPs.","http://arxiv.org/pdf/1103.4487v1","cs.LG	cs.AI	cs.CV	cs.NE","Handwritten Digit Recognition with a Committee of Deep Neural Nets on   GPUs",""
"Ridwan Al Iqbal","2","10","2011","Artificial Neural Network is among the most popular algorithm for supervised learning. However, Neural Networks have a well-known drawback of being a ""Black Box"" learner that is not comprehensible to the Users. This lack of transparency makes it unsuitable for many high risk tasks such as medical diagnosis that requires a rational justification for making a decision. Rule Extraction methods attempt to curb this limitation by extracting comprehensible rules from a trained Network. Many such extraction algorithms have been developed over the years with their respective strengths and weaknesses. They have been broadly categorized into three types based on their approach to use internal model of the Network. Eclectic Methods are hybrid algorithms that combine the other approaches to attain more performance. In this paper, we present an Eclectic method called HERETIC. Our algorithm uses Inductive Decision Tree learning combined with information of the neural network structure for extracting logical rules. Experiments and theoretical analysis show HERETIC to be better in terms of speed and performance.","http://arxiv.org/pdf/1110.0214v1","cs.LG	cs.AI	cs.CV	cs.NE","Eclectic Extraction of Propositional Rules from Neural Networks","ridwan@enosisbd.com"
"Arnab Ghosh	Viveka Kulharia	Vinay Namboodiri","5","12","2016","Communicating and sharing intelligence among agents is an important facet of achieving Artificial General Intelligence. As a first step towards this challenge, we introduce a novel framework for image generation: Message Passing Multi-Agent Generative Adversarial Networks (MPM GANs). While GANs have recently been shown to be very effective for image generation and other tasks, these networks have been limited to mostly single generator-discriminator networks. We show that we can obtain multi-agent GANs that communicate through message passing to achieve better image generation. The objectives of the individual agents in this framework are two fold: a co-operation objective and a competing objective. The co-operation objective ensures that the message sharing mechanism guides the other generator to generate better than itself while the competing objective encourages each generator to generate better than its counterpart. We analyze and visualize the messages that these GANs share among themselves in various scenarios. We quantitatively show that the message sharing formulation serves as a regularizer for the adversarial training. Qualitatively, we show that the different generators capture different traits of the underlying data distribution.","http://arxiv.org/pdf/1612.01294v1","cs.CV	cs.AI	cs.LG	cs.NE","Message Passing Multi-Agent GANs","arnabghosh93	vivekakulharia}@gmail.com	vinaypn@iitk.ac.in"
"Tong Che	Yanran Li	Athul Paul Jacob	Yoshua Bengio	Wenjie Li","7","12","2016","Although Generative Adversarial Networks achieve state-of-the-art results on a variety of generative tasks, they are regarded as highly unstable and prone to miss modes. We argue that these bad behaviors of GANs are due to the very particular functional shape of the trained discriminators in high dimensional spaces, which can easily make training stuck or push probability mass in the wrong direction, towards that of higher concentration than that of the data generating distribution. We introduce several ways of regularizing the objective, which can dramatically stabilize the training of GAN models. We also show that our regularizers can help the fair distribution of probability mass across the modes of the data generating distribution, during the early phases of training and thus providing a unified solution to the missing modes problem.","http://arxiv.org/pdf/1612.02136v5","cs.LG	cs.AI	cs.CV	cs.NE","Mode Regularized Generative Adversarial Networks","tong.che@umontreal.ca	ap.jacob@umontreal.ca	yoshua.bengio@umontreal.ca	csyli@comp.polyu.edu.hk	cswjli@comp.polyu.edu.hk"
"Bharat Singh	Soham De	Yangmuzi Zhang	Thomas Goldstein	Gavin Taylor","15","10","2015","The increasing complexity of deep learning architectures is resulting in training time requiring weeks or even months. This slow training is due in part to vanishing gradients, in which the gradients used by back-propagation are extremely large for weights connecting deep layers (layers near the output layer), and extremely small for shallow layers (near the input layer); this results in slow learning in the shallow layers. Additionally, it has also been shown that in highly non-convex problems, such as deep neural networks, there is a proliferation of high-error low curvature saddle points, which slows down learning dramatically. In this paper, we attempt to overcome the two above problems by proposing an optimization method for training deep neural networks which uses learning rates which are both specific to each layer in the network and adaptive to the curvature of the function, increasing the learning rate at low curvature points. This enables us to speed up learning in the shallow layers of the network and quickly escape high-error low curvature saddle points. We test our method on standard image classification datasets such as MNIST, CIFAR10 and ImageNet, and demonstrate that our method increases accuracy as well as reduces the required training time over standard algorithms.","http://arxiv.org/pdf/1510.04609v1","cs.CV	cs.AI	cs.LG	cs.NE","Layer-Specific Adaptive Learning Rates for Deep Networks","bharat	sohamde	tomg}@cs.umd.edu	ymzhang@umiacs.umd.edu,	taylor@usna.edu"
"Baochen Sun	Jiashi Feng	Kate Saenko","17","11","2015","Unlike human learning, machine learning often fails to handle changes between training (source) and test (target) input distributions. Such domain shifts, common in practical scenarios, severely damage the performance of conventional machine learning methods. Supervised domain adaptation methods have been proposed for the case when the target data have labels, including some that perform very well despite being ""frustratingly easy"" to implement. However, in practice, the target domain is often unlabeled, requiring unsupervised adaptation. We propose a simple, effective, and efficient method for unsupervised domain adaptation called CORrelation ALignment (CORAL). CORAL minimizes domain shift by aligning the second-order statistics of source and target distributions, without requiring any target labels. Even though it is extraordinarily simple--it can be implemented in four lines of Matlab code--CORAL performs remarkably well in extensive evaluations on standard benchmark datasets.","http://arxiv.org/pdf/1511.05547v2","cs.CV	cs.AI	cs.LG	cs.NE","Return of Frustratingly Easy Domain Adaptation","bsun@cs.uml.edu	elefjia@nus.edu.sg	saenko@cs.uml.edu"
"Lukas Cavigelli	Luca Benini","14","12","2015","An ever increasing number of computer vision and image/video processing challenges are being approached using deep convolutional neural networks, obtaining state-of-the-art results in object recognition and detection, semantic segmentation, action recognition, optical flow and superresolution. Hardware acceleration of these algorithms is essential to adopt these improvements in embedded and mobile computer vision systems. We present a new architecture, design and implementation as well as the first reported silicon measurements of such an accelerator, outperforming previous work in terms of power-, area- and I/O-efficiency. The manufactured device provides up to 196 GOp/s on 3.09 mm^2 of silicon in UMC 65nm technology and can achieve a power efficiency of 803 GOp/s/W. The massively reduced bandwidth requirements make it the first architecture scalable to TOp/s performance.","http://arxiv.org/pdf/1512.04295v2","cs.CV	cs.AI	cs.LG	cs.NE	B.7.1; I.2.6","Origami: A 803 GOp/s/W Convolutional Network Accelerator",""
"Aravind S. Lakshminarayanan	Ramnandan Krishnamurthy	Peeyush Kumar	Balaraman Ravindran","17","5","2016","This paper introduces an automated skill acquisition framework in reinforcement learning which involves identifying a hierarchical description of the given task in terms of abstract states and extended actions between abstract states. Identifying such structures present in the task provides ways to simplify and speed up reinforcement learning algorithms. These structures also help to generalize such algorithms over multiple tasks without relearning policies from scratch. We use ideas from dynamical systems to find metastable regions in the state space and associate them with abstract states. The spectral clustering algorithm PCCA+ is used to identify suitable abstractions aligned to the underlying structure. Skills are defined in terms of the sequence of actions that lead to transitions between such abstract states. The connectivity information from PCCA+ is used to generate these skills or options. These skills are independent of the learning task and can be efficiently reused across a variety of tasks defined over the same model. This approach works well even without the exact model of the environment by using sample trajectories to construct an approximate estimate. We also present our approach to scaling the skill acquisition framework to complex tasks with large state spaces for which we perform state aggregation using the representation learned from an action conditional video prediction network and use the skill acquisition framework on the aggregated state space.","http://arxiv.org/pdf/1605.05359v2","cs.LG	cs.AI	cs.CV	cs.NE","Option Discovery in Hierarchical Reinforcement Learning using   Spatio-Temporal Clustering","aravindsrinivas@gmail.com	nandparikrish@gmail.com	agoovi@gmail.com	ravi@cse.iitm.ac.in"
"Andreas Veit	Michael Wilber	Serge Belongie","20","5","2016","In this work we propose a novel interpretation of residual networks showing that they can be seen as a collection of many paths of differing length. Moreover, residual networks seem to enable very deep networks by leveraging only the short paths during training. To support this observation, we rewrite residual networks as an explicit collection of paths. Unlike traditional models, paths through residual networks vary in length. Further, a lesion study reveals that these paths show ensemble-like behavior in the sense that they do not strongly depend on each other. Finally, and most surprising, most paths are shorter than one might expect, and only the short paths are needed during training, as longer paths do not contribute any gradient. For example, most of the gradient in a residual network with 110 layers comes from paths that are only 10-34 layers deep. Our results reveal one of the key characteristics that seem to enable the training of very deep networks: Residual networks avoid the vanishing gradient problem by introducing short paths which can carry gradient throughout the extent of very deep networks.","http://arxiv.org/pdf/1605.06431v2","cs.CV	cs.AI	cs.LG	cs.NE","Residual Networks Behave Like Ensembles of Relatively Shallow Networks","av443@cornell.edu	mjw285@cornell.edu	sjb344@cornell.edu"
"Anh Nguyen	Alexey Dosovitskiy	Jason Yosinski	Thomas Brox	Jeff Clune","30","5","2016","Deep neural networks (DNNs) have demonstrated state-of-the-art results on many pattern recognition tasks, especially vision classification problems. Understanding the inner workings of such computational brains is both fascinating basic science that is interesting in its own right - similar to why we study the human brain - and will enable researchers to further improve DNNs. One path to understanding how a neural network functions internally is to study what each of its neurons has learned to detect. One such method is called activation maximization (AM), which synthesizes an input (e.g. an image) that highly activates a neuron. Here we dramatically improve the qualitative state of the art of activation maximization by harnessing a powerful, learned prior: a deep generator network (DGN). The algorithm (1) generates qualitatively state-of-the-art synthetic images that look almost real, (2) reveals the features learned by each neuron in an interpretable way, (3) generalizes well to new datasets and somewhat well to different network architectures without requiring the prior to be relearned, and (4) can be considered as a high-quality generative method (in this case, by generating novel, creative, interesting, recognizable images).","http://arxiv.org/pdf/1605.09304v5","cs.NE	cs.AI	cs.CV	cs.LG","Synthesizing the preferred inputs for neurons in neural networks via   deep generator networks","anguyen8@uwyo.edu	dosovits@cs.uni-freiburg.de	jason@geometric.ai	brox@cs.uni-freiburg.de	jeffclune@uwyo.edu"
"Rathinakumar Appuswamy	Tapan Nayak	John Arthur	Steven Esser	Paul Merolla	Jeffrey Mckinstry	Timothy Melano	Myron Flickner	Dharmendra Modha","8","6","2016","We derive a relationship between network representation in energy-efficient neuromorphic architectures and block Toplitz convolutional matrices. Inspired by this connection, we develop deep convolutional networks using a family of structured convolutional matrices and achieve state-of-the-art trade-off between energy efficiency and classification accuracy for well-known image recognition tasks. We also put forward a novel method to train binary convolutional networks by utilising an existing connection between noisy-rectified linear units and binary activations.","http://arxiv.org/pdf/1606.02407v1","cs.NE	cs.AI	cs.CV	cs.LG","Structured Convolution Matrices for Energy-efficient Deep learning","rappusw@us.ibm.com	tknayak@us.ibm.com	arthurjo@us.ibm.com	sesser@us.ibm.com	pameroll@us.ibm.com	jlmckins@us.ibm.com	tmelano@us.ibm.com	mdflickner@us.ibm.com	dmodha@us.ibm.com"
"Baochen Sun	Kate Saenko","6","7","2016","Deep neural networks are able to learn powerful representations from large quantities of labeled input data, however they cannot always generalize well across changes in input distributions. Domain adaptation algorithms have been proposed to compensate for the degradation in performance due to domain shift. In this paper, we address the case when the target domain is unlabeled, requiring unsupervised adaptation. CORAL is a ""frustratingly easy"" unsupervised domain adaptation method that aligns the second-order statistics of the source and target distributions with a linear transformation. Here, we extend CORAL to learn a nonlinear transformation that aligns correlations of layer activations in deep neural networks (Deep CORAL). Experiments on standard benchmark datasets show state-of-the-art performance.","http://arxiv.org/pdf/1607.01719v1","cs.CV	cs.AI	cs.LG	cs.NE","Deep CORAL: Correlation Alignment for Deep Domain Adaptation",""
"Jun Liu	Amir Shahroudy	Dong Xu	Gang Wang","24","7","2016","3D action recognition - analysis of human actions based on 3D skeleton data - becomes popular recently due to its succinctness, robustness, and view-invariant representation. Recent attempts on this problem suggested to develop RNN-based learning methods to model the contextual dependency in the temporal domain. In this paper, we extend this idea to spatio-temporal domains to analyze the hidden sources of action-related information within the input data over both domains concurrently. Inspired by the graphical structure of the human skeleton, we further propose a more powerful tree-structure based traversal method. To handle the noise and occlusion in 3D skeleton data, we introduce new gating mechanism within LSTM to learn the reliability of the sequential input data and accordingly adjust its effect on updating the long-term context information stored in the memory cell. Our method achieves state-of-the-art performance on 4 challenging benchmark datasets for 3D human action analysis.","http://arxiv.org/pdf/1607.07043v1","cs.CV	cs.AI	cs.LG	cs.NE","Spatio-Temporal LSTM with Trust Gates for 3D Human Action Recognition","jliu029@ntu.edu.sg	amir3@ntu.edu.sg	wanggang@ntu.edu.sg	dong.xu@sydney.edu.au"
"Suraj Srinivas	R. Venkatesh Babu","21","11","2016","Deep Neural Networks often require good regularizers to generalize well. Dropout is one such regularizer that is widely used among Deep Learning practitioners. Recent work has shown that Dropout can also be viewed as performing Approximate Bayesian Inference over the network parameters. In this work, we generalize this notion and introduce a rich family of regularizers which we call Generalized Dropout. One set of methods in this family, called Dropout++, is a version of Dropout with trainable parameters. Classical Dropout emerges as a special case of this method. Another member of this family selects the width of neural network layers. Experiments show that these methods help in improving generalization performance over Dropout.","http://arxiv.org/pdf/1611.06791v1","cs.LG	cs.AI	cs.CV	cs.NE","Generalized Dropout","surajsrinivas@grads.cds.iisc.ac.in	venky@cds.iisc.ac.in"
"I. Theodorakopoulos	V. Pothos	D. Kastaniotis	N. Fragoulis","18","1","2017","A new, radical CNN design approach is presented in this paper, considering the reduction of the total computational load during inference. This is achieved by a new holistic intervention on both the CNN architecture and the training procedure, which targets to the parsimonious inference by learning to exploit or remove the redundant capacity of a CNN architecture. This is accomplished, by the introduction of a new structural element that can be inserted as an add-on to any contemporary CNN architecture, whilst preserving or even improving its recognition accuracy. Our approach formulates a systematic and data-driven method for developing CNNs that are trained to eventually change size and form in real-time during inference, targeting to the smaller possible computational footprint. Results are provided for the optimal implementation on a few modern, high-end mobile computing platforms indicating a significant speed-up of up to x3 times.","http://arxiv.org/pdf/1701.05221v5","cs.CV	cs.AI	cs.LG	cs.NE	68T10, 62H30, 68Q32, 68T05, 68Q32, 91E40	I.5; F.1.1; F.4.1; K.3.2; I.4; I.4.8","Parsimonious Inference on Convolutional Neural Networks: Learning and   applying on-line kernel activation rules",""
"Chelsea Finn	Pieter Abbeel	Sergey Levine","9","3","2017","We propose an algorithm for meta-learning that is model-agnostic, in the sense that it is compatible with any model trained with gradient descent and applicable to a variety of different learning problems, including classification, regression, and reinforcement learning. The goal of meta-learning is to train a model on a variety of learning tasks, such that it can solve new learning tasks using only a small number of training samples. In our approach, the parameters of the model are explicitly trained such that a small number of gradient steps with a small amount of training data from a new task will produce good generalization performance on that task. In effect, our method trains the model to be easy to fine-tune. We demonstrate that this approach leads to state-of-the-art performance on two few-shot image classification benchmarks, produces good results on few-shot regression, and accelerates fine-tuning for policy gradient reinforcement learning with neural network policies.","http://arxiv.org/pdf/1703.03400v3","cs.LG	cs.AI	cs.CV	cs.NE","Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks",""
"Asit Mishra	Jeffrey J Cook	Eriko Nurvitadhi	Debbie Marr","10","4","2017","For computer vision applications, prior works have shown the efficacy of reducing the numeric precision of model parameters (network weights) in deep neural networks but also that reducing the precision of activations hurts model accuracy much more than reducing the precision of model parameters. We study schemes to train networks from scratch using reduced-precision activations without hurting the model accuracy. We reduce the precision of activation maps (along with model parameters) using a novel quantization scheme and increase the number of filter maps in a layer, and find that this scheme compensates or surpasses the accuracy of the baseline full-precision network. As a result, one can significantly reduce the dynamic memory footprint, memory bandwidth, computational energy and speed up the training and inference process with appropriate hardware support. We call our scheme WRPN - wide reduced-precision networks. We report results using our proposed schemes and show that our results are better than previously reported accuracies on ILSVRC-12 dataset while being computationally less expensive compared to previously reported reduced-precision networks.","http://arxiv.org/pdf/1704.03079v1","cs.LG	cs.AI	cs.CV	cs.NE","WRPN: Training and Inference using Wide Reduced-Precision Networks",""
"David Rolnick	Andreas Veit	Serge Belongie	Nir Shavit","30","5","2017","Deep neural networks trained on large supervised datasets have led to impressive results in image classification and other tasks. However, well-annotated datasets can be time-consuming and expensive to collect, lending increased interest to larger but noisy datasets that are more easily obtained. In this paper, we show that deep neural networks are capable of generalizing from training data for which true labels are massively outnumbered by incorrect labels. We demonstrate remarkably high test performance after training on corrupted data from MNIST, CIFAR, and ImageNet. For example, on MNIST we obtain test accuracy above 90 percent even after each clean training example has been diluted with 100 randomly-labeled examples. Such behavior holds across multiple patterns of label noise, even when erroneous labels are biased towards confusing classes. We show that training in this regime requires a significant but manageable increase in dataset size that is related to the factor by which correct labels have been diluted. Finally, we provide an analysis of our results that shows how increasing noise decreases the effective batch size.","http://arxiv.org/pdf/1705.10694v3","cs.LG	cs.AI	cs.CV	cs.NE","Deep Learning is Robust to Massive Label Noise",""
"Stefan Lattner	Maarten Grachten","5","7","2017","Content-invariance in mapping codes learned by GAEs is a useful feature for various relation learning tasks. In this paper we show that the content-invariance of mapping codes for images of 2D and 3D rotated objects can be substantially improved by extending the standard GAE loss (symmetric reconstruction error) with a regularization term that penalizes the symmetric cross-reconstruction error. This error term involves reconstruction of pairs with mapping codes obtained from other pairs exhibiting similar transformations. Although this would principally require knowledge of the transformations exhibited by training pairs, our experiments show that a bootstrapping approach can sidestep this issue, and that the regularization term can effectively be used in an unsupervised setting.","http://arxiv.org/pdf/1707.01357v1","cs.CV	cs.AI	cs.LG	cs.NE","Improving Content-Invariance in Gated Autoencoders for 2D and 3D Object   Rotation",""
"Jindong Wang	Yiqiang Chen	Shuji Hao	Xiaohui Peng	Lisha Hu","12","7","2017","Sensor-based activity recognition seeks the profound high-level knowledge about human activities from multitudes of low-level sensor readings. Conventional pattern recognition approaches have made tremendous progress in the past years. However, those methods often heavily rely on heuristic hand-crafted feature extraction, which could hinder their generalization performance. Additionally, existing methods are undermined for unsupervised and incremental learning tasks. Recently, the recent advancement of deep learning makes it possible to perform automatic high-level feature extraction thus achieves promising performance in many areas. Since then, deep learning based methods have been widely adopted for the sensor-based activity recognition tasks. This paper surveys the recent advance of deep learning based sensor-based activity recognition. We summarize existing literature from three aspects: sensor modality, deep model, and application. We also present detailed insights on existing work and propose grand challenges for future research.","http://arxiv.org/pdf/1707.03502v2","cs.CV	cs.AI	cs.LG	cs.NE","Deep Learning for Sensor-based Activity Recognition: A Survey",""
"Chengxi Ye	Yezhou Yang	Cornelia Fermuller	Yiannis Aloimonos","2","8","2017","We explain that the difficulties of training deep neural networks come from a syndrome of three consistency issues. This paper describes our efforts in their analysis and treatment. The first issue is the training speed inconsistency in different layers. We propose to address it with an intuitive, simple-to-implement, low footprint second-order method. The second issue is the scale inconsistency between the layer inputs and the layer residuals. We explain how second-order information provides favorable convenience in removing this roadblock. The third and most challenging issue is the inconsistency in residual propagation. Based on the fundamental theorem of linear algebra, we provide a mathematical characterization of the famous vanishing gradient problem. Thus, an important design principle for future optimization and neural network design is derived. We conclude this paper with the construction of a novel contractive neural network.","http://arxiv.org/pdf/1708.00631v1","cs.LG	cs.AI	cs.CV	cs.NE","On the Importance of Consistency in Training Deep Neural Networks","cxy@umiacs.umd.edu	fer@umiacs.umd.edu	yiannis@umiacs.umd.edu	yz.yang@asu.edu"
"Mario Amrehn	Sven Gaube	Mathias Unberath	Frank Schebesch	Tim Horz	Maddalena Strumia	Stefan Steidl	Markus Kowarschik	Andreas Maier","11","9","2017","For complex segmentation tasks, fully automatic systems are inherently limited in their achievable accuracy for extracting relevant objects. Especially in cases where only few data sets need to be processed for a highly accurate result, semi-automatic segmentation techniques exhibit a clear benefit for the user. One area of application is medical image processing during an intervention for a single patient. We propose a learning-based cooperative segmentation approach which includes the computing entity as well as the user into the task. Our system builds upon a state-of-the-art fully convolutional artificial neural network (FCN) as well as an active user model for training. During the segmentation process, a user of the trained system can iteratively add additional hints in form of pictorial scribbles as seed points into the FCN system to achieve an interactive and precise segmentation result. The segmentation quality of interactive FCNs is evaluated. Iterative FCN approaches can yield superior results compared to networks without the user input channel component, due to a consistent improvement in segmentation quality after each interaction.","http://arxiv.org/pdf/1709.03450v1","cs.CV	cs.AI	cs.LG	cs.NE	68T05, 68T45	I.2.6; I.4.6; I.5.5","UI-Net: Interactive Artificial Neural Networks for Iterative Image   Segmentation Based on a User Model",""
"Altaf H. Khan","15","12","2017","Most of the weights in a Lightweight Neural Network have a value of zero, while the remaining ones are either +1 or -1. These universal approximators require approximately 1.1 bits/weight of storage, posses a quick forward pass and achieve classification accuracies similar to conventional continuous-weight networks. Their training regimen focuses on error reduction initially, but later emphasizes discretization of weights. They ignore insignificant inputs, remove unnecessary weights, and drop unneeded hidden neurons. We have successfully tested them on the MNIST, credit card fraud, and credit card defaults data sets using networks having 2 to 16 hidden layers and up to 4.4 million weights.","http://arxiv.org/pdf/1712.05695v1","cs.LG	cs.AI	cs.CV	cs.NE","Lightweight Neural Networks","altaf@altafkhan.com"
"Nathaniel Thomas	Tess Smidt	Steven Kearnes	Lusann Yang	Li Li	Kai Kohlhoff	Patrick Riley","22","2","2018","We introduce tensor field networks, which are locally equivariant to 3D rotations, translations, and permutations of points at every layer. 3D rotation equivariance removes the need for data augmentation to identify features in arbitrary orientations. Our network uses filters built from spherical harmonics; due to the mathematical consequences of this filter choice, each layer accepts as input (and guarantees as output) scalars, vectors, and higher-order tensors, in the geometric sense of these terms. We demonstrate how tensor field networks learn to model simple physics (Newtonian gravitation and moment of inertia), classify simple 3D shapes (trained on one orientation and tested on shapes in arbitrary orientations), and, given a small organic molecule with an atom removed, replace the correct element at the correct location in space.","http://arxiv.org/pdf/1802.08219v2","cs.LG	cs.AI	cs.CV	cs.NE","Tensor Field Networks: Rotation- and Translation-Equivariant Neural   Networks for 3D Point Clouds",""
"Çağlar Gülçehre	Yoshua Bengio","17","1","2013","We explore the effect of introducing prior information into the intermediate level of neural networks for a learning task on which all the state-of-the-art machine learning algorithms tested failed to learn. We motivate our work from the hypothesis that humans learn such intermediate concepts from other individuals via a form of supervision or guidance using a curriculum. The experiments we have conducted provide positive evidence in favor of this hypothesis. In our experiments, a two-tiered MLP architecture is trained on a dataset with 64x64 binary inputs images, each image with three sprites. The final task is to decide whether all the sprites are the same or one of them is different. Sprites are pentomino tetris shapes and they are placed in an image with different locations using scaling and rotation transformations. The first part of the two-tiered MLP is pre-trained with intermediate-level targets being the presence of sprites at each location, while the second part takes the output of the first part as input and predicts the final task's target binary event. The two-tiered MLP architecture, with a few tens of thousand examples, was able to learn the task perfectly, whereas all other algorithms (include unsupervised pre-training, but also traditional algorithms like SVMs, decision trees and boosting) all perform no better than chance. We hypothesize that the optimization difficulty involved when the intermediate pre-training is not performed is due to the {\em composition} of two highly non-linear tasks. Our findings are also consistent with hypotheses on cultural learning inspired by the observations of optimization problems with deep learning, presumably because of effective local minima.","http://arxiv.org/pdf/1301.4083v6","cs.LG	cs.CV	cs.NE	stat.ML","Knowledge Matters: Importance of Prior Information for Optimization","gulcehrc@iro.umontreal.ca	bengioy@iro.umontreal.ca"
"Kishore Konda	Roland Memisevic	David Krueger","13","2","2014","Regularized training of an autoencoder typically results in hidden unit biases that take on large negative values. We show that negative biases are a natural result of using a hidden layer whose responsibility is to both represent the input data and act as a selection mechanism that ensures sparsity of the representation. We then show that negative biases impede the learning of data distributions whose intrinsic dimensionality is high. We also propose a new activation function that decouples the two roles of the hidden layer and that allows us to learn representations on data with very high intrinsic dimensionality, where standard autoencoders typically fail. Since the decoupled activation function acts like an implicit regularizer, the model can be trained by minimizing the reconstruction error of training data, without requiring any additional regularization.","http://arxiv.org/pdf/1402.3337v5","stat.ML	cs.CV	cs.LG	cs.NE","Zero-bias autoencoders and the benefits of co-adapting features","konda.kishorereddy@gmail.com	roland.memisevic@umontreal.ca	david.krueger@umontreal.ca"
"Bodo Rueckauer	Iulia-Alexandra Lungu	Yuhuang Hu	Michael Pfeiffer","13","12","2016","Deep convolutional neural networks (CNNs) have shown great potential for numerous real-world machine learning applications, but performing inference in large CNNs in real-time remains a challenge. We have previously demonstrated that traditional CNNs can be converted into deep spiking neural networks (SNNs), which exhibit similar accuracy while reducing both latency and computational load as a consequence of their data-driven, event-based style of computing. Here we provide a novel theory that explains why this conversion is successful, and derive from it several new tools to convert a larger and more powerful class of deep networks into SNNs. We identify the main sources of approximation errors in previous conversion methods, and propose simple mechanisms to fix these issues. Furthermore, we develop spiking implementations of common CNN operations such as max-pooling, softmax, and batch-normalization, which allow almost loss-less conversion of arbitrary CNN architectures into the spiking domain. Empirical evaluation of different network architectures on the MNIST and CIFAR10 benchmarks leads to the best SNN results reported to date.","http://arxiv.org/pdf/1612.04052v1","stat.ML	cs.CV	cs.LG	cs.NE","Theory and Tools for the Conversion of Analog to Spiking Convolutional   Neural Networks","rbodo@ini.uzh.ch	iulialexandra@ini.uzh.ch	yuhu@ini.uzh.ch	pfeiffer@ini.uzh.ch"
"Xun Huang	Yixuan Li	Omid Poursaeed	John Hopcroft	Serge Belongie","13","12","2016","In this paper, we propose a novel generative model named Stacked Generative Adversarial Networks (SGAN), which is trained to invert the hierarchical representations of a bottom-up discriminative network. Our model consists of a top-down stack of GANs, each learned to generate lower-level representations conditioned on higher-level representations. A representation discriminator is introduced at each feature hierarchy to encourage the representation manifold of the generator to align with that of the bottom-up discriminative network, leveraging the powerful discriminative representations to guide the generative model. In addition, we introduce a conditional loss that encourages the use of conditional information from the layer above, and a novel entropy loss that maximizes a variational lower bound on the conditional entropy of generator outputs. We first train each stack independently, and then train the whole model end-to-end. Unlike the original GAN that uses a single noise vector to represent all the variations, our SGAN decomposes variations into multiple levels and gradually resolves uncertainties in the top-down generative process. Based on visual inspection, Inception scores and visual Turing test, we demonstrate that SGAN is able to generate images of much higher quality than GANs without stacking.","http://arxiv.org/pdf/1612.04357v4","cs.CV	cs.LG	cs.NE	stat.ML","Stacked Generative Adversarial Networks","xh258@cornell.edu	yl2363@cornell.edu	op63@cornell.edu	sjb344@cornell.edu	jeh@cs.cornell.edu"
"David Warde-Farley	Andrew Rabinovich	Dragomir Anguelov","20","12","2014","We study the problem of large scale, multi-label visual recognition with a large number of possible classes. We propose a method for augmenting a trained neural network classifier with auxiliary capacity in a manner designed to significantly improve upon an already well-performing model, while minimally impacting its computational footprint. Using the predictions of the network itself as a descriptor for assessing visual similarity, we define a partitioning of the label space into groups of visually similar entities. We then augment the network with auxilliary hidden layer pathways with connectivity only to these groups of label units. We report a significant improvement in mean average precision on a large-scale object recognition task with the augmented model, while increasing the number of multiply-adds by less than 3%.","http://arxiv.org/pdf/1412.6563v2","stat.ML	cs.CV	cs.LG	cs.NE","Self-informed neural network structure learning","wardefar@iro.umontreal.ca	amrabino@google.com	dragomir@google.com"
"Forest Agostinelli	Matthew Hoffman	Peter Sadowski	Pierre Baldi","21","12","2014","Artificial neural networks typically have a fixed, non-linear activation function at each neuron. We have designed a novel form of piecewise linear activation function that is learned independently for each neuron using gradient descent. With this adaptive activation function, we are able to improve upon deep neural network architectures composed of static rectified linear units, achieving state-of-the-art performance on CIFAR-10 (7.51%), CIFAR-100 (30.83%), and a benchmark from high-energy physics involving Higgs boson decay modes.","http://arxiv.org/pdf/1412.6830v3","cs.NE	cs.CV	cs.LG	stat.ML","Learning Activation Functions to Improve Deep Neural Networks","fagostin@uci.edu	mathoffm@adobe.com	peter.j.sadowski@uci.edu	pfbaldi@uci.edu"
"Antti Rasmus	Tapani Raiko	Harri Valpola","22","12","2014","Suitable lateral connections between encoder and decoder are shown to allow higher layers of a denoising autoencoder (dAE) to focus on invariant representations. In regular autoencoders, detailed information needs to be carried through the highest layers but lateral connections from encoder to decoder relieve this pressure. It is shown that abstract invariant features can be translated to detailed reconstructions when invariant features are allowed to modulate the strength of the lateral connection. Three dAE structures with modulated and additive lateral connections, and without lateral connections were compared in experiments using real-world images. The experiments verify that adding modulated lateral connections to the model 1) improves the accuracy of the probability model for inputs, as measured by denoising performance; 2) results in representations whose degree of invariance grows faster towards the higher layers; and 3) supports the formation of diverse invariant poolings.","http://arxiv.org/pdf/1412.7210v4","cs.NE	cs.CV	cs.LG	stat.ML","Denoising autoencoder with modulated lateral connections learns   invariant representations of natural images","antti.rasmus@aalto.fi	tapani.raiko@aalto.fi	harri@zenrobotics.com"
"Ankit B. Patel	Tan Nguyen	Richard G. Baraniuk","2","4","2015","A grand challenge in machine learning is the development of computational algorithms that match or outperform humans in perceptual inference tasks that are complicated by nuisance variation. For instance, visual object recognition involves the unknown object position, orientation, and scale in object recognition while speech recognition involves the unknown voice pronunciation, pitch, and speed. Recently, a new breed of deep learning algorithms have emerged for high-nuisance inference tasks that routinely yield pattern recognition systems with near- or super-human capabilities. But a fundamental question remains: Why do they work? Intuitions abound, but a coherent framework for understanding, analyzing, and synthesizing deep learning architectures has remained elusive. We answer this question by developing a new probabilistic framework for deep learning based on the Deep Rendering Model: a generative probabilistic model that explicitly captures latent nuisance variation. By relaxing the generative model to a discriminative one, we can recover two of the current leading deep learning systems, deep convolutional neural networks and random decision forests, providing insights into their successes and shortcomings, as well as a principled route to their improvement.","http://arxiv.org/pdf/1504.00641v1","stat.ML	cs.CV	cs.LG	cs.NE","A Probabilistic Theory of Deep Learning","abp4@rice.edu	mn15@rice.edu	richb@rice.edu"
"Rein Houthooft	Filip De Turck","3","8","2015","Tackling pattern recognition problems in areas such as computer vision, bioinformatics, speech or text recognition is often done best by taking into account task-specific statistical relations between output variables. In structured prediction, this internal structure is used to predict multiple outputs simultaneously, leading to more accurate and coherent predictions. Structural support vector machines (SSVMs) are nonprobabilistic models that optimize a joint input-output function through margin-based learning. Because SSVMs generally disregard the interplay between unary and interaction factors during the training phase, final parameters are suboptimal. Moreover, its factors are often restricted to linear combinations of input features, limiting its generalization power. To improve prediction accuracy, this paper proposes: (i) Joint inference and learning by integration of back-propagation and loss-augmented inference in SSVM subgradient descent; (ii) Extending SSVM factors to neural networks that form highly nonlinear functions of input features. Image segmentation benchmark results demonstrate improvements over conventional SSVM training methods in terms of accuracy, highlighting the feasibility of end-to-end SSVM training with neural factors.","http://arxiv.org/pdf/1508.00451v4","stat.ML	cs.CV	cs.LG	cs.NE","Integrated Inference and Learning of Neural Factors in Structural   Support Vector Machines",""
"Patrick W. Gallagher	Shuai Tang	Zhuowen Tu","23","11","2015","Top-down information plays a central role in human perception, but plays relatively little role in many current state-of-the-art deep networks, such as Convolutional Neural Networks (CNNs). This work seeks to explore a path by which top-down information can have a direct impact within current deep networks. We explore this path by learning and using ""generators"" corresponding to the network internal effects of three types of transformation (each a restriction of a general affine transformation): rotation, scaling, and translation. We demonstrate how these learned generators can be used to transfer top-down information to novel settings, as mediated by the ""feature flows"" that the transformations (and the associated generators) correspond to inside the network. Specifically, we explore three aspects: 1) using generators as part of a method for synthesizing transformed images --- given a previously unseen image, produce versions of that image corresponding to one or more specified transformations, 2) ""zero-shot learning"" --- when provided with a feature flow corresponding to the effect of a transformation of unknown amount, leverage learned generators as part of a method by which to perform an accurate categorization of the amount of transformation, even for amounts never observed during training, and 3) (inside-CNN) ""data augmentation"" --- improve the classification performance of an existing network by using the learned generators to directly provide additional training ""inside the CNN"".","http://arxiv.org/pdf/1511.07125v1","cs.NE	cs.CV	cs.LG	stat.ML","What Happened to My Dog in That Network: Unraveling Top-down Generators   in Convolutional Neural Networks","pwgallag@ucsd.edu	shuaitang93@ucsd.edu	ztu@ucsd.edu"
"Adrien Gaidon	Qiao Wang	Yohann Cabon	Eleonora Vig","20","5","2016","Modern computer vision algorithms typically require expensive data acquisition and accurate manual labeling. In this work, we instead leverage the recent progress in computer graphics to generate fully labeled, dynamic, and photo-realistic proxy virtual worlds. We propose an efficient real-to-virtual world cloning method, and validate our approach by building and publicly releasing a new video dataset, called Virtual KITTI (see http://www.xrce.xerox.com/Research-Development/Computer-Vision/Proxy-Virtual-Worlds), automatically labeled with accurate ground truth for object detection, tracking, scene and instance segmentation, depth, and optical flow. We provide quantitative experimental evidence suggesting that (i) modern deep learning algorithms pre-trained on real data behave similarly in real and virtual worlds, and (ii) pre-training on virtual data improves performance. As the gap between real and virtual worlds is small, virtual worlds enable measuring the impact of various weather and imaging conditions on recognition performance, all other things being equal. We show these factors may affect drastically otherwise high-performing deep models for tracking.","http://arxiv.org/pdf/1605.06457v1","cs.CV	cs.LG	cs.NE	stat.ML","Virtual Worlds as Proxy for Multi-Object Tracking Analysis","adrien.gaidon@xrce.xerox.com	yohann.cabon@xrce.xerox.com	qiao.wang@asu.edu	eleonora.vig@dlr.de"
"Jianwen Xie	Song-Chun Zhu	Ying Nian Wu","3","6","2016","Video sequences contain rich dynamic patterns, such as dynamic texture patterns that exhibit stationarity in the temporal domain, and action patterns that are non-stationary in either spatial or temporal domain. We show that a spatial-temporal generative ConvNet can be used to model and synthesize dynamic patterns. The model defines a probability distribution on the video sequence, and the log probability is defined by a spatial-temporal ConvNet that consists of multiple layers of spatial-temporal filters to capture spatial-temporal patterns of different scales. The model can be learned from the training video sequences by an ""analysis by synthesis"" learning algorithm that iterates the following two steps. Step 1 synthesizes video sequences from the currently learned model. Step 2 then updates the model parameters based on the difference between the synthesized video sequences and the observed training sequences. We show that the learning algorithm can synthesize realistic dynamic patterns.","http://arxiv.org/pdf/1606.00972v2","stat.ML	cs.CV	cs.LG	cs.NE","Synthesizing Dynamic Patterns by Spatial-Temporal Generative ConvNet","jianwen@ucla.edu,	sczhu@stat.ucla.edu,	ywu@stat.ucla.edu"
"Mohammad Javad Shafiee	Akshaya Mishra	Alexander Wong","14","6","2016","Taking inspiration from biological evolution, we explore the idea of ""Can deep neural networks evolve naturally over successive generations into highly efficient deep neural networks?"" by introducing the notion of synthesizing new highly efficient, yet powerful deep neural networks over successive generations via an evolutionary process from ancestor deep neural networks. The architectural traits of ancestor deep neural networks are encoded using synaptic probability models, which can be viewed as the `DNA' of these networks. New descendant networks with differing network architectures are synthesized based on these synaptic probability models from the ancestor networks and computational environmental factor models, in a random manner to mimic heredity, natural selection, and random mutation. These offspring networks are then trained into fully functional networks, like one would train a newborn, and have more efficient, more diverse network architectures than their ancestor networks, while achieving powerful modeling capabilities. Experimental results for the task of visual saliency demonstrated that the synthesized `evolved' offspring networks can achieve state-of-the-art performance while having network architectures that are significantly more efficient (with a staggering $\sim$48-fold decrease in synapses by the fourth generation) compared to the original ancestor network.","http://arxiv.org/pdf/1606.04393v3","cs.CV	cs.LG	cs.NE	stat.ML","Deep Learning with Darwin: Evolutionary Synthesis of Deep Neural   Networks",""
"Tian Han	Yang Lu	Song-Chun Zhu	Ying Nian Wu","28","6","2016","This paper proposes an alternating back-propagation algorithm for learning the generator network model. The model is a non-linear generalization of factor analysis. In this model, the mapping from the continuous latent factors to the observed signal is parametrized by a convolutional neural network. The alternating back-propagation algorithm iterates the following two steps: (1) Inferential back-propagation, which infers the latent factors by Langevin dynamics or gradient descent. (2) Learning back-propagation, which updates the parameters given the inferred latent factors by gradient descent. The gradient computations in both steps are powered by back-propagation, and they share most of their code in common. We show that the alternating back-propagation algorithm can learn realistic generator models of natural images, video sequences, and sounds. Moreover, it can also be used to learn from incomplete or indirect training data.","http://arxiv.org/pdf/1606.08571v4","stat.ML	cs.CV	cs.LG	cs.NE","Alternating Back-Propagation for Generator Network",""
"Ilija Ilievski	Jiashi Feng","31","7","2016","Recently, several optimization methods have been successfully applied to the hyperparameter optimization of deep neural networks (DNNs). The methods work by modeling the joint distribution of hyperparameter values and corresponding error. Those methods become less practical when applied to modern DNNs whose training may take a few days and thus one cannot collect sufficient observations to accurately model the distribution. To address this challenging issue, we propose a method that learns to transfer optimal hyperparameter values for a small source dataset to hyperparameter values with comparable performance on a dataset of interest. As opposed to existing transfer learning methods, our proposed method does not use hand-designed features. Instead, it uses surrogates to model the hyperparameter-error distributions of the two datasets and trains a neural network to learn the transfer function. Extensive experiments on three CV benchmark datasets clearly demonstrate the efficiency of our method.","http://arxiv.org/pdf/1608.00218v1","cs.LG	cs.CV	cs.NE	stat.ML","Hyperparameter Transfer Learning through Surrogate Alignment for   Efficient Deep Neural Network Training","ilija.ilievski@u.nus.edu,	elefjia@nus.edu.sg"
"Hao Wang	Dit-Yan Yeung","24","8","2016","While perception tasks such as visual object recognition and text understanding play an important role in human intelligence, the subsequent tasks that involve inference, reasoning and planning require an even higher level of intelligence. The past few years have seen major advances in many perception tasks using deep learning models. For higher-level inference, however, probabilistic graphical models with their Bayesian nature are still more powerful and flexible. To achieve integrated intelligence that involves both perception and inference, it is naturally desirable to tightly integrate deep learning and Bayesian models within a principled probabilistic framework, which we call Bayesian deep learning. In this unified framework, the perception of text or images using deep learning can boost the performance of higher-level inference and in return, the feedback from the inference process is able to enhance the perception of text or images. This paper proposes a general framework for Bayesian deep learning and reviews its recent applications on recommender systems, topic models, and control. In this paper, we also discuss the relationship and differences between Bayesian deep learning and other related topics like Bayesian treatment of neural networks.","http://arxiv.org/pdf/1608.06884v2","stat.ML	cs.CV	cs.LG	cs.NE","Towards Bayesian Deep Learning: A Framework and Some Existing Methods",""
"Mason McGill	Pietro Perona","17","3","2017","We propose and systematically evaluate three strategies for training dynamically-routed artificial neural networks: graphs of learned transformations through which different input signals may take different paths. Though some approaches have advantages over others, the resulting networks are often qualitatively similar. We find that, in dynamically-routed networks trained to classify images, layers and branches become specialized to process distinct categories of images. Additionally, given a fixed computational budget, dynamically-routed networks tend to perform better than comparable statically-routed networks.","http://arxiv.org/pdf/1703.06217v2","stat.ML	cs.CV	cs.LG	cs.NE","Deciding How to Decide: Dynamic Routing in Artificial Neural Networks",""
"Hongyang Gao	Hao Yuan	Zhengyang Wang	Shuiwang Ji","18","5","2017","Deconvolutional layers have been widely used in a variety of deep models for up-sampling, including encoder-decoder networks for semantic segmentation and deep generative models for unsupervised learning. One of the key limitations of deconvolutional operations is that they result in the so-called checkerboard problem. This is caused by the fact that no direct relationship exists among adjacent pixels on the output feature map. To address this problem, we propose the pixel deconvolutional layer (PixelDCL) to establish direct relationships among adjacent pixels on the up-sampled feature map. Our method is based on a fresh interpretation of the regular deconvolution operation. The resulting PixelDCL can be used to replace any deconvolutional layer in a plug-and-play manner without compromising the fully trainable capabilities of original models. The proposed PixelDCL may result in slight decrease in efficiency, but this can be overcome by an implementation trick. Experimental results on semantic segmentation demonstrate that PixelDCL can consider spatial features such as edges and shapes and yields more accurate segmentation outputs than deconvolutional layers. When used in image generation tasks, our PixelDCL can largely overcome the checkerboard problem suffered by regular deconvolution operations.","http://arxiv.org/pdf/1705.06820v4","cs.LG	cs.CV	cs.NE	stat.ML","Pixel Deconvolutional Networks","hongyang.gao@wsu.edu	hao.yuan@wsu.edu	zwang6@eecs.wsu.edu	sji@eecs.wsu.edu"
"Stanislav Fort","9","8","2017","We propose a novel architecture for $k$-shot classification on the Omniglot dataset. Building on prototypical networks, we extend their architecture to what we call Gaussian prototypical networks. Prototypical networks learn a map between images and embedding vectors, and use their clustering for classification. In our model, a part of the encoder output is interpreted as a confidence region estimate about the embedding point, and expressed as a Gaussian covariance matrix. Our network then constructs a direction and class dependent distance metric on the embedding space, using uncertainties of individual data points as weights. We show that Gaussian prototypical networks are a preferred architecture over vanilla prototypical networks with an equivalent number of parameters. We report state-of-the-art performance in 1-shot and 5-shot classification both in 5-way and 20-way regime (for 5-shot 5-way, we are comparable to previous state-of-the-art) on the Omniglot dataset. We explore artificially down-sampling a fraction of images in the training set, which improves our performance even further. We therefore hypothesize that Gaussian prototypical networks might perform better in less homogeneous, noisier datasets, which are commonplace in real world applications.","http://arxiv.org/pdf/1708.02735v1","cs.LG	cs.CV	cs.NE	stat.ML","Gaussian Prototypical Networks for Few-Shot Learning on Omniglot",""
"Leslie N. Smith	Nicholay Topin","23","8","2017","In this paper, we show a phenomenon, which we named ""super-convergence"", where residual networks can be trained using an order of magnitude fewer iterations than is used with standard training methods. The existence of super-convergence is relevant to understanding why deep networks generalize well. One of the key elements of super-convergence is training with cyclical learning rates and a large maximum learning rate. Furthermore, we present evidence that training with large learning rates improves performance by regularizing the network. In addition, we show that super-convergence provides a greater boost in performance relative to standard training when the amount of labeled training data is limited. We also derive a simplification of the Hessian Free optimization method to compute an estimate of the optimal learning rate. The architectures and code to replicate the figures in this paper are available at github.com/lnsmith54/super-convergence.","http://arxiv.org/pdf/1708.07120v2","cs.LG	cs.CV	cs.NE	stat.ML","Super-Convergence: Very Fast Training of Residual Networks Using Large   Learning Rates","leslie.smith@nrl.navy.mil	ntopin1@umbc.edu"
"Boris Flach	Alexander Shekhovtsov	Ondrej Fikar","25","9","2017","Learning, taking into account full distribution of the data, referred to as generative, is not feasible with deep neural networks (DNNs) because they model only the conditional distribution of the outputs given the inputs. Current solutions are either based on joint probability models facing difficult estimation problems or learn two separate networks, mapping inputs to outputs (recognition) and vice-versa (generation). We propose an intermediate approach. First, we show that forward computation in DNNs with logistic sigmoid activations corresponds to a simplified approximate Bayesian inference in a directed probabilistic multi-layer model. This connection allows to interpret DNN as a probabilistic model of the output and all hidden units given the input. Second, we propose that in order for the recognition and generation networks to be more consistent with the joint model of the data, weights of the recognition and generator network should be related by transposition. We demonstrate in a tentative experiment that such a coupled pair can be learned generatively, modelling the full distribution of the data, and has enough capacity to perform well in both recognition and generation.","http://arxiv.org/pdf/1709.08524v1","cs.LG	cs.CV	cs.NE	stat.ML","Generative learning for deep networks","flachbor@cmp.felk.cvut.cz	shekhovtsov@gmail.com	fikarond@fel.cvut.cz"
"Hanxiao Liu	Karen Simonyan	Oriol Vinyals	Chrisantha Fernando	Koray Kavukcuoglu","1","11","2017","We explore efficient neural architecture search methods and show that a simple yet powerful evolutionary algorithm can discover new architectures with excellent performance. Our approach combines a novel hierarchical genetic representation scheme that imitates the modularized design pattern commonly adopted by human experts, and an expressive search space that supports complex topologies. Our algorithm efficiently discovers architectures that outperform a large number of manually designed models for image classification, obtaining top-1 error of 3.6% on CIFAR-10 and 20.3% when transferred to ImageNet, which is competitive with the best existing neural architecture search approaches. We also present results using random search, achieving 0.3% less top-1 accuracy on CIFAR-10 and 0.1% less on ImageNet whilst reducing the search time from 36 hours down to 1 hour.","http://arxiv.org/pdf/1711.00436v2","cs.LG	cs.CV	cs.NE	stat.ML","Hierarchical Representations for Efficient Architecture Search","hanxiaol@cs.cmu.edu	simonyan@google.com	vinyals@google.com	chrisantha@google.com	korayk@google.com"
"Antreas Antoniou	Amos Storkey	Harrison Edwards","12","11","2017","Effective training of neural networks requires much data. In the low-data regime, parameters are underdetermined, and learnt networks generalise poorly. Data Augmentation alleviates this by using existing data more effectively. However standard data augmentation produces only limited plausible alternative data. Given there is potential to generate a much broader set of augmentations, we design and train a generative model to do data augmentation. The model, based on image conditional Generative Adversarial Networks, takes data from a source domain and learns to take any data item and generalise it to generate other within-class data items. As this generative process does not depend on the classes themselves, it can be applied to novel unseen classes of data. We show that a Data Augmentation Generative Adversarial Network (DAGAN) augments standard vanilla classifiers well. We also show a DAGAN can enhance few-shot learning systems such as Matching Networks. We demonstrate these approaches on Omniglot, on EMNIST having learnt the DAGAN on Omniglot, and VGG-Face data. In our experiments we can see over 13% increase in accuracy in the low-data regime experiments in Omniglot (from 69% to 82%), EMNIST (73.9% to 76%) and VGG-Face (4.5% to 12%); in Matching Networks for Omniglot we observe an increase of 0.5% (from 96.9% to 97.4%) and an increase of 1.8% in EMNIST (from 59.5% to 61.3%).","http://arxiv.org/pdf/1711.04340v3","stat.ML	cs.CV	cs.LG	cs.NE","Data Augmentation Generative Adversarial Networks","a.antoniou@sms.ed.ac.uk	a.storkey@ed.ac.uk	h.l.edwards@sms.ed.ac.uk"
"Dror Sholomon	Eli David	Nathan S. Netanyahu","23","11","2017","This paper introduces the first deep neural network-based estimation metric for the jigsaw puzzle problem. Given two puzzle piece edges, the neural network predicts whether or not they should be adjacent in the correct assembly of the puzzle, using nothing but the pixels of each piece. The proposed metric exhibits an extremely high precision even though no manual feature extraction is performed. When incorporated into an existing puzzle solver, the solution's accuracy increases significantly, achieving thereby a new state-of-the-art standard.","http://arxiv.org/pdf/1711.08762v1","cs.CV	cs.LG	cs.NE	stat.ML","DNN-Buddies: A Deep Neural Network-Based Estimation Metric for the   Jigsaw Puzzle Problem","dror.sholomon@gmail.com,	mail@elidavid.com,	nathan@cs.biu.ac.il	nathan@cfar.umd.edu"
"Eli David	Nathan S. Netanyahu","23","11","2017","In this paper we describe the problem of painter classification, and propose a novel approach based on deep convolutional autoencoder neural networks. While previous approaches relied on image processing and manual feature extraction from paintings, our approach operates on the raw pixel level, without any preprocessing or manual feature extraction. We first train a deep convolutional autoencoder on a dataset of paintings, and subsequently use it to initialize a supervised convolutional neural network for the classification phase.   The proposed approach substantially outperforms previous methods, improving the previous state-of-the-art for the 3-painter classification problem from 90.44% accuracy (previous state-of-the-art) to 96.52% accuracy, i.e., a 63% reduction in error rate.","http://arxiv.org/pdf/1711.08763v1","cs.CV	cs.LG	cs.NE	stat.ML","DeepPainter: Painter Classification Using Deep Convolutional   Autoencoders","mail@elidavid.com,	nathan@cs.biu.ac.il	nathan@cfar.umd.edu"
"Ido Cohen	Eli David	Nathan S. Netanyahu	Noa Liscovitch	Gal Chechik","27","11","2017","This paper presents a novel deep learning-based method for learning a functional representation of mammalian neural images. The method uses a deep convolutional denoising autoencoder (CDAE) for generating an invariant, compact representation of in situ hybridization (ISH) images. While most existing methods for bio-imaging analysis were not developed to handle images with highly complex anatomical structures, the results presented in this paper show that functional representation extracted by CDAE can help learn features of functional gene ontology categories for their classification in a highly accurate manner. Using this CDAE representation, our method outperforms the previous state-of-the-art classification rate, by improving the average AUC from 0.92 to 0.98, i.e., achieving 75% reduction in error. The method operates on input images that were downsampled significantly with respect to the original ones to make it computationally feasible.","http://arxiv.org/pdf/1711.09663v1","cs.CV	cs.LG	cs.NE	stat.ML","DeepBrain: Functional Representation of Neural In-Situ Hybridization   Images for Gene Ontology Classification Using Deep Convolutional Autoencoders","cido15@gmail.com,	mail@elidavid.com,	nathan@cs.biu.ac.il	nathan@cfar.umd.edu	noalis@gmail.com,	gal.chechik@mail.biu.ac.il"
"Omid Poursaeed	Isay Katsman	Bicheng Gao	Serge Belongie","6","12","2017","In this paper, we propose novel generative models for creating adversarial examples, slightly perturbed images resembling natural images but maliciously crafted to fool pre-trained models. We present trainable deep neural networks for transforming images to adversarial perturbations. Our proposed models can produce image-agnostic and image-dependent perturbations for both targeted and non-targeted attacks. We also demonstrate that similar architectures can achieve impressive results in fooling classification and semantic segmentation models, obviating the need for hand-crafting attack methods for each task. Using extensive experiments on challenging high-resolution datasets such as ImageNet and Cityscapes, we show that our perturbations achieve high fooling rates with small perturbation norms. Moreover, our attacks are considerably faster than current iterative methods at inference time.","http://arxiv.org/pdf/1712.02328v1","cs.CV	cs.LG	cs.NE	stat.ML","Generative Adversarial Perturbations","op63@cornell.edu	isk22@cornell.edu	bg455@cornell.edu	sjb344@cornell.edu"
"Logan Engstrom	Brandon Tran	Dimitris Tsipras	Ludwig Schmidt	Aleksander Madry","7","12","2017","We show that simple transformations, namely translations and rotations alone, are sufficient to fool neural network-based vision models on a significant fraction of inputs. This is in sharp contrast to previous work that relied on more complicated optimization approaches that are unlikely to appear outside of a truly adversarial setting. Moreover, fooling rotations and translations are easy to find and require only a few black-box queries to the target model. Overall, our findings emphasize the need for designing robust classifiers even in natural, benign contexts.","http://arxiv.org/pdf/1712.02779v3","cs.LG	cs.CV	cs.NE	stat.ML","A Rotation and a Translation Suffice: Fooling CNNs with Simple   Transformations",""
"Boyang Deng	Junjie Yan	Dahua Lin","9","12","2017","The quest for performant networks has been a significant force that drives the advancements of deep learning in recent years. While rewarding, improving network design has never been an easy journey. The large design space combined with the tremendous cost required for network training poses a major obstacle to this endeavor. In this work, we propose a new approach to this problem, namely, predicting the performance of a network before training, based on its architecture. Specifically, we develop a unified way to encode individual layers into vectors and bring them together to form an integrated description via LSTM. Taking advantage of the recurrent network's strong expressive power, this method can reliably predict the performances of various network architectures. Our empirical studies showed that it not only achieved accurate predictions but also produced consistent rankings across datasets -- a key desideratum in performance prediction.","http://arxiv.org/pdf/1712.03351v1","cs.LG	cs.CV	cs.NE	stat.ML","Peephole: Predicting Network Performance Before Training","billydeng@buaa.edu.cn	yanjunjie@sensetime.com	dhlin@ie.cuhk.edu.hk"
"Abien Fred Agarap","10","12","2017","Convolutional neural networks (CNNs) are similar to ""ordinary"" neural networks in the sense that they are made up of hidden layers consisting of neurons with ""learnable"" parameters. These neurons receive inputs, performs a dot product, and then follows it with a non-linearity. The whole network expresses the mapping between raw image pixels and their class scores. Conventionally, the Softmax function is the classifier used at the last layer of this network. However, there have been studies (Alalshekmubarak and Smith, 2013; Agarap, 2017; Tang, 2013) conducted to challenge this norm. The cited studies introduce the usage of linear support vector machine (SVM) in an artificial neural network architecture. This project is yet another take on the subject, and is inspired by (Tang, 2013). Empirical data has shown that the CNN-SVM model was able to achieve a test accuracy of ~99.04% using the MNIST dataset (LeCun, Cortes, and Burges, 2010). On the other hand, the CNN-Softmax was able to achieve a test accuracy of ~99.23% using the same dataset. Both models were also tested on the recently-published Fashion-MNIST dataset (Xiao, Rasul, and Vollgraf, 2017), which is suppose to be a more difficult image classification dataset than MNIST (Zalandoresearch, 2017). This proved to be the case as CNN-SVM reached a test accuracy of ~90.72%, while the CNN-Softmax reached a test accuracy of ~91.86%. The said results may be improved if data preprocessing techniques were employed on the datasets, and if the base CNN model was a relatively more sophisticated than the one used in this study.","http://arxiv.org/pdf/1712.03541v1","cs.CV	cs.LG	cs.NE	stat.ML","An Architecture Combining Convolutional Neural Network (CNN) and Support   Vector Machine (SVM) for Image Classification","abien.fred.agarap@adamson.edu.ph"
"Ekaba Bisong","22","12","2017","Artifical Neural Networks are a particular class of learning systems modeled after biological neural functions with an interesting penchant for Hebbian learning, that is ""neurons that wire together, fire together"". However, unlike their natural counterparts, artificial neural networks have a close and stringent coupling between the modules of neurons in the network. This coupling or locking imposes upon the network a strict and inflexible structure that prevent layers in the network from updating their weights until a full feed-forward and backward pass has occurred. Such a constraint though may have sufficed for a while, is now no longer feasible in the era of very-large-scale machine learning, coupled with the increased desire for parallelization of the learning process across multiple computing infrastructures. To solve this problem, synthetic gradients (SG) with decoupled neural interfaces (DNI) are introduced as a viable alternative to the backpropagation algorithm. This paper performs a speed benchmark to compare the speed and accuracy capabilities of SG-DNI as opposed to a standard neural interface using multilayer perceptron MLP. SG-DNI shows good promise, in that it not only captures the learning problem, it is also over 3-fold faster due to it asynchronous learning capabilities.","http://arxiv.org/pdf/1712.08314v2","cs.LG	cs.CV	cs.NE	stat.ML","Benchmarking Decoupled Neural Interfaces with Synthetic Gradients","ekaba.bisong@carleton.ca"
"Amin Fehri	Santiago Velasco-Forero	Fernand Meyer","20","2","2018","Image segmentation is the process of partitioning an image into a set of meaningful regions according to some criteria. Hierarchical segmentation has emerged as a major trend in this regard as it favors the emergence of important regions at different scales. On the other hand, many methods allow us to have prior information on the position of structures of interest in the images. In this paper, we present a versatile hierarchical segmentation method that takes into account any prior spatial information and outputs a hierarchical segmentation that emphasizes the contours or regions of interest while preserving the important structures in the image. An application of this method to the weakly-supervised segmentation problem is presented.","http://arxiv.org/pdf/1802.07008v1","stat.ML	cs.CV	cs.LG	cs.NE","Segmentation hiérarchique faiblement supervisée","amin.fehri@mines-paristech.fr}	santiago.velasco@mines-paristech.fr}	fernand.meyer@mines-paristech.fr@mines-paristech.fr}"
"Mark D. McDonnell","23","2","2018","For fast and energy-efficient deployment of trained deep neural networks on resource-constrained embedded hardware, each learned weight parameter should ideally be represented and stored using a single bit. Error-rates usually increase when this requirement is imposed. Here, we report large improvements in error rates on multiple datasets, for deep convolutional neural networks deployed with 1-bit-per-weight. Using wide residual networks as our main baseline, our approach simplifies existing methods that binarize weights by applying the sign function in training; we apply scaling factors for each layer with constant unlearned values equal to the layer-specific standard deviations used for initialization. For CIFAR-10, CIFAR-100 and ImageNet, and models with 1-bit-per-weight requiring less than 10 MB of parameter memory, we achieve error rates of 3.9%, 18.5% and 26.0% / 8.5% (Top-1 / Top-5) respectively. We also considered MNIST, SVHN and ImageNet32, achieving 1-bit-per-weight test results of 0.27%, 1.9%, and 41.3% / 19.1% respectively. For CIFAR, our error rates halve previously reported values, and are within about 1% of our error-rates for the same network with full-precision weights. For networks that overfit, we also show significant improvements in error rate by not learning batch normalization scale and offset parameters. This applies to both full precision and 1-bit-per-weight networks. Using a warm-restart learning-rate schedule, we found that training for 1-bit-per-weight is just as fast as full-precision networks, with better accuracy than standard schedules, and achieved about 98%-99% of peak performance in just 62 training epochs for CIFAR-10/100. For full training code and trained models in MATLAB, Keras and PyTorch see https://github.com/McDonnell-Lab/1-bit-per-weight/ .","http://arxiv.org/pdf/1802.08530v1","cs.LG	cs.CV	cs.NE	stat.ML","Training wide residual networks for deployment using a single bit for   each weight","mark.mcdonnell@unisa.edu.au"
"Abien Fred Agarap","22","3","2018","We introduce the use of rectified linear units (ReLU) as the classification function in a deep neural network (DNN). Conventionally, ReLU is used as an activation function in DNNs, with Softmax function as their classification function. However, there have been several studies on using a classification function other than Softmax, and this study is an addition to those. We accomplish this by taking the activation of the penultimate layer $h_{n - 1}$ in a neural network, then multiply it by weight parameters $\theta$ to get the raw scores $o_{i}$. Afterwards, we threshold the raw scores $o_{i}$ by $0$, i.e. $f(o) = \max(0, o_{i})$, where $f(o)$ is the ReLU function. We provide class predictions $\hat{y}$ through argmax function, i.e. argmax $f(x)$.","http://arxiv.org/pdf/1803.08375v1","cs.NE	cs.CV	cs.LG	stat.ML","Deep Learning using Rectified Linear Units (ReLU)","abien.fred.agarap@adamson.edu.ph"
"Djork-Arné Clevert	Andreas Mayr	Thomas Unterthiner	Sepp Hochreiter","23","2","2015","We propose rectified factor networks (RFNs) to efficiently construct very sparse, non-linear, high-dimensional representations of the input. RFN models identify rare and small events in the input, have a low interference between code units, have a small reconstruction error, and explain the data covariance structure. RFN learning is a generalized alternating minimization algorithm derived from the posterior regularization method which enforces non-negative and normalized posterior means. We proof convergence and correctness of the RFN learning algorithm. On benchmarks, RFNs are compared to other unsupervised methods like autoencoders, RBMs, factor analysis, ICA, and PCA. In contrast to previous sparse coding methods, RFNs yield sparser codes, capture the data's covariance structure more precisely, and have a significantly smaller reconstruction error. We test RFNs as pretraining technique for deep networks on different vision datasets, where RFNs were superior to RBMs and autoencoders. On gene expression data from two pharmaceutical drug discovery studies, RFNs detected small and rare gene modules that revealed highly relevant new biological insights which were so far missed by other unsupervised methods.","http://arxiv.org/pdf/1502.06464v2","cs.LG	cs.CV	cs.NE	stat.ML","Rectified Factor Networks","okko@bioinf.jku.at	mayr@bioinf.jku.at	unterthiner@bioinf.jku.at	hochreit@bioinf.jku.at"
"Qi Wang	Joseph JaJa","18","11","2013","Motivated by an important insight from neural science, we propose a new framework for understanding the success of the recently proposed ""maxout"" networks. The framework is based on encoding information on sparse pathways and recognizing the correct pathway at inference time. Elaborating further on this insight, we propose a novel deep network architecture, called ""channel-out"" network, which takes a much better advantage of sparse pathway encoding. In channel-out networks, pathways are not only formed a posteriori, but they are also actively selected according to the inference outputs from the lower layers. From a mathematical perspective, channel-out networks can represent a wider class of piece-wise continuous functions, thereby endowing the network with more expressive power than that of maxout networks. We test our channel-out networks on several well-known image classification benchmarks, setting new state-of-the-art performance on CIFAR-100 and STL-10, which represent some of the ""harder"" image classification benchmarks.","http://arxiv.org/pdf/1312.1909v1","cs.NE	cs.CV	cs.LG	stat.ML","From Maxout to Channel-Out: Encoding Information on Sparse Pathways","qwang37@umiacs.umd.edu	joseph@umiacs.umd.edu"
"Takashi Shinozaki	Yasushi Naruse","20","12","2013","We propose a novel learning method for multilayered neural networks which uses feedforward supervisory signal and associates classification of a new input with that of pre-trained input. The proposed method effectively uses rich input information in the earlier layer for robust leaning and revising internal representation in a multilayer neural network.","http://arxiv.org/pdf/1312.5845v7","cs.NE	cs.CV	cs.LG	stat.ML","Competitive Learning with Feedforward Supervisory Signal for Pre-trained   Multilayered Networks",""
"Chen-Yu Lee	Saining Xie	Patrick Gallagher	Zhengyou Zhang	Zhuowen Tu","18","9","2014","Our proposed deeply-supervised nets (DSN) method simultaneously minimizes classification error while making the learning process of hidden layers direct and transparent. We make an attempt to boost the classification performance by studying a new formulation in deep networks. Three aspects in convolutional neural networks (CNN) style architectures are being looked at: (1) transparency of the intermediate layers to the overall classification; (2) discriminativeness and robustness of learned features, especially in the early layers; (3) effectiveness in training due to the presence of the exploding and vanishing gradients. We introduce ""companion objective"" to the individual hidden layers, in addition to the overall objective at the output layer (a different strategy to layer-wise pre-training). We extend techniques from stochastic gradient methods to analyze our algorithm. The advantage of our method is evident and our experimental result on benchmark datasets shows significant performance gain over existing methods (e.g. all state-of-the-art results on MNIST, CIFAR-10, CIFAR-100, and SVHN).","http://arxiv.org/pdf/1409.5185v2","stat.ML	cs.CV	cs.LG	cs.NE","Deeply-Supervised Nets","chl260@ucsd.edu	s9xie@ucsd.edu	rexaran@gmail.com	zhang@microsoft.com	ztu@ucsd.edu"
"Behnam Neyshabur	Ruslan Salakhutdinov	Nathan Srebro","8","6","2015","We revisit the choice of SGD for training deep neural networks by reconsidering the appropriate geometry in which to optimize the weights. We argue for a geometry invariant to rescaling of weights that does not affect the output of the network, and suggest Path-SGD, which is an approximate steepest descent method with respect to a path-wise regularizer related to max-norm regularization. Path-SGD is easy and efficient to implement and leads to empirical gains over SGD and AdaGrad.","http://arxiv.org/pdf/1506.02617v1","cs.LG	cs.CV	cs.NE	stat.ML","Path-SGD: Path-Normalized Optimization in Deep Neural Networks",""
"Alan Mosca	George D. Magoulas","15","9","2015","The Resilient Propagation (Rprop) algorithm has been very popular for backpropagation training of multilayer feed-forward neural networks in various applications. The standard Rprop however encounters difficulties in the context of deep neural networks as typically happens with gradient-based learning algorithms. In this paper, we propose a modification of the Rprop that combines standard Rprop steps with a special drop out technique. We apply the method for training Deep Neural Networks as standalone components and in ensemble formulations. Results on the MNIST dataset show that the proposed modification alleviates standard Rprop's problems demonstrating improved learning speed and accuracy.","http://arxiv.org/pdf/1509.04612v2","cs.NE	cs.CV	cs.LG	stat.ML","Adapting Resilient Propagation for Deep Learning","a.mosca@dcs.bbk.ac.uk	gmagoulas@dcs.bbk.ac.uk"
"Nastaran Mohammadian Rad	Andrea Bizzego	Seyed Mostafa Kia	Giuseppe Jurman	Paola Venuti	Cesare Furlanello","5","11","2015","Autism Spectrum Disorders (ASDs) are often associated with specific atypical postural or motor behaviors, of which Stereotypical Motor Movements (SMMs) have a specific visibility. While the identification and the quantification of SMM patterns remain complex, its automation would provide support to accurate tuning of the intervention in the therapy of autism. Therefore, it is essential to develop automatic SMM detection systems in a real world setting, taking care of strong inter-subject and intra-subject variability. Wireless accelerometer sensing technology can provide a valid infrastructure for real-time SMM detection, however such variability remains a problem also for machine learning methods, in particular whenever handcrafted features extracted from accelerometer signal are considered. Here, we propose to employ the deep learning paradigm in order to learn discriminating features from multi-sensor accelerometer signals. Our results provide preliminary evidence that feature learning and transfer learning embedded in the deep architecture achieve higher accurate SMM detectors in longitudinal scenarios.","http://arxiv.org/pdf/1511.01865v3","cs.NE	cs.CV	cs.LG	stat.ML","Convolutional Neural Network for Stereotypical Motor Movement Detection   in Autism",""
"Sasha Targ	Diogo Almeida	Kevin Lyman","25","3","2016","Residual networks (ResNets) have recently achieved state-of-the-art on challenging computer vision tasks. We introduce Resnet in Resnet (RiR): a deep dual-stream architecture that generalizes ResNets and standard CNNs and is easily implemented with no computational overhead. RiR consistently improves performance over ResNets, outperforms architectures with similar amounts of augmentation on CIFAR-10, and establishes a new state-of-the-art on CIFAR-100.","http://arxiv.org/pdf/1603.08029v1","cs.LG	cs.CV	cs.NE	stat.ML","Resnet in Resnet: Generalizing Residual Architectures","diogo@enlitic.com	kevin@enlitic.com"
"Mohammad Javad Shafiee	Alexander Wong","6","9","2016","There has been significant recent interest towards achieving highly efficient deep neural network architectures. A promising paradigm for achieving this is the concept of evolutionary deep intelligence, which attempts to mimic biological evolution processes to synthesize highly-efficient deep neural networks over successive generations. An important aspect of evolutionary deep intelligence is the genetic encoding scheme used to mimic heredity, which can have a significant impact on the quality of offspring deep neural networks. Motivated by the neurobiological phenomenon of synaptic clustering, we introduce a new genetic encoding scheme where synaptic probability is driven towards the formation of a highly sparse set of synaptic clusters. Experimental results for the task of image classification demonstrated that the synthesized offspring networks using this synaptic cluster-driven genetic encoding scheme can achieve state-of-the-art performance while having network architectures that are not only significantly more efficient (with a ~125-fold decrease in synapses for MNIST) compared to the original ancestor network, but also tailored for GPU-accelerated machine learning applications.","http://arxiv.org/pdf/1609.01360v2","cs.LG	cs.CV	cs.NE	stat.ML","Evolutionary Synthesis of Deep Neural Networks via Synaptic   Cluster-driven Genetic Encoding","mjshafiee@uwaterloo.ca	a28wong@uwaterloo.ca"
"Andrew Brock	Theodore Lim	J. M. Ritchie	Nick Weston","22","9","2016","The increasingly photorealistic sample quality of generative image models suggests their feasibility in applications beyond image generation. We present the Neural Photo Editor, an interface that leverages the power of generative neural networks to make large, semantically coherent changes to existing images. To tackle the challenge of achieving accurate reconstructions without loss of feature quality, we introduce the Introspective Adversarial Network, a novel hybridization of the VAE and GAN. Our model efficiently captures long-range dependencies through use of a computational block based on weight-shared dilated convolutions, and improves generalization performance with Orthogonal Regularization, a novel weight regularization method. We validate our contributions on CelebA, SVHN, and CIFAR-100, and produce samples and reconstructions with high visual fidelity.","http://arxiv.org/pdf/1609.07093v3","cs.LG	cs.CV	cs.NE	stat.ML","Neural Photo Editing with Introspective Adversarial Networks","ajb5@hw.ac.uk	t.lim@hw.ac.uk	j.m.ritchie@hw.ac.uk	Nick.Weston@renishaw.com"
"Tolga Bolukbasi	Joseph Wang	Ofer Dekel	Venkatesh Saligrama","25","2","2017","We present an approach to adaptively utilize deep neural networks in order to reduce the evaluation time on new examples without loss of accuracy. Rather than attempting to redesign or approximate existing networks, we propose two schemes that adaptively utilize networks. We first pose an adaptive network evaluation scheme, where we learn a system to adaptively choose the components of a deep network to be evaluated for each example. By allowing examples correctly classified using early layers of the system to exit, we avoid the computational time associated with full evaluation of the network. We extend this to learn a network selection system that adaptively selects the network to be evaluated for each example. We show that computational time can be dramatically reduced by exploiting the fact that many examples can be correctly classified using relatively efficient networks and that complex, computationally costly networks are only necessary for a small fraction of examples. We pose a global objective for learning an adaptive early exit or network selection policy and solve it by reducing the policy learning problem to a layer-by-layer weighted binary classification problem. Empirically, these approaches yield dramatic reductions in computational cost, with up to a 2.8x speedup on state-of-the-art networks from the ImageNet image recognition challenge with minimal (<1%) loss of top5 accuracy.","http://arxiv.org/pdf/1702.07811v2","cs.LG	cs.CV	cs.NE	stat.ML","Adaptive Neural Networks for Efficient Inference",""
"Zhengyang Wang	Hao Yuan	Shuiwang Ji","18","5","2017","The key idea of variational auto-encoders (VAEs) resembles that of traditional auto-encoder models in which spatial information is supposed to be explicitly encoded in the latent space. However, the latent variables in VAEs are vectors, which are commonly interpreted as multiple feature maps of size 1x1. Such representations can only convey spatial information implicitly when coupled with powerful decoders. In this work, we propose spatial VAEs that use latent variables as feature maps of larger size to explicitly capture spatial information. This is achieved by allowing the latent variables to be sampled from matrix-variate normal (MVN) distributions whose parameters are computed from the encoder network. To increase dependencies among locations on latent feature maps and reduce the number of parameters, we further propose spatial VAEs via low-rank MVN distributions. Experimental results show that the proposed spatial VAEs outperform original VAEs in capturing rich structural and spatial information.","http://arxiv.org/pdf/1705.06821v1","cs.LG	cs.CV	cs.NE	stat.ML","Spatial Variational Auto-Encoding via Matrix-Variate Normal   Distributions","zwang6@eecs.wsu.edu	hao.yuan@wsu.edu	sji@eecs.wsu.edu"
"Jun Li	Yongjun Chen	Lei Cai	Ian Davidson	Shuiwang Ji","24","5","2017","The key idea of current deep learning methods for dense prediction is to apply a model on a regular patch centered on each pixel to make pixel-wise predictions. These methods are limited in the sense that the patches are determined by network architecture instead of learned from data. In this work, we propose the dense transformer networks, which can learn the shapes and sizes of patches from data. The dense transformer networks employ an encoder-decoder architecture, and a pair of dense transformer modules are inserted into each of the encoder and decoder paths. The novelty of this work is that we provide technical solutions for learning the shapes and sizes of patches from data and efficiently restoring the spatial correspondence required for dense prediction. The proposed dense transformer modules are differentiable, thus the entire network can be trained. We apply the proposed networks on natural and biological image segmentation tasks and show superior performance is achieved in comparison to baseline methods.","http://arxiv.org/pdf/1705.08881v2","cs.CV	cs.LG	cs.NE	stat.ML","Dense Transformer Networks","jun.li3@wsu.edu	yongjun.chen@wsu.edu	davidson@cs.ucdavis.edu	lei.cai@wsu.edu	sji@eecs.wsu.edu"
"Saikat Chatterjee	Alireza M. Javid	Mostafa Sadeghi	Partha P. Mitra	Mikael Skoglund","23","10","2017","We develop an algorithm for systematic design of a large artificial neural network using a progression property. We find that some non-linear functions, such as the rectifier linear unit and its derivatives, hold the property. The systematic design addresses the choice of network size and regularization of parameters. The number of nodes and layers in network increases in progression with the objective of consistently reducing an appropriate cost. Each layer is optimized at a time, where appropriate parameters are learned using convex optimization. Regularization parameters for convex optimization do not need a significant manual effort for tuning. We also use random instances for some weight matrices, and that helps to reduce the number of parameters we learn. The developed network is expected to show good generalization power due to appropriate regularization and use of random weights in the layers. This expectation is verified by extensive experiments for classification and regression problems, using standard databases.","http://arxiv.org/pdf/1710.08177v1","cs.NE	cs.CV	cs.LG	stat.ML","Progressive Learning for Systematic Design of Large Neural Networks",""
"Shibani Santurkar	Ludwig Schmidt	Aleksander Mądry","2","11","2017","A fundamental, and still largely unanswered, question in the context of Generative Adversarial Networks (GANs) is whether GANs are actually able to capture the key characteristics of the datasets they are trained on. The current approaches to examining this issue require significant human supervision, such as visual inspection of sampled images, and often offer only fairly limited scalability. In this paper, we propose new techniques that employ a classification-based perspective to evaluate synthetic GAN distributions and their capability to accurately reflect the essential properties of the training data. These techniques require only minimal human supervision and can easily be scaled and adapted to evaluate a variety of state-of-the-art GANs on large, popular datasets. Our analysis indicates that GANs have significant problems in reproducing the more distributional properties of the training dataset. In particular, when seen through the lens of classification, the diversity of GAN data is orders of magnitude less than that of the original data.","http://arxiv.org/pdf/1711.00970v3","cs.LG	cs.CV	cs.NE	stat.ML","A Classification-Based Perspective on GAN Distributions","shibani@mit.edu	ludwigs@mit.edu	madry@mit.edu"
"Ethan Perez	Harm de Vries	Florian Strub	Vincent Dumoulin	Aaron Courville","10","7","2017","Achieving artificial visual reasoning - the ability to answer image-related questions which require a multi-step, high-level process - is an important step towards artificial general intelligence. This multi-modal task requires learning a question-dependent, structured reasoning process over images from language. Standard deep learning approaches tend to exploit biases in the data rather than learn this underlying structure, while leading methods learn to visually reason successfully but are hand-crafted for reasoning. We show that a general-purpose, Conditional Batch Normalization approach achieves state-of-the-art results on the CLEVR Visual Reasoning benchmark with a 2.4% error rate. We outperform the next best end-to-end method (4.5%) and even methods that use extra supervision (3.1%). We probe our model to shed light on how it reasons, showing it has learned a question-dependent, multi-step process. Previous work has operated under the assumption that visual reasoning calls for a specialized architecture, but we show that a general architecture with proper conditioning can learn to visually reason effectively.","http://arxiv.org/pdf/1707.03017v5","cs.CV	cs.AI	cs.CL	stat.ML","Learning Visual Reasoning Without Strong Priors","ethanperez@rice.edu,	mail@harmdevries.com,	florian.strub@inria.fr	dumouliv@iro.umontreal.ca,	courvila@iro.umontreal.ca"
"Jieyu Zhao	Tianlu Wang	Mark Yatskar	Vicente Ordonez	Kai-Wei Chang","29","7","2017","Language is increasingly being used to define rich visual recognition problems with supporting image collections sourced from the web. Structured prediction models are used in these tasks to take advantage of correlations between co-occurring labels and visual input but risk inadvertently encoding social biases found in web corpora. In this work, we study data and models associated with multilabel object classification and visual semantic role labeling. We find that (a) datasets for these tasks contain significant gender bias and (b) models trained on these datasets further amplify existing bias. For example, the activity cooking is over 33% more likely to involve females than males in a training set, and a trained model further amplifies the disparity to 68% at test time. We propose to inject corpus-level constraints for calibrating existing structured prediction models and design an algorithm based on Lagrangian relaxation for collective inference. Our method results in almost no performance loss for the underlying recognition task but decreases the magnitude of bias amplification by 47.5% and 40.5% for multilabel classification and visual semantic role labeling, respectively.","http://arxiv.org/pdf/1707.09457v1","cs.AI	cs.CL	cs.CV	stat.ML","Men Also Like Shopping: Reducing Gender Bias Amplification using   Corpus-level Constraints","jz4fu@virginia.edu	tw8cb@virginia.edu	vicente@virginia.edu	kc2wc@virginia.edu	my89@cs.washington.edu"
"Guillem Collell	Luc Van Gool	Marie-Francine Moens","18","11","2017","Spatial understanding is a fundamental problem with wide-reaching real-world applications. The representation of spatial knowledge is often modeled with spatial templates, i.e., regions of acceptability of two objects under an explicit spatial relationship (e.g., ""on"", ""below"", etc.). In contrast with prior work that restricts spatial templates to explicit spatial prepositions (e.g., ""glass on table""), here we extend this concept to implicit spatial language, i.e., those relationships (generally actions) for which the spatial arrangement of the objects is only implicitly implied (e.g., ""man riding horse""). In contrast with explicit relationships, predicting spatial arrangements from implicit spatial language requires significant common sense spatial understanding. Here, we introduce the task of predicting spatial templates for two objects under a relationship, which can be seen as a spatial question-answering task with a (2D) continuous output (""where is the man w.r.t. a horse when the man is walking the horse?""). We present two simple neural-based models that leverage annotated images and structured text to learn this task. The good performance of these models reveals that spatial locations are to a large extent predictable from implicit spatial language. Crucially, the models attain similar performance in a challenging generalized setting, where the object-relation-object combinations (e.g.,""man walking dog"") have never been seen before. Next, we go one step further by presenting the models with unseen objects (e.g., ""dog""). In this scenario, we show that leveraging word embeddings enables the models to output accurate spatial predictions, proving that the models acquire solid common sense spatial knowledge allowing for such generalization.","http://arxiv.org/pdf/1711.06821v2","cs.AI	cs.CL	cs.CV	stat.ML","Acquiring Common Sense Spatial Knowledge through Implicit Spatial   Templates","gcollell@kuleuven.be	vangool@vision.ee.ethz.ch	sien.moens@cs.kuleuven.be"
"Ethan Perez	Florian Strub	Harm de Vries	Vincent Dumoulin	Aaron Courville","22","9","2017","We introduce a general-purpose conditioning method for neural networks called FiLM: Feature-wise Linear Modulation. FiLM layers influence neural network computation via a simple, feature-wise affine transformation based on conditioning information. We show that FiLM layers are highly effective for visual reasoning - answering image-related questions which require a multi-step, high-level process - a task which has proven difficult for standard deep learning methods that do not explicitly model reasoning. Specifically, we show on visual reasoning tasks that FiLM layers 1) halve state-of-the-art error for the CLEVR benchmark, 2) modulate features in a coherent manner, 3) are robust to ablations and architectural modifications, and 4) generalize well to challenging, new data from few examples or even zero-shot.","http://arxiv.org/pdf/1709.07871v2","cs.CV	cs.AI	cs.CL	stat.ML","FiLM: Visual Reasoning with a General Conditioning Layer","ethanperez@rice.edu,	florian.strub@inria.fr,	dumouliv@iro.umontreal.ca	courvila@iro.umontreal.ca"
"Ivan Titov	Ehsan Khoddam","8","12","2014","We introduce a new approach to unsupervised estimation of feature-rich semantic role labeling models. Our model consists of two components: (1) an encoding component: a semantic role labeling model which predicts roles given a rich set of syntactic and lexical features; (2) a reconstruction component: a tensor factorization model which relies on roles to predict argument fillers. When the components are estimated jointly to minimize errors in argument reconstruction, the induced roles largely correspond to roles defined in annotated resources. Our method performs on par with most accurate role induction methods on English and German, even though, unlike these previous approaches, we do not incorporate any prior linguistic knowledge about the languages.","http://arxiv.org/pdf/1412.2812v1","cs.CL	cs.AI	cs.LG	stat.ML","Unsupervised Induction of Semantic Roles within a Reconstruction-Error   Minimization Framework","titov@uva.nl	e.khoddammohammadi@uva.nl"
"Tolga Bolukbasi	Kai-Wei Chang	James Zou	Venkatesh Saligrama	Adam Kalai","21","7","2016","The blind application of machine learning runs the risk of amplifying biases present in data. Such a danger is facing us with word embedding, a popular framework to represent text data as vectors which has been used in many machine learning and natural language processing tasks. We show that even word embeddings trained on Google News articles exhibit female/male gender stereotypes to a disturbing extent. This raises concerns because their widespread use, as we describe, often tends to amplify these biases. Geometrically, gender bias is first shown to be captured by a direction in the word embedding. Second, gender neutral words are shown to be linearly separable from gender definition words in the word embedding. Using these properties, we provide a methodology for modifying an embedding to remove gender stereotypes, such as the association between between the words receptionist and female, while maintaining desired associations such as between the words queen and female. We define metrics to quantify both direct and indirect gender biases in embeddings, and develop algorithms to ""debias"" the embedding. Using crowd-worker evaluation as well as standard benchmarks, we empirically demonstrate that our algorithms significantly reduce gender bias in embeddings while preserving the its useful properties such as the ability to cluster related concepts and to solve analogy tasks. The resulting embeddings can be used in applications without amplifying gender bias.","http://arxiv.org/pdf/1607.06520v1","cs.CL	cs.AI	cs.LG	stat.ML","Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word   Embeddings","tolgab@bu.edu,	kw@kwchang.net,	jamesyzou@gmail.com,	srv@bu.edu,	adam.kalai@microsoft.com"
"Adji B. Dieng	Chong Wang	Jianfeng Gao	John Paisley","5","11","2016","In this paper, we propose TopicRNN, a recurrent neural network (RNN)-based language model designed to directly capture the global semantic meaning relating words in a document via latent topics. Because of their sequential nature, RNNs are good at capturing the local structure of a word sequence - both semantic and syntactic - but might face difficulty remembering long-range dependencies. Intuitively, these long-range dependencies are of semantic nature. In contrast, latent topic models are able to capture the global underlying semantic structure of a document but do not account for word ordering. The proposed TopicRNN model integrates the merits of RNNs and latent topic models: it captures local (syntactic) dependencies using an RNN and global (semantic) dependencies using latent topics. Unlike previous work on contextual RNN language modeling, our model is learned end-to-end. Empirical results on word prediction show that TopicRNN outperforms existing contextual RNN baselines. In addition, TopicRNN can be used as an unsupervised feature extractor for documents. We do this for sentiment analysis on the IMDB movie review dataset and report an error rate of $6.28\%$. This is comparable to the state-of-the-art $5.91\%$ resulting from a semi-supervised approach. Finally, TopicRNN also yields sensible topics, making it a useful alternative to document models such as latent Dirichlet allocation.","http://arxiv.org/pdf/1611.01702v2","cs.CL	cs.AI	cs.LG	stat.ML","TopicRNN: A Recurrent Neural Network with Long-Range Semantic Dependency","abd2141@columbia.edu	chowang@microsoft.com	jpaisley@columbia.edu	jfgao@microsoft.com"
"Liwen Zhang	John Winn	Ryota Tomioka","7","11","2016","We propose the Gaussian attention model for content-based neural memory access. With the proposed attention model, a neural network has the additional degree of freedom to control the focus of its attention from a laser sharp attention to a broad attention. It is applicable whenever we can assume that the distance in the latent space reflects some notion of semantics. We use the proposed attention model as a scoring function for the embedding of a knowledge base into a continuous vector space and then train a model that performs question answering about the entities in the knowledge base. The proposed attention model can handle both the propagation of uncertainty when following a series of relations and also the conjunction of conditions in a natural way. On a dataset of soccer players who participated in the FIFA World Cup 2014, we demonstrate that our model can handle both path queries and conjunctive queries well.","http://arxiv.org/pdf/1611.02266v2","stat.ML	cs.AI	cs.CL	cs.LG","Gaussian Attention Model and Its Application to Knowledge Base Embedding   and Question Answering","jwinn@microsoft.com	ryoto@microsoft.com	liwenz@cs.uchicago.edu"
"Yacine Jernite	Edouard Grave	Armand Joulin	Tomas Mikolov","18","11","2016","Recurrent neural networks (RNNs) have been used extensively and with increasing success to model various types of sequential data. Much of this progress has been achieved through devising recurrent units and architectures with the flexibility to capture complex statistics in the data, such as long range dependency or localized attention phenomena. However, while many sequential data (such as video, speech or language) can have highly variable information flow, most recurrent models still consume input features at a constant rate and perform a constant number of computations per time step, which can be detrimental to both speed and model capacity. In this paper, we explore a modification to existing recurrent units which allows them to learn to vary the amount of computation they perform at each step, without prior knowledge of the sequence's time structure. We show experimentally that not only do our models require fewer operations, they also lead to better performance overall on evaluation tasks.","http://arxiv.org/pdf/1611.06188v2","stat.ML	cs.AI	cs.CL	cs.LG","Variable Computation in Recurrent Neural Networks","jernite@cs.nyu.edu	egrave@fb.com	ajoulin@fb.com	tmikolov@fb.com"
"Mostafa Dehghani	Aliaksei Severyn	Sascha Rothe	Jaap Kamps","30","11","2017","In this paper, we propose a method for training neural networks when we have a large set of data with weak labels and a small amount of data with true labels. In our proposed model, we train two neural networks: a target network, the learner and a confidence network, the meta-learner. The target network is optimized to perform a given task and is trained using a large set of unlabeled data that are weakly annotated. We propose to control the magnitude of the gradient updates to the target network using the scores provided by the second confidence network, which is trained on a small amount of supervised data. Thus we avoid that the weight updates computed from noisy labels harm the quality of the target network model.","http://arxiv.org/pdf/1711.11383v1","stat.ML	cs.AI	cs.CL	cs.LG","Learning to Learn from Weak Supervision by Full Supervision","dehghani@uva.nl	severyn@google.com	rothe@google.com	kamps@uva.nl"
"Garrett B. Goh	Nathan O. Hodas	Charles Siegel	Abhinav Vishnu","6","12","2017","Chemical databases store information in text representations, and the SMILES format is a universal standard used in many cheminformatics software. Encoded in each SMILES string is structural information that can be used to predict complex chemical properties. In this work, we develop SMILES2vec, a deep RNN that automatically learns features from SMILES to predict chemical properties, without the need for additional explicit feature engineering. Using Bayesian optimization methods to tune the network architecture, we show that an optimized SMILES2vec model can serve as a general-purpose neural network for predicting distinct chemical properties including toxicity, activity, solubility and solvation energy, while also outperforming contemporary MLP neural networks that uses engineered features. Furthermore, we demonstrate proof-of-concept of interpretability by developing an explanation mask that localizes on the most important characters used in making a prediction. When tested on the solubility dataset, it identified specific parts of a chemical that is consistent with established first-principles knowledge with an accuracy of 88%. Our work demonstrates that neural networks can learn technically accurate chemical concept and provide state-of-the-art accuracy, making interpretable deep neural networks a useful tool of relevance to the chemical industry.","http://arxiv.org/pdf/1712.02034v2","stat.ML	cs.AI	cs.CL	cs.LG","SMILES2Vec: An Interpretable General-Purpose Deep Neural Network for   Predicting Chemical Properties","garrett.goh@pnnl.gov	nathan.hodas@pnnl.gov	charles.siegel@pnnl.gov	abhinav.vishnu@pnnl.gov"
"Gellért Weisz	Paweł Budzianowski	Pei-Hao Su	Milica Gašić","11","2","2018","In spoken dialogue systems, we aim to deploy artificial intelligence to build automated dialogue agents that can converse with humans. A part of this effort is the policy optimisation task, which attempts to find a policy describing how to respond to humans, in the form of a function taking the current state of the dialogue and returning the response of the system. In this paper, we investigate deep reinforcement learning approaches to solve this problem. Particular attention is given to actor-critic methods, off-policy reinforcement learning with experience replay, and various methods aimed at reducing the bias and variance of estimators. When combined, these methods result in the previously proposed ACER algorithm that gave competitive results in gaming environments. These environments however are fully observable and have a relatively small action set so in this paper we examine the application of ACER to dialogue policy optimisation. We show that this method beats the current state-of-the-art in deep learning approaches for spoken dialogue systems. This not only leads to a more sample efficient algorithm that can train faster, but also allows us to apply the algorithm in more difficult environments than before. We thus experiment with learning in a very large action space, which has two orders of magnitude more actions than previously considered. We find that ACER trains significantly faster than the current state-of-the-art.","http://arxiv.org/pdf/1802.03753v1","cs.CL	cs.AI	cs.LG	stat.ML","Sample Efficient Deep Reinforcement Learning for Dialogue Systems with   Large Action Spaces",""
"M. Andrecut","23","2","2018","In this paper we explore the ""vector semantics"" problem from the perspective of ""almost orthogonal"" property of high-dimensional random vectors. We show that this intriguing property can be used to ""memorize"" random vectors by simply adding them, and we provide an efficient probabilistic solution to the set membership problem. Also, we discuss several applications to word context vector embeddings, document sentences similarity, and spam filtering.","http://arxiv.org/pdf/1802.09914v1","cs.CL	cs.AI	cs.LG	stat.ML","High-Dimensional Vector Semantics","mircea.andrecut@gmail.com"
"Ashutosh Modi	Ivan Titov","18","12","2013","Induction of common sense knowledge about prototypical sequences of events has recently received much attention. Instead of inducing this knowledge in the form of graphs, as in much of the previous work, in our method, distributed representations of event realizations are computed based on distributed representations of predicates and their arguments, and then these representations are used to predict prototypical event orderings. The parameters of the compositional process for computing the event representations and the ranking component of the model are jointly estimated from texts. We show that this approach results in a substantial boost in ordering performance with respect to previous methods.","http://arxiv.org/pdf/1312.5198v4","cs.LG	cs.AI	cs.CL	stat.ML	I.2.6; I.2.7","Learning Semantic Script Knowledge with Event Embeddings",""
"Andrew S. Lan	Divyanshu Vats	Andrew E. Waters	Richard G. Baraniuk","18","1","2015","While computer and communication technologies have provided effective means to scale up many aspects of education, the submission and grading of assessments such as homework assignments and tests remains a weak link. In this paper, we study the problem of automatically grading the kinds of open response mathematical questions that figure prominently in STEM (science, technology, engineering, and mathematics) courses. Our data-driven framework for mathematical language processing (MLP) leverages solution data from a large number of learners to evaluate the correctness of their solutions, assign partial-credit scores, and provide feedback to each learner on the likely locations of any errors. MLP takes inspiration from the success of natural language processing for text data and comprises three main steps. First, we convert each solution to an open response mathematical question into a series of numerical features. Second, we cluster the features from several solutions to uncover the structures of correct, partially correct, and incorrect solutions. We develop two different clustering approaches, one that leverages generic clustering algorithms and one based on Bayesian nonparametrics. Third, we automatically grade the remaining (potentially large number of) solutions based on their assigned cluster and one instructor-provided grade per cluster. As a bonus, we can track the cluster assignment of each step of a multistep solution and determine when it departs from a cluster of correct solutions, which enables us to indicate the likely locations of errors to learners. We test and validate MLP on real-world MOOC data to demonstrate how it can substantially reduce the human effort required in large-scale educational platforms.","http://arxiv.org/pdf/1501.04346v1","stat.ML	cs.AI	cs.CL	cs.LG","Mathematical Language Processing: Automatic Grading and Feedback for   Open Response Mathematical Questions","mr.lan@sparfa.com	dvats@sparfa.com	waters@sparfa.com	richb@sparfa.com"
"Tadahiro Taniguchi	Ryo Nakashima	Shogo Nagasaka","22","6","2015","Human infants can discover words directly from unsegmented speech signals without any explicitly labeled data. In this paper, we develop a novel machine learning method called nonparametric Bayesian double articulation analyzer (NPB-DAA) that can directly acquire language and acoustic models from observed continuous speech signals. For this purpose, we propose an integrative generative model that combines a language model and an acoustic model into a single generative model called the ""hierarchical Dirichlet process hidden language model"" (HDP-HLM). The HDP-HLM is obtained by extending the hierarchical Dirichlet process hidden semi-Markov model (HDP-HSMM) proposed by Johnson et al. An inference procedure for the HDP-HLM is derived using the blocked Gibbs sampler originally proposed for the HDP-HSMM. This procedure enables the simultaneous and direct inference of language and acoustic models from continuous speech signals. Based on the HDP-HLM and its inference procedure, we developed a novel double articulation analyzer. By assuming HDP-HLM as a generative model of observed time series data, and by inferring latent variables of the model, the method can analyze latent double articulation structure, i.e., hierarchically organized latent words and phonemes, of the data in an unsupervised manner. The novel unsupervised double articulation analyzer is called NPB-DAA.   The NPB-DAA can automatically estimate double articulation structure embedded in speech signals. We also carried out two evaluation experiments using synthetic data and actual human continuous speech signals representing Japanese vowel sequences. In the word acquisition and phoneme categorization tasks, the NPB-DAA outperformed a conventional double articulation analyzer (DAA) and baseline automatic speech recognition system whose acoustic model was trained in a supervised manner.","http://arxiv.org/pdf/1506.06646v2","cs.AI	cs.CL	cs.LG	stat.ML","Nonparametric Bayesian Double Articulation Analyzer for Direct Language   Acquisition from Continuous Speech Signals",""
"Zhiting Hu	Xuezhe Ma	Zhengzhong Liu	Eduard Hovy	Eric Xing","21","3","2016","Combining deep neural networks with structured logic rules is desirable to harness flexibility and reduce uninterpretability of the neural models. We propose a general framework capable of enhancing various types of neural networks (e.g., CNNs and RNNs) with declarative first-order logic rules. Specifically, we develop an iterative distillation method that transfers the structured information of logic rules into the weights of neural networks. We deploy the framework on a CNN for sentiment analysis, and an RNN for named entity recognition. With a few highly intuitive rules, we obtain substantial improvements and achieve state-of-the-art or comparable results to previous best-performing systems.","http://arxiv.org/pdf/1603.06318v4","cs.LG	cs.AI	cs.CL	stat.ML","Harnessing Deep Neural Networks with Logic Rules","zhitingh@cs.cmu.edu	xuezhem@cs.cmu.edu	liu@cs.cmu.edu	hovy@cmu.edu	epxing@cs.cmu.edu"
"Zhiting Hu	Zichao Yang	Xiaodan Liang	Ruslan Salakhutdinov	Eric P. Xing","2","3","2017","Generic generation and manipulation of text is challenging and has limited success compared to recent deep generative modeling in visual domain. This paper aims at generating plausible natural language sentences, whose attributes are dynamically controlled by learning disentangled latent representations with designated semantics. We propose a new neural generative model which combines variational auto-encoders and holistic attribute discriminators for effective imposition of semantic structures. With differentiable approximation to discrete text samples, explicit constraints on independent attribute controls, and efficient collaborative learning of generator and discriminators, our model learns highly interpretable representations from even only word annotations, and produces realistic sentences with desired attributes. Quantitative evaluation validates the accuracy of sentence and attribute generation.","http://arxiv.org/pdf/1703.00955v3","cs.LG	cs.AI	cs.CL	stat.ML","Toward Controlled Generation of Text",""
"Lianhui Qin	Zhisong Zhang	Hai Zhao	Zhiting Hu	Eric P. Xing","1","4","2017","Implicit discourse relation classification is of great challenge due to the lack of connectives as strong linguistic cues, which motivates the use of annotated implicit connectives to improve the recognition. We propose a feature imitation framework in which an implicit relation network is driven to learn from another neural network with access to connectives, and thus encouraged to extract similarly salient features for accurate classification. We develop an adversarial model to enable an adaptive imitation scheme through competition between the implicit network and a rival feature discriminator. Our method effectively transfers discriminability of connectives to the implicit features, and achieves state-of-the-art performance on the PDTB benchmark.","http://arxiv.org/pdf/1704.00217v1","cs.CL	cs.AI	cs.LG	stat.ML","Adversarial Connective-exploiting Networks for Implicit Discourse   Relation Classification","qinlianhui	zzs2011}@sjtu.edu.cn	zhaohai@cs.sjtu.edu.cn,	zhitingh@cs.cmu.edu	epxing@cs.cmu.edu"
"Maxim Rabinovich	Mitchell Stern	Dan Klein","25","4","2017","Tasks like code generation and semantic parsing require mapping unstructured (or partially structured) inputs to well-formed, executable outputs. We introduce abstract syntax networks, a modeling framework for these problems. The outputs are represented as abstract syntax trees (ASTs) and constructed by a decoder with a dynamically-determined modular structure paralleling the structure of the output tree. On the benchmark Hearthstone dataset for code generation, our model obtains 79.2 BLEU and 22.7% exact match accuracy, compared to previous state-of-the-art values of 67.1 and 6.1%. Furthermore, we perform competitively on the Atis, Jobs, and Geo semantic parsing datasets with no task-specific engineering.","http://arxiv.org/pdf/1704.07535v1","cs.CL	cs.AI	cs.LG	stat.ML","Abstract Syntax Networks for Code Generation and Semantic Parsing",""
"Ben Athiwaratkun	Andrew Gordon Wilson","27","4","2017","Word embeddings provide point representations of words containing useful semantic information. We introduce multimodal word distributions formed from Gaussian mixtures, for multiple word meanings, entailment, and rich uncertainty information. To learn these distributions, we propose an energy-based max-margin objective. We show that the resulting approach captures uniquely expressive semantic information, and outperforms alternatives, such as word2vec skip-grams, and Gaussian embeddings, on benchmark datasets such as word similarity and entailment.","http://arxiv.org/pdf/1704.08424v1","stat.ML	cs.AI	cs.CL	cs.LG","Multimodal Word Distributions","pa338@cornell.edu	andrew@cornell.edu"
"Brent Harrison	Upol Ehsan	Mark O. Riedl","26","7","2017","In this work we present a technique to use natural language to help reinforcement learning generalize to unseen environments. This technique uses neural machine translation, specifically the use of encoder-decoder networks, to learn associations between natural language behavior descriptions and state-action information. We then use this learned model to guide agent exploration using a modified version of policy shaping to make it more effective at learning in unseen environments. We evaluate this technique using the popular arcade game, Frogger, under ideal and non-ideal conditions. This evaluation shows that our modified policy shaping algorithm improves over a Q-learning agent as well as a baseline version of policy shaping.","http://arxiv.org/pdf/1707.08616v2","cs.AI	cs.CL	cs.LG	stat.ML","Guiding Reinforcement Learning Exploration Using Natural Language","harrison@cs.uky.edu	ehsan@gatech.edu	riedl@cc.gatech.edu"
"Mo Yu	Xiaoxiao Guo	Jinfeng Yi	Shiyu Chang	Saloni Potdar	Gerald Tesauro	Haoyu Wang	Bowen Zhou","26","8","2017","We investigate task clustering for deep-learning based multi-task and few-shot learning in a many-task setting. We propose a new method to measure task similarities with cross-task transfer performance matrix for the deep learning scenario. Although this matrix provides us critical information regarding similarity between tasks, its asymmetric property and unreliable performance scores can affect conventional clustering methods adversely. Additionally, the uncertain task-pairs, i.e., the ones with extremely asymmetric transfer scores, may collectively mislead clustering algorithms to output an inaccurate task-partition. To overcome these limitations, we propose a novel task-clustering algorithm by using the matrix completion technique. The proposed algorithm constructs a partially-observed similarity matrix based on the certainty of cluster membership of the task-pairs. We then use a matrix completion algorithm to complete the similarity matrix. Our theoretical analysis shows that under mild constraints, the proposed algorithm will perfectly recover the underlying ""true"" similarity matrix with a high probability. Our results show that the new task clustering method can discover task clusters for training flexible and superior neural network models in a multi-task learning setup for sentiment classification and dialog intent classification tasks. Our task clustering approach also extends metric-based few-shot learning methods to adapt multiple metrics, which demonstrates empirical advantages when the tasks are diverse.","http://arxiv.org/pdf/1708.07918v1","cs.LG	cs.AI	cs.CL	stat.ML","Robust Task Clustering for Deep Many-Task Learning",""
"Gino Brunner	Yuyi Wang	Roger Wattenhofer	Michael Weigelt","18","1","2018","We train multi-task autoencoders on linguistic tasks and analyze the learned hidden sentence representations. The representations change significantly when translation and part-of-speech decoders are added. The more decoders a model employs, the better it clusters sentences according to their syntactic similarity, as the representation space becomes less entangled. We explore the structure of the representation space by interpolating between sentences, which yields interesting pseudo-English sentences, many of which have recognizable syntactic structure. Lastly, we point out an interesting property of our models: The difference-vector between two sentences can be added to change a third sentence with similar features in a meaningful way.","http://arxiv.org/pdf/1801.06024v1","cs.CL	cs.AI	cs.LG	stat.ML","Natural Language Multitasking: Analyzing and Improving Syntactic   Saliency of Hidden Representations","brunnegi@ethz.ch	yuwang@ethz.ch	wattenhofer@ethz.ch	weigeltm@ethz.ch"
"Minghai Chen	Sen Wang	Paul Pu Liang	Tadas Baltrušaitis	Amir Zadeh	Louis-Philippe Morency","3","2","2018","With the increasing popularity of video sharing websites such as YouTube and Facebook, multimodal sentiment analysis has received increasing attention from the scientific community. Contrary to previous works in multimodal sentiment analysis which focus on holistic information in speech segments such as bag of words representations and average facial expression intensity, we develop a novel deep architecture for multimodal sentiment analysis that performs modality fusion at the word level. In this paper, we propose the Gated Multimodal Embedding LSTM with Temporal Attention (GME-LSTM(A)) model that is composed of 2 modules. The Gated Multimodal Embedding alleviates the difficulties of fusion when there are noisy modalities. The LSTM with Temporal Attention performs word level fusion at a finer fusion resolution between input modalities and attends to the most important time steps. As a result, the GME-LSTM(A) is able to better model the multimodal structure of speech through time and perform better sentiment comprehension. We demonstrate the effectiveness of this approach on the publicly-available Multimodal Corpus of Sentiment Intensity and Subjectivity Analysis (CMU-MOSI) dataset by achieving state-of-the-art sentiment classification and regression results. Qualitative analysis on our model emphasizes the importance of the Temporal Attention Layer in sentiment prediction because the additional acoustic and visual modalities are noisy. We also demonstrate the effectiveness of the Gated Multimodal Embedding in selectively filtering these noisy modalities out. Our results and analysis open new areas in the study of sentiment analysis in human communication and provide new models for multimodal fusion.","http://arxiv.org/pdf/1802.00924v1","cs.LG	cs.AI	cs.CL	stat.ML","Multimodal Sentiment Analysis with Word-Level Fusion and Reinforcement   Learning","minghai1@cs.cmu.edu	senw1@cs.cmu.edu	pliang@cs.cmu.edu	tb346@cl.cam.ac.uk	abagherz@cs.cmu.edu	morency@cs.cmu.edu"
"Ed Collins	Isabelle Augenstein	Sebastian Riedel","13","6","2017","Automatic summarisation is a popular approach to reduce a document to its main arguments. Recent research in the area has focused on neural approaches to summarisation, which can be very data-hungry. However, few large datasets exist and none for the traditionally popular domain of scientific publications, which opens up challenging research avenues centered on encoding large, complex documents. In this paper, we introduce a new dataset for summarisation of computer science publications by exploiting a large resource of author provided summaries and show straightforward ways of extending it further. We develop models on the dataset making use of both neural sentence encoding and traditionally used summarisation features and show that models which encode sentences as well as their local and global context perform best, significantly outperforming well-established baseline methods.","http://arxiv.org/pdf/1706.03946v1","cs.CL	cs.AI	cs.NE	stat.AP	stat.ML","A Supervised Approach to Extractive Summarisation of Scientific Papers","edward.collins.13@ucl.ac.uk	i.augenstein@ucl.ac.uk	s.riedel@ucl.ac.uk"
"Jacob Devlin	Hao Cheng	Hao Fang	Saurabh Gupta	Li Deng	Xiaodong He	Geoffrey Zweig	Margaret Mitchell","7","5","2015","Two recent approaches have achieved state-of-the-art results in image captioning. The first uses a pipelined process where a set of candidate words is generated by a convolutional neural network (CNN) trained on images, and then a maximum entropy (ME) language model is used to arrange these words into a coherent sentence. The second uses the penultimate activation layer of the CNN as input to a recurrent neural network (RNN) that then generates the caption sequence. In this paper, we compare the merits of these different language modeling approaches for the first time by using the same state-of-the-art CNN as input. We examine issues in the different approaches, including linguistic irregularities, caption repetition, and data set overlap. By combining key aspects of the ME and RNN methods, we achieve a new record performance over previously published results on the benchmark COCO dataset. However, the gains we see in BLEU do not translate to human judgments.","http://arxiv.org/pdf/1505.01809v3","cs.CL	cs.AI	cs.CV	cs.LG","Language Models for Image Captioning: The Quirks and What Works","jdevlin@microsoft.com	xiaohe@microsoft.com	gzweig@microsoft.com	memitc@microsoft.com"
"Mengye Ren	Ryan Kiros	Richard Zemel","8","5","2015","This work aims to address the problem of image-based question-answering (QA) with new models and datasets. In our work, we propose to use neural networks and visual semantic embeddings, without intermediate stages such as object detection and image segmentation, to predict answers to simple questions about images. Our model performs 1.8 times better than the only published results on an existing image QA dataset. We also present a question generation algorithm that converts image descriptions, which are widely available, into QA form. We used this algorithm to produce an order-of-magnitude larger dataset, with more evenly distributed answers. A suite of baseline results on this new dataset are also presented.","http://arxiv.org/pdf/1505.02074v4","cs.LG	cs.AI	cs.CL	cs.CV","Exploring Models and Data for Image Question Answering","mren@cs.toronto.edu	rkiros@cs.toronto.edu	zemel@cs.toronto.edu"
"Yash Goyal	Tejas Khot	Douglas Summers-Stay	Dhruv Batra	Devi Parikh","2","12","2016","Problems at the intersection of vision and language are of significant importance both as challenging research questions and for the rich set of applications they enable. However, inherent structure in our world and bias in our language tend to be a simpler signal for learning than visual modalities, resulting in models that ignore visual information, leading to an inflated sense of their capability.   We propose to counter these language priors for the task of Visual Question Answering (VQA) and make vision (the V in VQA) matter! Specifically, we balance the popular VQA dataset by collecting complementary images such that every question in our balanced dataset is associated with not just a single image, but rather a pair of similar images that result in two different answers to the question. Our dataset is by construction more balanced than the original VQA dataset and has approximately twice the number of image-question pairs. Our complete balanced dataset is publicly available at www.visualqa.org as part of the 2nd iteration of the Visual Question Answering Dataset and Challenge (VQA v2.0).   We further benchmark a number of state-of-art VQA models on our balanced dataset. All models perform significantly worse on our balanced dataset, suggesting that these models have indeed learned to exploit language priors. This finding provides the first concrete empirical evidence for what seems to be a qualitative sense among practitioners.   Finally, our data collection protocol for identifying complementary images enables us to develop a novel interpretable model, which in addition to providing an answer to the given (image, question) pair, also provides a counter-example based explanation. Specifically, it identifies an image that is similar to the original image, but it believes has a different answer to the same question. This can help in building trust for machines among their users.","http://arxiv.org/pdf/1612.00837v3","cs.CV	cs.AI	cs.CL	cs.LG","Making the V in VQA Matter: Elevating the Role of Image Understanding in   Visual Question Answering","ygoyal@vt.edu	tjskhot@vt.edu	douglas.a.summers-stay.civ@mail.mil	dbatra@gatech.edu	parikh@gatech.edu"
"Mateusz Malinowski	Mario Fritz","1","10","2014","We propose a method for automatically answering questions about images by bringing together recent advances from natural language processing and computer vision. We combine discrete reasoning with uncertain predictions by a multi-world approach that represents uncertainty about the perceived world in a bayesian framework. Our approach can handle human questions of high complexity about realistic scenes and replies with range of answer like counts, object classes, instances and lists of them. The system is directly trained from question-answer pairs. We establish a first benchmark for this task that can be seen as a modern attempt at a visual turing test.","http://arxiv.org/pdf/1410.0210v4","cs.AI	cs.CL	cs.CV	cs.LG","A Multi-World Approach to Question Answering about Real-World Scenes   based on Uncertain Input","mmalinow@mpi-inf.mpg.de	mfritz@mpi-inf.mpg.de"
"Mateusz Malinowski	Mario Fritz","14","1","2015","Progress in language and image understanding by machines has sparkled the interest of the research community in more open-ended, holistic tasks, and refueled an old AI dream of building intelligent machines. We discuss a few prominent challenges that characterize such holistic tasks and argue for ""question answering about images"" as a particular appealing instance of such a holistic task. In particular, we point out that it is a version of a Turing Test that is likely to be more robust to over-interpretations and contrast it with tasks like grounding and generation of descriptions. Finally, we discuss tools to measure progress in this field.","http://arxiv.org/pdf/1501.03302v2","cs.AI	cs.CL	cs.CV	cs.LG","Hard to Cheat: A Turing Test based on Answering Questions about Images","mmalinow@mpi-inf.mpg.de	mfritz@mpi-inf.mpg.de"
"Aishwarya Agrawal	Dhruv Batra	Devi Parikh","23","6","2016","Recently, a number of deep-learning based models have been proposed for the task of Visual Question Answering (VQA). The performance of most models is clustered around 60-70%. In this paper we propose systematic methods to analyze the behavior of these models as a first step towards recognizing their strengths and weaknesses, and identifying the most fruitful directions for progress. We analyze two models, one each from two major classes of VQA models -- with-attention and without-attention and show the similarities and differences in the behavior of these models. We also analyze the winning entry of the VQA Challenge 2016.   Our behavior analysis reveals that despite recent progress, today's VQA models are ""myopic"" (tend to fail on sufficiently novel instances), often ""jump to conclusions"" (converge on a predicted answer after 'listening' to just half the question), and are ""stubborn"" (do not change their answers across images).","http://arxiv.org/pdf/1606.07356v2","cs.CL	cs.AI	cs.CV	cs.LG","Analyzing the Behavior of Visual Question Answering Models","aish@vt.edu	dbatra@vt.edu	parikh@vt.edu"
"Harsh Agrawal	Arjun Chandrasekaran	Dhruv Batra	Devi Parikh	Mohit Bansal","23","6","2016","Temporal common sense has applications in AI tasks such as QA, multi-document summarization, and human-AI communication. We propose the task of sequencing -- given a jumbled set of aligned image-caption pairs that belong to a story, the task is to sort them such that the output sequence forms a coherent story. We present multiple approaches, via unary (position) and pairwise (order) predictions, and their ensemble-based combinations, achieving strong results on this task. We use both text-based and image-based features, which depict complementary improvements. Using qualitative examples, we demonstrate that our models have learnt interesting aspects of temporal common sense.","http://arxiv.org/pdf/1606.07493v5","cs.CL	cs.AI	cs.CV	cs.LG","Sort Story: Sorting Jumbled Images and Captions into Stories","harsh92	carjun	dbatra	parikh}@vt.edu	mbansal@cs.unc.edu"
"Ashkan Mokarian	Mateusz Malinowski	Mario Fritz","9","8","2016","We present Mean Box Pooling, a novel visual representation that pools over CNN representations of a large number, highly overlapping object proposals. We show that such representation together with nCCA, a successful multimodal embedding technique, achieves state-of-the-art performance on the Visual Madlibs task. Moreover, inspired by the nCCA's objective function, we extend classical CNN+LSTM approach to train the network by directly maximizing the similarity between the internal representation of the deep learning architecture and candidate answers. Again, such approach achieves a significant improvement over the prior work that also uses CNN+LSTM approach on Visual Madlibs.","http://arxiv.org/pdf/1608.02717v1","cs.CV	cs.AI	cs.CL	cs.LG","Mean Box Pooling: A Rich Image Representation and Output Embedding for   the Visual Madlibs Task","ashkan@mpi-inf.mpg.de	mmalinow@mpi-inf.mpg.de	mfritz@mpi-inf.mpg.de"
"Yuval Atzmon	Jonathan Berant	Vahid Kezami	Amir Globerson	Gal Chechik","27","8","2016","Recurrent neural networks have recently been used for learning to describe images using natural language. However, it has been observed that these models generalize poorly to scenes that were not observed during training, possibly depending too strongly on the statistics of the text in the training data. Here we propose to describe images using short structured representations, aiming to capture the crux of a description. These structured representations allow us to tease-out and evaluate separately two types of generalization: standard generalization to new images with similar scenes, and generalization to new combinations of known entities. We compare two learning approaches on the MS-COCO dataset: a state-of-the-art recurrent network based on an LSTM (Show, Attend and Tell), and a simple structured prediction model on top of a deep network. We find that the structured model generalizes to new compositions substantially better than the LSTM, ~7 times the accuracy of predicting structured representations. By providing a concrete method to quantify generalization for unseen combinations, we argue that structured representations and compositional splits are a useful benchmark for image captioning, and advocate compositional models that capture linguistic and visual structure.","http://arxiv.org/pdf/1608.07639v1","cs.CV	cs.AI	cs.CL	cs.LG","Learning to generalize to new compositions in image understanding","yuval.atzmon@biu.ac.il"
"C. Lawrence Zitnick	Aishwarya Agrawal	Stanislaw Antol	Margaret Mitchell	Dhruv Batra	Devi Parikh","31","8","2016","As machines have become more intelligent, there has been a renewed interest in methods for measuring their intelligence. A common approach is to propose tasks for which a human excels, but one which machines find difficult. However, an ideal task should also be easy to evaluate and not be easily gameable. We begin with a case study exploring the recently popular task of image captioning and its limitations as a task for measuring machine intelligence. An alternative and more promising task is Visual Question Answering that tests a machine's ability to reason about language and vision. We describe a dataset unprecedented in size created for the task that contains over 760,000 human generated questions about images. Using around 10 million human generated answers, machines may be easily evaluated.","http://arxiv.org/pdf/1608.08716v1","cs.AI	cs.CL	cs.CV	cs.LG","Measuring Machine Intelligence Through Visual Question Answering","zitnick@fb.com	aish@vt.edu	santol@vt.edu	memitc@microsoft.com	dbatra@vt.edu	parikh@vt.edu"
"Yash Goyal	Akrit Mohapatra	Devi Parikh	Dhruv Batra","31","8","2016","Deep neural networks have shown striking progress and obtained state-of-the-art results in many AI research fields in the recent years. However, it is often unsatisfying to not know why they predict what they do. In this paper, we address the problem of interpreting Visual Question Answering (VQA) models. Specifically, we are interested in finding what part of the input (pixels in images or words in questions) the VQA model focuses on while answering the question. To tackle this problem, we use two visualization techniques -- guided backpropagation and occlusion -- to find important words in the question and important regions in the image. We then present qualitative and quantitative analyses of these importance maps. We found that even without explicit attention mechanisms, VQA models may sometimes be implicitly attending to relevant regions in the image, and often to appropriate words in the question.","http://arxiv.org/pdf/1608.08974v2","cs.CV	cs.AI	cs.CL	cs.LG","Towards Transparent AI Systems: Interpreting Visual Question Answering   Models","ygoyal@vt.edu	akrit@vt.edu	parikh@vt.edu	dbatra@vt.edu"
"Abhishek Das	Satwik Kottur	Khushi Gupta	Avi Singh	Deshraj Yadav	José M. F. Moura	Devi Parikh	Dhruv Batra","26","11","2016","We introduce the task of Visual Dialog, which requires an AI agent to hold a meaningful dialog with humans in natural, conversational language about visual content. Specifically, given an image, a dialog history, and a question about the image, the agent has to ground the question in image, infer context from history, and answer the question accurately. Visual Dialog is disentangled enough from a specific downstream task so as to serve as a general test of machine intelligence, while being grounded in vision enough to allow objective evaluation of individual responses and benchmark progress. We develop a novel two-person chat data-collection protocol to curate a large-scale Visual Dialog dataset (VisDial). VisDial v0.9 has been released and contains 1 dialog with 10 question-answer pairs on ~120k images from COCO, with a total of ~1.2M dialog question-answer pairs.   We introduce a family of neural encoder-decoder models for Visual Dialog with 3 encoders -- Late Fusion, Hierarchical Recurrent Encoder and Memory Network -- and 2 decoders (generative and discriminative), which outperform a number of sophisticated baselines. We propose a retrieval-based evaluation protocol for Visual Dialog where the AI agent is asked to sort a set of candidate answers and evaluated on metrics such as mean-reciprocal-rank of human response. We quantify gap between machine and human performance on the Visual Dialog task via human studies. Putting it all together, we demonstrate the first 'visual chatbot'! Our dataset, code, trained models and visual chatbot are available on https://visualdialog.org","http://arxiv.org/pdf/1611.08669v5","cs.CV	cs.AI	cs.CL	cs.LG","Visual Dialog","abhshkdz@gatech.edu	parikh@gatech.edu	dbatra@gatech.edu	skottur@andrew.cmu.edu	khushig@andrew.cmu.edu	moura@andrew.cmu.edu	avisingh@cs.berkeley.edu	deshraj@vt.edu"
"Abhinav Thanda	Shankar M Venkatesan","10","1","2017","Multi-task learning (MTL) involves the simultaneous training of two or more related tasks over shared representations. In this work, we apply MTL to audio-visual automatic speech recognition(AV-ASR). Our primary task is to learn a mapping between audio-visual fused features and frame labels obtained from acoustic GMM/HMM model. This is combined with an auxiliary task which maps visual features to frame labels obtained from a separate visual GMM/HMM model. The MTL model is tested at various levels of babble noise and the results are compared with a base-line hybrid DNN-HMM AV-ASR model. Our results indicate that MTL is especially useful at higher level of noise. Compared to base-line, upto 7\% relative improvement in WER is reported at -3 SNR dB","http://arxiv.org/pdf/1701.02477v1","cs.CL	cs.AI	cs.CV	cs.LG","Multi-task Learning Of Deep Neural Networks For Audio Visual Automatic   Speech Recognition",""
"Abhishek Das	Satwik Kottur	José M. F. Moura	Stefan Lee	Dhruv Batra","20","3","2017","We introduce the first goal-driven training for visual question answering and dialog agents. Specifically, we pose a cooperative 'image guessing' game between two agents -- Qbot and Abot -- who communicate in natural language dialog so that Qbot can select an unseen image from a lineup of images. We use deep reinforcement learning (RL) to learn the policies of these agents end-to-end -- from pixels to multi-agent multi-round dialog to game reward.   We demonstrate two experimental results.   First, as a 'sanity check' demonstration of pure RL (from scratch), we show results on a synthetic world, where the agents communicate in ungrounded vocabulary, i.e., symbols with no pre-specified meanings (X, Y, Z). We find that two bots invent their own communication protocol and start using certain symbols to ask/answer about certain visual attributes (shape/color/style). Thus, we demonstrate the emergence of grounded language and communication among 'visual' dialog agents with no human supervision.   Second, we conduct large-scale real-image experiments on the VisDial dataset, where we pretrain with supervised dialog data and show that the RL 'fine-tuned' agents significantly outperform SL agents. Interestingly, the RL Qbot learns to ask questions that Abot is good at, ultimately resulting in more informative dialog and a better team.","http://arxiv.org/pdf/1703.06585v2","cs.CV	cs.AI	cs.CL	cs.LG","Learning Cooperative Visual Dialog Agents with Deep Reinforcement   Learning",""
"Wei-Lun Chao	Hexiang Hu	Fei Sha","24","4","2017","Visual question answering (QA) has attracted a lot of attention lately, seen essentially as a form of (visual) Turing test that artificial intelligence should strive to achieve. In this paper, we study a crucial component of this task: how can we design good datasets for the task? We focus on the design of multiple-choice based datasets where the learner has to select the right answer from a set of candidate ones including the target (i.e. the correct one) and the decoys (i.e. the incorrect ones). Through careful analysis of the results attained by state-of-the-art learning models and human annotators on existing datasets, we show the design of the decoy answers has a significant impact on how and what the learning models learn from the datasets. In particular, the resulting learner can ignore the visual information, the question, or the both while still doing well on the task. Inspired by this, we propose automatic procedures to remedy such design deficiencies. We apply the procedures to re-construct decoy answers for two popular visual QA datasets as well as to create a new visual QA dataset from the Visual Genome project, resulting in the largest dataset for this task. Extensive empirical studies show that the design deficiencies have been alleviated in the remedied datasets and the performance on them is likely a more faithful indicator of the difference among learning models. The datasets are released and publicly available via http://www.teds.usc.edu/website_vqa/.","http://arxiv.org/pdf/1704.07121v1","cs.CL	cs.AI	cs.CV	cs.LG","Being Negative but Constructively: Lessons Learnt from Creating Better   Visual Question Answering Datasets","weilunc@usc.edu,	hexiang.frank.hu@gmail.com,	feisha@usc.edu"
"Aishwarya Agrawal	Aniruddha Kembhavi	Dhruv Batra	Devi Parikh","26","4","2017","Visual Question Answering (VQA) has received a lot of attention over the past couple of years. A number of deep learning models have been proposed for this task. However, it has been shown that these models are heavily driven by superficial correlations in the training data and lack compositionality -- the ability to answer questions about unseen compositions of seen concepts. This compositionality is desirable and central to intelligence. In this paper, we propose a new setting for Visual Question Answering where the test question-answer pairs are compositionally novel compared to training question-answer pairs. To facilitate developing models under this setting, we present a new compositional split of the VQA v1.0 dataset, which we call Compositional VQA (C-VQA). We analyze the distribution of questions and answers in the C-VQA splits. Finally, we evaluate several existing VQA models under this new setting and show that the performances of these models degrade by a significant amount compared to the original VQA setting.","http://arxiv.org/pdf/1704.08243v1","cs.CV	cs.AI	cs.CL	cs.LG","C-VQA: A Compositional Split of the Visual Question Answering (VQA) v1.0   Dataset","aish@vt.edu,	anik@allenai.org,	dbatra@gatech.edu	parikh@gatech.edu"
"Alexander Kuhnle	Ann Copestake","5","6","2017","We discuss problems with the standard approaches to evaluation for tasks like visual question answering, and argue that artificial data can be used to address these as a complement to current practice. We demonstrate that with the help of existing 'deep' linguistic processing technology we are able to create challenging abstract datasets, which enable us to investigate the language understanding abilities of multimodal deep learning models in detail.","http://arxiv.org/pdf/1706.01322v1","cs.CL	cs.AI	cs.CV	cs.LG","Deep learning evaluation using deep linguistic processing","aok25@cam.ac.uk	aac10@cam.ac.uk"
"Xu Sun	Xuancheng Ren	Shuming Ma	Houfeng Wang","19","6","2017","We propose a simple yet effective technique for neural network learning. The forward propagation is computed as usual. In back propagation, only a small subset of the full gradient is computed to update the model parameters. The gradient vectors are sparsified in such a way that only the top-$k$ elements (in terms of magnitude) are kept. As a result, only $k$ rows or columns (depending on the layout) of the weight matrix are modified, leading to a linear reduction ($k$ divided by the vector dimension) in the computational cost. Surprisingly, experimental results demonstrate that we can update only 1--4\% of the weights at each back propagation pass. This does not result in a larger number of training iterations. More interestingly, the accuracy of the resulting models is actually improved rather than degraded, and a detailed analysis is given. The code is available at https://github.com/jklj077/meProp","http://arxiv.org/pdf/1706.06197v4","cs.LG	cs.AI	cs.CL	cs.CV","meProp: Sparsified Back Propagation for Accelerated Deep Learning with   Reduced Overfitting",""
"Suranjana Samanta	Sameep Mehta","10","7","2017","Adversarial samples are strategically modified samples, which are crafted with the purpose of fooling a classifier at hand. An attacker introduces specially crafted adversarial samples to a deployed classifier, which are being mis-classified by the classifier. However, the samples are perceived to be drawn from entirely different classes and thus it becomes hard to detect the adversarial samples. Most of the prior works have been focused on synthesizing adversarial samples in the image domain. In this paper, we propose a new method of crafting adversarial text samples by modification of the original samples. Modifications of the original text samples are done by deleting or replacing the important or salient words in the text or by introducing new words in the text sample. Our algorithm works best for the datasets which have sub-categories within each of the classes of examples. While crafting adversarial samples, one of the key constraint is to generate meaningful sentences which can at pass off as legitimate from language (English) viewpoint. Experimental results on IMDB movie review dataset for sentiment analysis and Twitter dataset for gender detection show the efficiency of our proposed method.","http://arxiv.org/pdf/1707.02812v1","cs.LG	cs.AI	cs.CL	cs.CV","Towards Crafting Text Adversarial Samples","suransam@in.ibm.com	sameepmehta@in.ibm.com"
"Ramakanth Pasunuru	Mohit Bansal","7","8","2017","Sequence-to-sequence models have shown promising improvements on the temporal task of video captioning, but they optimize word-level cross-entropy loss during training. First, using policy gradient and mixed-loss methods for reinforcement learning, we directly optimize sentence-level task-based metrics (as rewards), achieving significant improvements over the baseline, based on both automatic metrics and human evaluation on multiple datasets. Next, we propose a novel entailment-enhanced reward (CIDEnt) that corrects phrase-matching based metrics (such as CIDEr) to only allow for logically-implied partial matches and avoid contradictions, achieving further significant improvements over the CIDEr-reward model. Overall, our CIDEnt-reward model achieves the new state-of-the-art on the MSR-VTT dataset.","http://arxiv.org/pdf/1708.02300v1","cs.CL	cs.AI	cs.CV	cs.LG","Reinforced Video Captioning with Entailment Rewards","ram@cs.unc.edu	mbansal@cs.unc.edu"
"Licheng Yu	Mohit Bansal	Tamara L. Berg","9","8","2017","We address the problem of end-to-end visual storytelling. Given a photo album, our model first selects the most representative (summary) photos, and then composes a natural language story for the album. For this task, we make use of the Visual Storytelling dataset and a model composed of three hierarchically-attentive Recurrent Neural Nets (RNNs) to: encode the album photos, select representative (summary) photos, and compose the story. Automatic and human evaluations show our model achieves better performance on selection, generation, and retrieval than baselines.","http://arxiv.org/pdf/1708.02977v1","cs.CL	cs.AI	cs.CV	cs.LG","Hierarchically-Attentive RNN for Album Summarization and Storytelling","licheng@cs.unc.edu	mbansal@cs.unc.edu	tlberg@cs.unc.edu"
